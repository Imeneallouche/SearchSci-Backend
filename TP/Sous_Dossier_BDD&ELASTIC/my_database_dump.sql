PRAGMA foreign_keys=OFF;
BEGIN TRANSACTION;
CREATE TABLE IF NOT EXISTS "django_migrations" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "app" varchar(255) NOT NULL, "name" varchar(255) NOT NULL, "applied" datetime NOT NULL);
INSERT INTO django_migrations VALUES(1,'contenttypes','0001_initial','2023-12-08 22:15:27.022891');
INSERT INTO django_migrations VALUES(2,'auth','0001_initial','2023-12-08 22:15:27.075618');
INSERT INTO django_migrations VALUES(3,'admin','0001_initial','2023-12-08 22:15:27.102910');
INSERT INTO django_migrations VALUES(4,'admin','0002_logentry_remove_auto_add','2023-12-08 22:15:27.143156');
INSERT INTO django_migrations VALUES(5,'admin','0003_logentry_add_action_flag_choices','2023-12-08 22:15:27.168108');
INSERT INTO django_migrations VALUES(6,'contenttypes','0002_remove_content_type_name','2023-12-08 22:15:27.232914');
INSERT INTO django_migrations VALUES(7,'auth','0002_alter_permission_name_max_length','2023-12-08 22:15:27.333889');
INSERT INTO django_migrations VALUES(8,'auth','0003_alter_user_email_max_length','2023-12-08 22:15:27.412458');
INSERT INTO django_migrations VALUES(9,'auth','0004_alter_user_username_opts','2023-12-08 22:15:27.432836');
INSERT INTO django_migrations VALUES(10,'auth','0005_alter_user_last_login_null','2023-12-08 22:15:27.459262');
INSERT INTO django_migrations VALUES(11,'auth','0006_require_contenttypes_0002','2023-12-08 22:15:27.471876');
INSERT INTO django_migrations VALUES(12,'auth','0007_alter_validators_add_error_messages','2023-12-08 22:15:27.492092');
INSERT INTO django_migrations VALUES(13,'auth','0008_alter_user_username_max_length','2023-12-08 22:15:27.516571');
INSERT INTO django_migrations VALUES(14,'auth','0009_alter_user_last_name_max_length','2023-12-08 22:15:27.535913');
INSERT INTO django_migrations VALUES(15,'auth','0010_alter_group_name_max_length','2023-12-08 22:15:27.560158');
INSERT INTO django_migrations VALUES(16,'auth','0011_update_proxy_permissions','2023-12-08 22:15:27.576483');
INSERT INTO django_migrations VALUES(17,'auth','0012_alter_user_first_name_max_length','2023-12-08 22:15:27.611986');
INSERT INTO django_migrations VALUES(18,'sessions','0001_initial','2023-12-08 22:15:27.632943');
INSERT INTO django_migrations VALUES(19,'articleApp','0001_initial','2023-12-14 08:22:28.869316');
INSERT INTO django_migrations VALUES(20,'accountsApp','0001_initial','2023-12-15 21:08:16.468278');
INSERT INTO django_migrations VALUES(21,'accountsApp','0002_delete_user','2023-12-15 21:08:16.484379');
INSERT INTO django_migrations VALUES(22,'accountsApp','0003_initial','2023-12-15 21:44:11.726329');
INSERT INTO django_migrations VALUES(23,'articleApp','0002_elasticdemo','2023-12-25 19:35:33.900424');
INSERT INTO django_migrations VALUES(24,'articleApp','0003_delete_elasticdemo_remove_auteur_nom_and_more','2023-12-29 13:05:20.974127');
INSERT INTO django_migrations VALUES(25,'articleApp','0004_rename_mots_cles_article_motscles_and_more','2024-01-04 23:41:03.656068');
INSERT INTO django_migrations VALUES(26,'articleApp','0005_remove_article_texteintegral','2024-01-11 18:35:08.267057');
INSERT INTO django_migrations VALUES(27,'articleApp','0006_article_texteintegral','2024-01-31 21:43:51.384909');
CREATE TABLE IF NOT EXISTS "auth_group_permissions" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "group_id" integer NOT NULL REFERENCES "auth_group" ("id") DEFERRABLE INITIALLY DEFERRED, "permission_id" integer NOT NULL REFERENCES "auth_permission" ("id") DEFERRABLE INITIALLY DEFERRED);
CREATE TABLE IF NOT EXISTS "auth_user_groups" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "user_id" integer NOT NULL REFERENCES "auth_user" ("id") DEFERRABLE INITIALLY DEFERRED, "group_id" integer NOT NULL REFERENCES "auth_group" ("id") DEFERRABLE INITIALLY DEFERRED);
CREATE TABLE IF NOT EXISTS "auth_user_user_permissions" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "user_id" integer NOT NULL REFERENCES "auth_user" ("id") DEFERRABLE INITIALLY DEFERRED, "permission_id" integer NOT NULL REFERENCES "auth_permission" ("id") DEFERRABLE INITIALLY DEFERRED);
CREATE TABLE IF NOT EXISTS "django_admin_log" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "object_id" text NULL, "object_repr" varchar(200) NOT NULL, "action_flag" smallint unsigned NOT NULL CHECK ("action_flag" >= 0), "change_message" text NOT NULL, "content_type_id" integer NULL REFERENCES "django_content_type" ("id") DEFERRABLE INITIALLY DEFERRED, "user_id" integer NOT NULL REFERENCES "auth_user" ("id") DEFERRABLE INITIALLY DEFERRED, "action_time" datetime NOT NULL);
INSERT INTO django_admin_log VALUES(1,'3','NadaA@gmail.com',3,'',4,1,'2023-12-15 22:00:45.663930');
INSERT INTO django_admin_log VALUES(2,'2','Nada@gmail.com',3,'',4,1,'2023-12-16 14:58:02.673065');
INSERT INTO django_admin_log VALUES(3,'4','NadaAA@gmail.com',3,'',4,1,'2023-12-16 14:58:02.695559');
INSERT INTO django_admin_log VALUES(4,'7','TT@gmail.com',3,'',4,1,'2023-12-16 14:58:02.703549');
INSERT INTO django_admin_log VALUES(5,'8','TTT@gmail.com',3,'',4,1,'2023-12-16 14:58:02.719851');
INSERT INTO django_admin_log VALUES(6,'9','TTTAAA@gmail.com',3,'',4,1,'2023-12-16 14:58:02.736199');
INSERT INTO django_admin_log VALUES(7,'5','test@gmail.com',3,'',4,1,'2023-12-16 14:58:02.744204');
INSERT INTO django_admin_log VALUES(8,'6','testt@gmail.com',3,'',4,1,'2023-12-16 14:58:02.760419');
INSERT INTO django_admin_log VALUES(9,'11','Imen@gmail.com',3,'',4,1,'2023-12-28 18:33:20.111405');
INSERT INTO django_admin_log VALUES(10,'17','Ines@gmail.com',3,'',4,1,'2023-12-28 18:33:20.127493');
INSERT INTO django_admin_log VALUES(11,'18','NaDa@gmail.com',3,'',4,1,'2023-12-28 18:33:20.142421');
INSERT INTO django_admin_log VALUES(12,'10','Nada@gmail.com',3,'',4,1,'2023-12-28 18:33:20.145958');
INSERT INTO django_admin_log VALUES(13,'12','TEST1@gmail.com',3,'',4,1,'2023-12-28 18:33:20.164315');
INSERT INTO django_admin_log VALUES(14,'14','TEST2@gmail.com',3,'',4,1,'2023-12-28 18:33:20.176517');
INSERT INTO django_admin_log VALUES(15,'13','TEST@gmail.com',3,'',4,1,'2023-12-28 18:33:20.189961');
INSERT INTO django_admin_log VALUES(16,'19','Taki@gmail.com',3,'',4,1,'2023-12-28 18:33:20.199098');
INSERT INTO django_admin_log VALUES(17,'20','mama@gmail.com',3,'',4,1,'2023-12-28 18:33:20.212197');
INSERT INTO django_admin_log VALUES(18,'21','mamab@gmail.com',3,'',4,1,'2023-12-28 18:33:20.225191');
INSERT INTO django_admin_log VALUES(19,'1','Livre Architecture des ordinateurs',1,'[{"added": {}}]',7,1,'2023-12-29 21:21:40.631314');
INSERT INTO django_admin_log VALUES(20,'2','Basic of machine learning',1,'[{"added": {}}]',7,1,'2023-12-29 21:22:10.312279');
INSERT INTO django_admin_log VALUES(21,'3','Optimisation et l''analyse numerique',1,'[{"added": {}}]',7,1,'2023-12-29 21:23:01.795432');
INSERT INTO django_admin_log VALUES(22,'4','Processus sous linux system',1,'[{"added": {}}]',7,1,'2023-12-29 21:23:27.255330');
INSERT INTO django_admin_log VALUES(23,'1','ESI',1,'[{"added": {}}]',8,1,'2023-12-29 21:24:03.041252');
INSERT INTO django_admin_log VALUES(24,'2','ENSIA',1,'[{"added": {}}]',8,1,'2023-12-29 21:24:16.266014');
INSERT INTO django_admin_log VALUES(25,'3','ENS',1,'[{"added": {}}]',8,1,'2023-12-29 21:24:33.843775');
INSERT INTO django_admin_log VALUES(26,'4','USTHB',1,'[{"added": {}}]',8,1,'2023-12-29 21:24:52.089484');
INSERT INTO django_admin_log VALUES(27,'1','Hamid Haddadou',1,'[{"added": {}}]',9,1,'2023-12-29 21:25:21.500168');
INSERT INTO django_admin_log VALUES(28,'2','Koudil',1,'[{"added": {}}]',9,1,'2023-12-29 21:25:56.506761');
INSERT INTO django_admin_log VALUES(29,'3','Ayad Khadija',1,'[{"added": {}}]',9,1,'2023-12-29 21:27:12.407956');
INSERT INTO django_admin_log VALUES(30,'4','Ilham Bekkar',1,'[{"added": {}}]',9,1,'2023-12-29 21:27:51.062410');
INSERT INTO django_admin_log VALUES(31,'5','Achour',1,'[{"added": {}}]',9,1,'2023-12-29 21:28:13.521189');
INSERT INTO django_admin_log VALUES(32,'1','Analyse Numerique',1,'[{"added": {}}]',10,1,'2023-12-29 21:30:01.003341');
INSERT INTO django_admin_log VALUES(33,'2','memoire assossiative',1,'[{"added": {}}]',10,1,'2023-12-29 21:31:00.650792');
INSERT INTO django_admin_log VALUES(34,'3','Synchronisation Processus',1,'[{"added": {}}]',10,1,'2023-12-29 21:31:35.650958');
INSERT INTO django_admin_log VALUES(35,'4','Preprocessing Methods',1,'[{"added": {}}]',10,1,'2023-12-29 21:32:22.418124');
INSERT INTO django_admin_log VALUES(36,'4','Preprocessing Methods',2,'[{"changed": {"fields": ["DateDePublication"]}}]',10,1,'2024-01-11 18:39:07.954782');
INSERT INTO django_admin_log VALUES(37,'5','Teeeeest',1,'[{"added": {}}]',10,1,'2024-01-11 19:07:21.268903');
INSERT INTO django_admin_log VALUES(38,'6','nadaaaaaaa',1,'[{"added": {}}]',10,1,'2024-01-11 19:11:36.480493');
INSERT INTO django_admin_log VALUES(39,'7','teeest12',1,'[{"added": {}}]',10,1,'2024-01-11 19:15:40.747635');
INSERT INTO django_admin_log VALUES(40,'8','Teeeeestthg',1,'[{"added": {}}]',10,1,'2024-01-11 19:17:24.324814');
INSERT INTO django_admin_log VALUES(41,'9','AZERTYUIOIUYTRF',1,'[{"added": {}}]',10,1,'2024-01-11 19:18:46.569439');
INSERT INTO django_admin_log VALUES(42,'4',' ',1,'[{"added": {}}]',12,1,'2024-01-30 23:15:34.690123');
INSERT INTO django_admin_log VALUES(43,'8','Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model',3,'',10,1,'2024-01-31 21:45:52.684855');
INSERT INTO django_admin_log VALUES(44,'9','Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model',3,'',10,1,'2024-01-31 21:59:14.165226');
INSERT INTO django_admin_log VALUES(45,'5','Rayan Salah Hag Ali and Neamat El Gayar. 2019. Sentiment analysis using unla- beled email data. In 2019 International Conference on Computational Intelligence and Knowledge Economy (ICCIKE). IEEE, 328',3,'',7,1,'2024-01-31 21:59:21.633136');
INSERT INTO django_admin_log VALUES(46,'7','Ali Shafigh Aski and Navid Khalilzadeh Sourati. 2016. Proposed efficient algo- rithm to filter spam using machine learning techniques. Pacific Science Review A: Natural Science and Engineering 18, 2 (',3,'',7,1,'2024-01-31 22:03:59.730296');
INSERT INTO django_admin_log VALUES(47,'6','Rayan Salah Hag Ali and Neamat El Gayar. 2019. Sentiment analysis using unla- beled email data. In 2019 International Conference on Computational Intelligence and Knowledge Economy (ICCIKE). IEEE, 328',3,'',7,1,'2024-01-31 22:03:59.746853');
INSERT INTO django_admin_log VALUES(48,'11','Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model',3,'',10,1,'2024-01-31 22:04:07.037382');
INSERT INTO django_admin_log VALUES(49,'10','Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model',3,'',10,1,'2024-01-31 22:04:07.055231');
INSERT INTO django_admin_log VALUES(50,'12','Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model',3,'',10,1,'2024-01-31 22:05:58.375310');
INSERT INTO django_admin_log VALUES(51,'9','Ali Shafigh Aski and Navid Khalilzadeh Sourati. 2016. Proposed efficient algo- rithm to filter spam using machine learning techniques. Pacific Science Review A: Natural Science and Engineering 18, 2 (',3,'',7,1,'2024-01-31 22:06:07.098528');
INSERT INTO django_admin_log VALUES(52,'8','Rayan Salah Hag Ali and Neamat El Gayar. 2019. Sentiment analysis using unla- beled email data. In 2019 International Conference on Computational Intelligence and Knowledge Economy (ICCIKE). IEEE, 328',3,'',7,1,'2024-01-31 22:06:07.116458');
INSERT INTO django_admin_log VALUES(53,'13','Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model',3,'',10,1,'2024-01-31 22:13:53.892802');
INSERT INTO django_admin_log VALUES(54,'11','Ali Shafigh Aski and Navid Khalilzadeh Sourati. 2016. Proposed efficient algo- rithm to filter spam using machine learning techniques. Pacific Science Review A: Natural Science and Engineering 18, 2 (',3,'',7,1,'2024-01-31 22:14:12.916389');
INSERT INTO django_admin_log VALUES(55,'10','Rayan Salah Hag Ali and Neamat El Gayar. 2019. Sentiment analysis using unla- beled email data. In 2019 International Conference on Computational Intelligence and Knowledge Economy (ICCIKE). IEEE, 328',3,'',7,1,'2024-01-31 22:14:12.928287');
INSERT INTO django_admin_log VALUES(56,'8','Mainak Adhikari',3,'',9,1,'2024-01-31 22:14:28.015468');
INSERT INTO django_admin_log VALUES(57,'7','Khushbu Doulani',3,'',9,1,'2024-01-31 22:14:28.028224');
INSERT INTO django_admin_log VALUES(58,'6','Shivangi Sachan',3,'',9,1,'2024-01-31 22:14:28.036648');
INSERT INTO django_admin_log VALUES(59,'22','Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model',3,'',10,1,'2024-02-04 17:02:53.106920');
INSERT INTO django_admin_log VALUES(60,'21','Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model',3,'',10,1,'2024-02-04 17:02:53.119394');
INSERT INTO django_admin_log VALUES(61,'20','Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model',3,'',10,1,'2024-02-04 17:02:53.128817');
INSERT INTO django_admin_log VALUES(62,'19','Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model',3,'',10,1,'2024-02-04 17:02:53.140702');
INSERT INTO django_admin_log VALUES(63,'18','Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model',3,'',10,1,'2024-02-04 17:02:53.149974');
INSERT INTO django_admin_log VALUES(64,'25','Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model',3,'',10,1,'2024-02-04 18:36:30.567761');
INSERT INTO django_admin_log VALUES(65,'24','Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model',3,'',10,1,'2024-02-04 18:36:30.579986');
INSERT INTO django_admin_log VALUES(66,'23','Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model',3,'',10,1,'2024-02-04 18:36:30.590292');
INSERT INTO django_admin_log VALUES(67,'31','Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model',3,'',10,1,'2024-02-04 23:16:35.818105');
INSERT INTO django_admin_log VALUES(68,'30','Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model',3,'',10,1,'2024-02-04 23:16:35.852037');
INSERT INTO django_admin_log VALUES(69,'29','Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model',3,'',10,1,'2024-02-04 23:16:35.869624');
INSERT INTO django_admin_log VALUES(70,'28','Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model',3,'',10,1,'2024-02-04 23:16:35.878013');
INSERT INTO django_admin_log VALUES(71,'27','Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model',3,'',10,1,'2024-02-04 23:16:35.892973');
INSERT INTO django_admin_log VALUES(72,'26','Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model',3,'',10,1,'2024-02-04 23:16:35.909324');
INSERT INTO django_admin_log VALUES(73,'16','Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model',3,'',10,1,'2024-02-05 10:32:29.271565');
INSERT INTO django_admin_log VALUES(74,'14','Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model',3,'',10,1,'2024-02-05 10:32:29.287594');
INSERT INTO django_admin_log VALUES(75,'7','teeest12',3,'',10,1,'2024-02-05 20:56:36.523167');
INSERT INTO django_admin_log VALUES(76,'6','nadaaaaaaa',3,'',10,1,'2024-02-05 20:56:36.547711');
INSERT INTO django_admin_log VALUES(77,'5','Teeeeest',3,'',10,1,'2024-02-05 20:56:36.555724');
CREATE TABLE IF NOT EXISTS "django_content_type" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "app_label" varchar(100) NOT NULL, "model" varchar(100) NOT NULL);
INSERT INTO django_content_type VALUES(1,'admin','logentry');
INSERT INTO django_content_type VALUES(2,'auth','permission');
INSERT INTO django_content_type VALUES(3,'auth','group');
INSERT INTO django_content_type VALUES(4,'auth','user');
INSERT INTO django_content_type VALUES(5,'contenttypes','contenttype');
INSERT INTO django_content_type VALUES(6,'sessions','session');
INSERT INTO django_content_type VALUES(7,'articleApp','reference');
INSERT INTO django_content_type VALUES(8,'articleApp','institution');
INSERT INTO django_content_type VALUES(9,'articleApp','auteur');
INSERT INTO django_content_type VALUES(10,'articleApp','article');
INSERT INTO django_content_type VALUES(11,'accountsApp','moderateur');
INSERT INTO django_content_type VALUES(12,'accountsApp','utilisateur');
INSERT INTO django_content_type VALUES(13,'accountsApp','administrateur');
INSERT INTO django_content_type VALUES(14,'articleApp','elasticdemo');
CREATE TABLE IF NOT EXISTS "auth_permission" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "content_type_id" integer NOT NULL REFERENCES "django_content_type" ("id") DEFERRABLE INITIALLY DEFERRED, "codename" varchar(100) NOT NULL, "name" varchar(255) NOT NULL);
INSERT INTO auth_permission VALUES(1,1,'add_logentry','Can add log entry');
INSERT INTO auth_permission VALUES(2,1,'change_logentry','Can change log entry');
INSERT INTO auth_permission VALUES(3,1,'delete_logentry','Can delete log entry');
INSERT INTO auth_permission VALUES(4,1,'view_logentry','Can view log entry');
INSERT INTO auth_permission VALUES(5,2,'add_permission','Can add permission');
INSERT INTO auth_permission VALUES(6,2,'change_permission','Can change permission');
INSERT INTO auth_permission VALUES(7,2,'delete_permission','Can delete permission');
INSERT INTO auth_permission VALUES(8,2,'view_permission','Can view permission');
INSERT INTO auth_permission VALUES(9,3,'add_group','Can add group');
INSERT INTO auth_permission VALUES(10,3,'change_group','Can change group');
INSERT INTO auth_permission VALUES(11,3,'delete_group','Can delete group');
INSERT INTO auth_permission VALUES(12,3,'view_group','Can view group');
INSERT INTO auth_permission VALUES(13,4,'add_user','Can add user');
INSERT INTO auth_permission VALUES(14,4,'change_user','Can change user');
INSERT INTO auth_permission VALUES(15,4,'delete_user','Can delete user');
INSERT INTO auth_permission VALUES(16,4,'view_user','Can view user');
INSERT INTO auth_permission VALUES(17,5,'add_contenttype','Can add content type');
INSERT INTO auth_permission VALUES(18,5,'change_contenttype','Can change content type');
INSERT INTO auth_permission VALUES(19,5,'delete_contenttype','Can delete content type');
INSERT INTO auth_permission VALUES(20,5,'view_contenttype','Can view content type');
INSERT INTO auth_permission VALUES(21,6,'add_session','Can add session');
INSERT INTO auth_permission VALUES(22,6,'change_session','Can change session');
INSERT INTO auth_permission VALUES(23,6,'delete_session','Can delete session');
INSERT INTO auth_permission VALUES(24,6,'view_session','Can view session');
INSERT INTO auth_permission VALUES(25,7,'add_reference','Can add reference');
INSERT INTO auth_permission VALUES(26,7,'change_reference','Can change reference');
INSERT INTO auth_permission VALUES(27,7,'delete_reference','Can delete reference');
INSERT INTO auth_permission VALUES(28,7,'view_reference','Can view reference');
INSERT INTO auth_permission VALUES(29,8,'add_institution','Can add institution');
INSERT INTO auth_permission VALUES(30,8,'change_institution','Can change institution');
INSERT INTO auth_permission VALUES(31,8,'delete_institution','Can delete institution');
INSERT INTO auth_permission VALUES(32,8,'view_institution','Can view institution');
INSERT INTO auth_permission VALUES(33,9,'add_auteur','Can add auteur');
INSERT INTO auth_permission VALUES(34,9,'change_auteur','Can change auteur');
INSERT INTO auth_permission VALUES(35,9,'delete_auteur','Can delete auteur');
INSERT INTO auth_permission VALUES(36,9,'view_auteur','Can view auteur');
INSERT INTO auth_permission VALUES(37,10,'add_article','Can add article');
INSERT INTO auth_permission VALUES(38,10,'change_article','Can change article');
INSERT INTO auth_permission VALUES(39,10,'delete_article','Can delete article');
INSERT INTO auth_permission VALUES(40,10,'view_article','Can view article');
INSERT INTO auth_permission VALUES(41,11,'add_moderateur','Can add moderateur');
INSERT INTO auth_permission VALUES(42,11,'change_moderateur','Can change moderateur');
INSERT INTO auth_permission VALUES(43,11,'delete_moderateur','Can delete moderateur');
INSERT INTO auth_permission VALUES(44,11,'view_moderateur','Can view moderateur');
INSERT INTO auth_permission VALUES(45,12,'add_utilisateur','Can add utilisateur');
INSERT INTO auth_permission VALUES(46,12,'change_utilisateur','Can change utilisateur');
INSERT INTO auth_permission VALUES(47,12,'delete_utilisateur','Can delete utilisateur');
INSERT INTO auth_permission VALUES(48,12,'view_utilisateur','Can view utilisateur');
INSERT INTO auth_permission VALUES(49,13,'add_administrateur','Can add administrateur');
INSERT INTO auth_permission VALUES(50,13,'change_administrateur','Can change administrateur');
INSERT INTO auth_permission VALUES(51,13,'delete_administrateur','Can delete administrateur');
INSERT INTO auth_permission VALUES(52,13,'view_administrateur','Can view administrateur');
INSERT INTO auth_permission VALUES(53,14,'add_elasticdemo','Can add elastic demo');
INSERT INTO auth_permission VALUES(54,14,'change_elasticdemo','Can change elastic demo');
INSERT INTO auth_permission VALUES(55,14,'delete_elasticdemo','Can delete elastic demo');
INSERT INTO auth_permission VALUES(56,14,'view_elasticdemo','Can view elastic demo');
CREATE TABLE IF NOT EXISTS "auth_group" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "name" varchar(150) NOT NULL UNIQUE);
CREATE TABLE IF NOT EXISTS "auth_user" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "password" varchar(128) NOT NULL, "last_login" datetime NULL, "is_superuser" bool NOT NULL, "username" varchar(150) NOT NULL UNIQUE, "last_name" varchar(150) NOT NULL, "email" varchar(254) NOT NULL, "is_staff" bool NOT NULL, "is_active" bool NOT NULL, "date_joined" datetime NOT NULL, "first_name" varchar(150) NOT NULL);
INSERT INTO auth_user VALUES(1,'pbkdf2_sha256$720000$D1BfvGXdUGcVVkV5u0cLS3$kR7VBmGI9FI6OXFe8G/T6QTE2eNrdN8CgbydxeEB2ZE=','2024-02-01 16:17:26.733102',1,'Administrateur','','admin@gmail.com',1,1,'2023-12-15 21:48:15.497697','');
INSERT INTO auth_user VALUES(23,'pbkdf2_sha256$720000$chZNUC95wrllZuc7gmF3KJ$9Y6MWVxXxbY6c0A5N4DJjF6E6g84E1Iu17f+lzJtP+o=',NULL,0,'nada@gmail.com','Kouadri','nada@gmail.com',0,1,'2023-12-28 18:40:35.704559','Nada');
INSERT INTO auth_user VALUES(24,'pbkdf2_sha256$720000$ABthfRPsYSHdnDDAAxIgbe$y8S3vG2g5copQHCw6Y86ALBs8E5gQjIkbptM63KTS0U=',NULL,0,'ikram@gmail.com','debbih','ikram@gmail.com',0,1,'2023-12-28 18:40:57.255457','ikram');
INSERT INTO auth_user VALUES(25,'pbkdf2_sha256$720000$vgD8hbHEnPJzfCtjAYCfLR$m49VNSUEfRkcinCIaLZLbdP8VMCFXjwJps3GiWwx4a4=',NULL,0,'hadjer@gmail.com','bouzara','hadjer@gmail.com',0,1,'2023-12-28 18:41:19.182814','hadjer');
INSERT INTO auth_user VALUES(26,'pbkdf2_sha256$720000$vUHpFb6hOEQwikoF9HSsSy$fFHFuQYO1Lm4OVTPFheynTXxscNc/YQLd+cIWVhYv0U=',NULL,0,'Nada@gmail.com','Kouadri','Nada@gmail.com',0,1,'2024-01-31 07:26:51.489355','Nada');
INSERT INTO auth_user VALUES(27,'pbkdf2_sha256$720000$lKP6p04oEZtwrhcx699M9O$Ka8IK5mEoF+cRB0KCIKmJRM3w+8LobWVzBjMvwqD2Ys=',NULL,0,'Nadaaaaa@gmail.com','Kouadri','Nadaaaaa@gmail.com',0,1,'2024-02-04 16:32:18.218211','Nada');
INSERT INTO auth_user VALUES(28,'pbkdf2_sha256$720000$nQJUEPmRNl21ntRCre0BWT$mwk6nLrpWvc5vuZhGfY7TBLTsnUgtgVfUckvae0K434=',NULL,0,'ln_kouadri@esi.dzz','Kouadri','ln_kouadri@esi.dzz',0,1,'2024-02-05 17:20:01.685350','Nada');
INSERT INTO auth_user VALUES(29,'pbkdf2_sha256$720000$VY2U9Nnx7wkQU4JLz9mxPX$KxEUPVnq3/hfGF3kcWhFNLCUTTywoLKdg4AqGzu+pqw=',NULL,0,'test@gmail.com','test','test@gmail.com',0,1,'2024-02-05 17:21:15.113817','test');
CREATE TABLE IF NOT EXISTS "django_session" ("session_key" varchar(40) NOT NULL PRIMARY KEY, "session_data" text NOT NULL, "expire_date" datetime NOT NULL);
INSERT INTO django_session VALUES('89rwba6gr5tbwlz39uyadmij93886ta5','.eJxVjDEOwjAMRe-SGUVposaFkZ0zRLZjkwJKpaadKu4OlTrA-t97fzMJ16WktcmcxmwupjOn342Qn1J3kB9Y75PlqS7zSHZX7EGbvU1ZXtfD_Tso2Mq39q4b-kGCsuZMrB5jUJfFkYSIiGdkACEOBKoOPPoI4j06FSKG3rw_HY05mw:1rEG3C:TsYGJWNEXeCRegQsbdQuKu5Nux_IVfbDWr32acY_FG0','2023-12-29 21:48:54.106347');
INSERT INTO django_session VALUES('a9lh11wsosfoszo1o97tg3lu40aq8qsq','.eJxVjEEOwiAQRe_C2hCgUMCle89AZpipVA0kpV0Z765NutDtf-_9l0iwrSVtnZc0kziLKE6_G0J-cN0B3aHemsytrsuMclfkQbu8NuLn5XD_Dgr08q1BcUYbQ2RvUGmd2TgLkwswqeD8wOg5aGUIshq0GzyiJT-6kUwOZEG8P-3DN_w:1rEVHJ:DFcHiqySzkqHrVKc6iJwc-QnACzi_qiVtENyrP_VqHs','2023-12-30 14:04:29.750424');
INSERT INTO django_session VALUES('l0qk997t9s1qhs416wd4k5cndec3uas5','.eJxVjDEOwjAMRe-SGUVposaFkZ0zRLZjkwJKpaadKu4OlTrA-t97fzMJ16WktcmcxmwupjOn342Qn1J3kB9Y75PlqS7zSHZX7EGbvU1ZXtfD_Tso2Mq39q4b-kGCsuZMrB5jUJfFkYSIiGdkACEOBKoOPPoI4j06FSKG3rw_HY05mw:1rHONR:HpalhSuIeaLHL_igrOZW4DQpYtc51z6acn8XyuAkC_k','2024-01-07 13:18:45.970896');
INSERT INTO django_session VALUES('nzj81prdpxrtcob7makt46pyc11psoqs','.eJxVjDEOwjAMRe-SGUVposaFkZ0zRLZjkwJKpaadKu4OlTrA-t97fzMJ16WktcmcxmwupjOn342Qn1J3kB9Y75PlqS7zSHZX7EGbvU1ZXtfD_Tso2Mq39q4b-kGCsuZMrB5jUJfFkYSIiGdkACEOBKoOPPoI4j06FSKG3rw_HY05mw:1rHr6D:3QtPm8fn2Y91-VPDv1j2dDUfLIy-uhY2uJsM9ADEJrk','2024-01-08 19:58:53.210987');
INSERT INTO django_session VALUES('w6isbdn8aigaf6wa6rffvbue1nbahskm','.eJxVjDEOwjAMRe-SGUVposaFkZ0zRLZjkwJKpaadKu4OlTrA-t97fzMJ16WktcmcxmwupjOn342Qn1J3kB9Y75PlqS7zSHZX7EGbvU1ZXtfD_Tso2Mq39q4b-kGCsuZMrB5jUJfFkYSIiGdkACEOBKoOPPoI4j06FSKG3rw_HY05mw:1rNzwg:1wCNA3m_SkKMbJaQych1VUCVKYcxHyEDUGrjOhVFIyg','2024-01-25 18:38:26.439029');
INSERT INTO django_session VALUES('vge0k9sehmgk6md3naxo84fzr04dybp3','.eJxVjDEOwjAMRe-SGUVposaFkZ0zRLZjkwJKpaadKu4OlTrA-t97fzMJ16WktcmcxmwupjOn342Qn1J3kB9Y75PlqS7zSHZX7EGbvU1ZXtfD_Tso2Mq39q4b-kGCsuZMrB5jUJfFkYSIiGdkACEOBKoOPPoI4j06FSKG3rw_HY05mw:1rUxJW:wTHxAkvSrc59kleX_LoyaUEMb_j5LcoIhWWYlf4EbX0','2024-02-13 23:14:46.036259');
INSERT INTO django_session VALUES('fsut666is7wuf39eryz6bq2gno7npz9t','.eJxVjDEOwjAMRe-SGUVposaFkZ0zRLZjkwJKpaadKu4OlTrA-t97fzMJ16WktcmcxmwupjOn342Qn1J3kB9Y75PlqS7zSHZX7EGbvU1ZXtfD_Tso2Mq39q4b-kGCsuZMrB5jUJfFkYSIiGdkACEOBKoOPPoI4j06FSKG3rw_HY05mw:1rVZkk:8LfHagMcqE1WNR81suheNq_dw-mbGkzNRUB78dzrzWk','2024-02-15 16:17:26.741087');
CREATE TABLE IF NOT EXISTS "articleApp_article_auteurs" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "article_id" bigint NOT NULL REFERENCES "articleApp_article" ("id") DEFERRABLE INITIALLY DEFERRED, "auteur_id" bigint NOT NULL REFERENCES "articleApp_auteur" ("id") DEFERRABLE INITIALLY DEFERRED);
INSERT INTO articleApp_article_auteurs VALUES(1,1,1);
INSERT INTO articleApp_article_auteurs VALUES(2,1,5);
INSERT INTO articleApp_article_auteurs VALUES(4,3,3);
INSERT INTO articleApp_article_auteurs VALUES(17,4,12);
INSERT INTO articleApp_article_auteurs VALUES(18,15,13);
INSERT INTO articleApp_article_auteurs VALUES(19,15,14);
INSERT INTO articleApp_article_auteurs VALUES(47,32,9);
INSERT INTO articleApp_article_auteurs VALUES(48,32,10);
INSERT INTO articleApp_article_auteurs VALUES(49,32,11);
INSERT INTO articleApp_article_auteurs VALUES(50,33,9);
INSERT INTO articleApp_article_auteurs VALUES(51,33,10);
INSERT INTO articleApp_article_auteurs VALUES(52,33,11);
INSERT INTO articleApp_article_auteurs VALUES(53,34,13);
INSERT INTO articleApp_article_auteurs VALUES(54,34,14);
INSERT INTO articleApp_article_auteurs VALUES(55,36,16);
INSERT INTO articleApp_article_auteurs VALUES(56,37,17);
INSERT INTO articleApp_article_auteurs VALUES(57,37,18);
INSERT INTO articleApp_article_auteurs VALUES(58,37,19);
CREATE TABLE IF NOT EXISTS "articleApp_article_references" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "article_id" bigint NOT NULL REFERENCES "articleApp_article" ("id") DEFERRABLE INITIALLY DEFERRED, "reference_id" bigint NOT NULL REFERENCES "articleApp_reference" ("id") DEFERRABLE INITIALLY DEFERRED);
INSERT INTO articleApp_article_references VALUES(1,1,3);
INSERT INTO articleApp_article_references VALUES(3,3,4);
INSERT INTO articleApp_article_references VALUES(4,4,2);
INSERT INTO articleApp_article_references VALUES(37,15,35);
INSERT INTO articleApp_article_references VALUES(38,15,36);
INSERT INTO articleApp_article_references VALUES(39,15,37);
INSERT INTO articleApp_article_references VALUES(40,15,38);
INSERT INTO articleApp_article_references VALUES(41,15,39);
INSERT INTO articleApp_article_references VALUES(42,15,40);
INSERT INTO articleApp_article_references VALUES(43,15,41);
INSERT INTO articleApp_article_references VALUES(44,15,42);
INSERT INTO articleApp_article_references VALUES(45,15,43);
INSERT INTO articleApp_article_references VALUES(46,15,44);
INSERT INTO articleApp_article_references VALUES(47,15,45);
INSERT INTO articleApp_article_references VALUES(48,15,46);
INSERT INTO articleApp_article_references VALUES(49,15,47);
INSERT INTO articleApp_article_references VALUES(50,15,48);
INSERT INTO articleApp_article_references VALUES(51,15,49);
INSERT INTO articleApp_article_references VALUES(75,17,50);
INSERT INTO articleApp_article_references VALUES(260,32,12);
INSERT INTO articleApp_article_references VALUES(261,32,13);
INSERT INTO articleApp_article_references VALUES(262,32,14);
INSERT INTO articleApp_article_references VALUES(263,32,15);
INSERT INTO articleApp_article_references VALUES(264,32,16);
INSERT INTO articleApp_article_references VALUES(265,32,17);
INSERT INTO articleApp_article_references VALUES(266,32,18);
INSERT INTO articleApp_article_references VALUES(267,32,19);
INSERT INTO articleApp_article_references VALUES(268,32,20);
INSERT INTO articleApp_article_references VALUES(269,32,21);
INSERT INTO articleApp_article_references VALUES(270,32,22);
INSERT INTO articleApp_article_references VALUES(271,32,23);
INSERT INTO articleApp_article_references VALUES(272,32,24);
INSERT INTO articleApp_article_references VALUES(273,32,25);
INSERT INTO articleApp_article_references VALUES(274,32,26);
INSERT INTO articleApp_article_references VALUES(275,32,27);
INSERT INTO articleApp_article_references VALUES(276,32,28);
INSERT INTO articleApp_article_references VALUES(277,32,29);
INSERT INTO articleApp_article_references VALUES(278,32,30);
INSERT INTO articleApp_article_references VALUES(279,32,31);
INSERT INTO articleApp_article_references VALUES(280,32,32);
INSERT INTO articleApp_article_references VALUES(281,32,33);
INSERT INTO articleApp_article_references VALUES(282,32,34);
INSERT INTO articleApp_article_references VALUES(283,33,12);
INSERT INTO articleApp_article_references VALUES(284,33,13);
INSERT INTO articleApp_article_references VALUES(285,33,14);
INSERT INTO articleApp_article_references VALUES(286,33,15);
INSERT INTO articleApp_article_references VALUES(287,33,16);
INSERT INTO articleApp_article_references VALUES(288,33,17);
INSERT INTO articleApp_article_references VALUES(289,33,18);
INSERT INTO articleApp_article_references VALUES(290,33,19);
INSERT INTO articleApp_article_references VALUES(291,33,20);
INSERT INTO articleApp_article_references VALUES(292,33,21);
INSERT INTO articleApp_article_references VALUES(293,33,22);
INSERT INTO articleApp_article_references VALUES(294,33,23);
INSERT INTO articleApp_article_references VALUES(295,33,24);
INSERT INTO articleApp_article_references VALUES(296,33,25);
INSERT INTO articleApp_article_references VALUES(297,33,26);
INSERT INTO articleApp_article_references VALUES(298,33,27);
INSERT INTO articleApp_article_references VALUES(299,33,28);
INSERT INTO articleApp_article_references VALUES(300,33,29);
INSERT INTO articleApp_article_references VALUES(301,33,30);
INSERT INTO articleApp_article_references VALUES(302,33,31);
INSERT INTO articleApp_article_references VALUES(303,33,32);
INSERT INTO articleApp_article_references VALUES(304,33,33);
INSERT INTO articleApp_article_references VALUES(305,33,34);
INSERT INTO articleApp_article_references VALUES(306,34,35);
INSERT INTO articleApp_article_references VALUES(307,34,36);
INSERT INTO articleApp_article_references VALUES(308,34,37);
INSERT INTO articleApp_article_references VALUES(309,34,38);
INSERT INTO articleApp_article_references VALUES(310,34,39);
INSERT INTO articleApp_article_references VALUES(311,34,40);
INSERT INTO articleApp_article_references VALUES(312,34,41);
INSERT INTO articleApp_article_references VALUES(313,34,42);
INSERT INTO articleApp_article_references VALUES(314,34,43);
INSERT INTO articleApp_article_references VALUES(315,34,44);
INSERT INTO articleApp_article_references VALUES(316,34,45);
INSERT INTO articleApp_article_references VALUES(317,34,46);
INSERT INTO articleApp_article_references VALUES(318,34,47);
INSERT INTO articleApp_article_references VALUES(319,34,48);
INSERT INTO articleApp_article_references VALUES(320,34,49);
INSERT INTO articleApp_article_references VALUES(321,35,51);
INSERT INTO articleApp_article_references VALUES(322,35,52);
INSERT INTO articleApp_article_references VALUES(323,35,53);
INSERT INTO articleApp_article_references VALUES(324,35,54);
INSERT INTO articleApp_article_references VALUES(325,35,55);
INSERT INTO articleApp_article_references VALUES(326,35,56);
INSERT INTO articleApp_article_references VALUES(327,35,57);
INSERT INTO articleApp_article_references VALUES(328,35,58);
INSERT INTO articleApp_article_references VALUES(329,35,59);
INSERT INTO articleApp_article_references VALUES(330,35,60);
INSERT INTO articleApp_article_references VALUES(331,35,61);
INSERT INTO articleApp_article_references VALUES(332,35,62);
INSERT INTO articleApp_article_references VALUES(333,35,63);
INSERT INTO articleApp_article_references VALUES(334,35,64);
INSERT INTO articleApp_article_references VALUES(335,35,65);
INSERT INTO articleApp_article_references VALUES(336,35,66);
INSERT INTO articleApp_article_references VALUES(337,35,67);
INSERT INTO articleApp_article_references VALUES(338,35,68);
INSERT INTO articleApp_article_references VALUES(339,35,69);
INSERT INTO articleApp_article_references VALUES(340,35,70);
INSERT INTO articleApp_article_references VALUES(341,36,71);
INSERT INTO articleApp_article_references VALUES(342,36,72);
INSERT INTO articleApp_article_references VALUES(343,36,73);
INSERT INTO articleApp_article_references VALUES(344,36,74);
INSERT INTO articleApp_article_references VALUES(345,36,75);
INSERT INTO articleApp_article_references VALUES(346,36,76);
INSERT INTO articleApp_article_references VALUES(347,36,77);
INSERT INTO articleApp_article_references VALUES(348,36,78);
INSERT INTO articleApp_article_references VALUES(349,36,79);
INSERT INTO articleApp_article_references VALUES(350,36,80);
INSERT INTO articleApp_article_references VALUES(351,36,81);
INSERT INTO articleApp_article_references VALUES(352,36,82);
INSERT INTO articleApp_article_references VALUES(353,36,83);
INSERT INTO articleApp_article_references VALUES(354,36,84);
INSERT INTO articleApp_article_references VALUES(355,36,85);
INSERT INTO articleApp_article_references VALUES(356,36,86);
INSERT INTO articleApp_article_references VALUES(357,36,87);
INSERT INTO articleApp_article_references VALUES(358,36,88);
INSERT INTO articleApp_article_references VALUES(359,36,89);
INSERT INTO articleApp_article_references VALUES(360,36,90);
INSERT INTO articleApp_article_references VALUES(361,36,91);
INSERT INTO articleApp_article_references VALUES(362,36,92);
INSERT INTO articleApp_article_references VALUES(363,36,93);
INSERT INTO articleApp_article_references VALUES(364,36,94);
INSERT INTO articleApp_article_references VALUES(365,36,95);
INSERT INTO articleApp_article_references VALUES(366,36,96);
INSERT INTO articleApp_article_references VALUES(367,36,97);
INSERT INTO articleApp_article_references VALUES(368,36,98);
INSERT INTO articleApp_article_references VALUES(369,36,99);
INSERT INTO articleApp_article_references VALUES(370,36,100);
INSERT INTO articleApp_article_references VALUES(371,36,101);
INSERT INTO articleApp_article_references VALUES(372,36,102);
INSERT INTO articleApp_article_references VALUES(373,36,103);
INSERT INTO articleApp_article_references VALUES(374,36,104);
INSERT INTO articleApp_article_references VALUES(375,36,105);
INSERT INTO articleApp_article_references VALUES(376,36,106);
INSERT INTO articleApp_article_references VALUES(377,36,107);
INSERT INTO articleApp_article_references VALUES(378,36,108);
INSERT INTO articleApp_article_references VALUES(379,36,109);
INSERT INTO articleApp_article_references VALUES(380,36,110);
INSERT INTO articleApp_article_references VALUES(381,36,111);
INSERT INTO articleApp_article_references VALUES(382,36,112);
INSERT INTO articleApp_article_references VALUES(383,36,113);
INSERT INTO articleApp_article_references VALUES(384,36,114);
INSERT INTO articleApp_article_references VALUES(385,36,115);
INSERT INTO articleApp_article_references VALUES(386,36,116);
INSERT INTO articleApp_article_references VALUES(387,36,117);
INSERT INTO articleApp_article_references VALUES(388,36,118);
INSERT INTO articleApp_article_references VALUES(389,36,119);
INSERT INTO articleApp_article_references VALUES(390,36,120);
INSERT INTO articleApp_article_references VALUES(391,36,121);
INSERT INTO articleApp_article_references VALUES(392,36,122);
INSERT INTO articleApp_article_references VALUES(393,36,123);
INSERT INTO articleApp_article_references VALUES(394,36,124);
INSERT INTO articleApp_article_references VALUES(395,36,125);
INSERT INTO articleApp_article_references VALUES(396,36,126);
INSERT INTO articleApp_article_references VALUES(397,36,127);
INSERT INTO articleApp_article_references VALUES(398,36,128);
INSERT INTO articleApp_article_references VALUES(399,36,129);
INSERT INTO articleApp_article_references VALUES(400,36,130);
INSERT INTO articleApp_article_references VALUES(401,37,131);
INSERT INTO articleApp_article_references VALUES(402,37,132);
INSERT INTO articleApp_article_references VALUES(403,37,133);
INSERT INTO articleApp_article_references VALUES(404,37,134);
INSERT INTO articleApp_article_references VALUES(405,37,135);
INSERT INTO articleApp_article_references VALUES(406,37,136);
INSERT INTO articleApp_article_references VALUES(407,37,137);
INSERT INTO articleApp_article_references VALUES(408,37,138);
INSERT INTO articleApp_article_references VALUES(409,37,139);
INSERT INTO articleApp_article_references VALUES(410,37,140);
INSERT INTO articleApp_article_references VALUES(411,37,141);
INSERT INTO articleApp_article_references VALUES(412,37,142);
INSERT INTO articleApp_article_references VALUES(413,37,143);
INSERT INTO articleApp_article_references VALUES(414,37,144);
INSERT INTO articleApp_article_references VALUES(415,37,145);
INSERT INTO articleApp_article_references VALUES(416,37,146);
INSERT INTO articleApp_article_references VALUES(417,37,147);
INSERT INTO articleApp_article_references VALUES(418,37,148);
INSERT INTO articleApp_article_references VALUES(419,37,149);
INSERT INTO articleApp_article_references VALUES(420,37,150);
INSERT INTO articleApp_article_references VALUES(421,37,151);
INSERT INTO articleApp_article_references VALUES(422,37,152);
INSERT INTO articleApp_article_references VALUES(423,37,153);
INSERT INTO articleApp_article_references VALUES(424,37,154);
INSERT INTO articleApp_article_references VALUES(425,37,155);
INSERT INTO articleApp_article_references VALUES(426,37,156);
INSERT INTO articleApp_article_references VALUES(427,37,157);
INSERT INTO articleApp_article_references VALUES(428,37,158);
INSERT INTO articleApp_article_references VALUES(429,37,159);
INSERT INTO articleApp_article_references VALUES(430,37,160);
INSERT INTO articleApp_article_references VALUES(431,37,161);
INSERT INTO articleApp_article_references VALUES(432,37,162);
INSERT INTO articleApp_article_references VALUES(433,37,163);
INSERT INTO articleApp_article_references VALUES(434,37,164);
INSERT INTO articleApp_article_references VALUES(435,37,165);
INSERT INTO articleApp_article_references VALUES(436,37,166);
INSERT INTO articleApp_article_references VALUES(437,37,167);
INSERT INTO articleApp_article_references VALUES(438,37,168);
INSERT INTO articleApp_article_references VALUES(439,37,169);
INSERT INTO articleApp_article_references VALUES(440,37,170);
INSERT INTO articleApp_article_references VALUES(441,37,171);
INSERT INTO articleApp_article_references VALUES(442,37,172);
INSERT INTO articleApp_article_references VALUES(443,37,173);
INSERT INTO articleApp_article_references VALUES(444,37,174);
INSERT INTO articleApp_article_references VALUES(445,37,175);
INSERT INTO articleApp_article_references VALUES(446,37,176);
INSERT INTO articleApp_article_references VALUES(447,37,177);
INSERT INTO articleApp_article_references VALUES(448,37,178);
INSERT INTO articleApp_article_references VALUES(449,37,179);
INSERT INTO articleApp_article_references VALUES(450,37,180);
INSERT INTO articleApp_article_references VALUES(451,37,181);
INSERT INTO articleApp_article_references VALUES(452,37,182);
INSERT INTO articleApp_article_references VALUES(453,37,183);
INSERT INTO articleApp_article_references VALUES(454,37,184);
INSERT INTO articleApp_article_references VALUES(455,37,185);
INSERT INTO articleApp_article_references VALUES(456,37,186);
INSERT INTO articleApp_article_references VALUES(457,37,187);
INSERT INTO articleApp_article_references VALUES(458,37,188);
INSERT INTO articleApp_article_references VALUES(459,37,189);
INSERT INTO articleApp_article_references VALUES(460,37,190);
INSERT INTO articleApp_article_references VALUES(461,37,191);
INSERT INTO articleApp_article_references VALUES(462,37,192);
INSERT INTO articleApp_article_references VALUES(463,37,193);
INSERT INTO articleApp_article_references VALUES(464,37,194);
INSERT INTO articleApp_article_references VALUES(465,37,195);
INSERT INTO articleApp_article_references VALUES(466,37,196);
INSERT INTO articleApp_article_references VALUES(467,37,197);
INSERT INTO articleApp_article_references VALUES(468,37,198);
INSERT INTO articleApp_article_references VALUES(469,37,199);
INSERT INTO articleApp_article_references VALUES(470,37,200);
INSERT INTO articleApp_article_references VALUES(471,37,201);
INSERT INTO articleApp_article_references VALUES(472,37,202);
INSERT INTO articleApp_article_references VALUES(473,37,203);
INSERT INTO articleApp_article_references VALUES(474,37,204);
INSERT INTO articleApp_article_references VALUES(475,37,205);
INSERT INTO articleApp_article_references VALUES(476,37,206);
INSERT INTO articleApp_article_references VALUES(477,37,207);
INSERT INTO articleApp_article_references VALUES(478,37,208);
INSERT INTO articleApp_article_references VALUES(479,37,209);
CREATE TABLE IF NOT EXISTS "accountsApp_administrateur" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "user_id" integer NOT NULL UNIQUE REFERENCES "auth_user" ("id") DEFERRABLE INITIALLY DEFERRED);
CREATE TABLE IF NOT EXISTS "accountsApp_moderateur" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "user_id" integer NOT NULL UNIQUE REFERENCES "auth_user" ("id") DEFERRABLE INITIALLY DEFERRED);
INSERT INTO accountsApp_moderateur VALUES(18,23);
INSERT INTO accountsApp_moderateur VALUES(19,24);
INSERT INTO accountsApp_moderateur VALUES(20,25);
INSERT INTO accountsApp_moderateur VALUES(21,28);
CREATE TABLE IF NOT EXISTS "accountsApp_utilisateur" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "user_id" integer NOT NULL UNIQUE REFERENCES "auth_user" ("id") DEFERRABLE INITIALLY DEFERRED);
INSERT INTO accountsApp_utilisateur VALUES(4,1);
INSERT INTO accountsApp_utilisateur VALUES(5,26);
INSERT INTO accountsApp_utilisateur VALUES(6,27);
INSERT INTO accountsApp_utilisateur VALUES(7,29);
CREATE TABLE IF NOT EXISTS "accountsApp_utilisateur_Favoris" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "utilisateur_id" bigint NOT NULL REFERENCES "accountsApp_utilisateur" ("id") DEFERRABLE INITIALLY DEFERRED, "article_id" bigint NOT NULL REFERENCES "articleApp_article" ("id") DEFERRABLE INITIALLY DEFERRED);
INSERT INTO accountsApp_utilisateur_Favoris VALUES(1,4,4);
CREATE TABLE IF NOT EXISTS "articleApp_auteur" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "institution_id" bigint NULL REFERENCES "articleApp_institution" ("id") DEFERRABLE INITIALLY DEFERRED, "email" varchar(254) NOT NULL, "full_name" varchar(200) NOT NULL);
INSERT INTO articleApp_auteur VALUES(1,4,'haddadou@gmail.com','Hamid Haddadou');
INSERT INTO articleApp_auteur VALUES(2,1,'Koudil@gmail.com','Koudil');
INSERT INTO articleApp_auteur VALUES(3,1,'ayad@esi.dz','Ayad Khadija');
INSERT INTO articleApp_auteur VALUES(4,2,'ilhem@gmail.com','Ilham Bekkar');
INSERT INTO articleApp_auteur VALUES(5,4,'achour@gmail.com','Achour');
INSERT INTO articleApp_auteur VALUES(9,5,'mcs21025@iiitl.ac.in','Shivangi Sachan');
INSERT INTO articleApp_auteur VALUES(10,6,'khushidoulani@gmail.com','Khushbu Doulani');
INSERT INTO articleApp_auteur VALUES(11,7,'mainak.ism@gmail.com','Mainak Adhikari');
INSERT INTO articleApp_auteur VALUES(12,2,'ilhem@gmail.com','Ilham Bekkarrrrrrrrrrrr');
INSERT INTO articleApp_auteur VALUES(13,8,'vmen@ait.edu.gr','Vlado Menkovski');
INSERT INTO articleApp_auteur VALUES(14,8,'dmeta@ait.edu.gr','Dimitrios Metafas');
INSERT INTO articleApp_auteur VALUES(15,9,'Lucknow','IIIT Lucknow');
INSERT INTO articleApp_auteur VALUES(16,10,'ddbarrio@idiap.ch','del Barrio');
INSERT INTO articleApp_auteur VALUES(17,11,'smysore@cs.umass.edu','Sheshera Mysore');
INSERT INTO articleApp_auteur VALUES(18,11,'mccallum@cs.umass.edu','Andrew McCallum');
INSERT INTO articleApp_auteur VALUES(19,11,'hzamani@cs.umass.edu','Hamed Zamani');
CREATE TABLE IF NOT EXISTS "articleApp_institution" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "nom" varchar(100) NOT NULL, "adress" text NOT NULL);
INSERT INTO articleApp_institution VALUES(1,'ESI','Oued Smar Algiers Algeria');
INSERT INTO articleApp_institution VALUES(2,'ENSIA','Algiers Algeria');
INSERT INTO articleApp_institution VALUES(3,'ENS','Kouba Algiers Algeria');
INSERT INTO articleApp_institution VALUES(4,'USTHB','Bab Ezouar Algiers Algeria');
INSERT INTO articleApp_institution VALUES(5,'Department of CSE',' IIIT Lucknow, Lucknow, UP, India');
INSERT INTO articleApp_institution VALUES(6,'Vardhaman College of Engineering',' Hyderabad, India');
INSERT INTO articleApp_institution VALUES(7,'Department of CSE',' IIIT Lucknow, UP, India');
INSERT INTO articleApp_institution VALUES(8,'Athens Information Technology',' 0.8km Markopoulou Ave., Peania, 19002, Greece');
INSERT INTO articleApp_institution VALUES(9,'UP',' India');
INSERT INTO articleApp_institution VALUES(10,'Idiap Research Institute',' Switzerland');
INSERT INTO articleApp_institution VALUES(11,'University of Massachusetts Amherst',' USA');
CREATE TABLE IF NOT EXISTS "articleApp_reference" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "titre" text NOT NULL);
INSERT INTO articleApp_reference VALUES(1,'Livre Architecture des ordinateurs');
INSERT INTO articleApp_reference VALUES(2,'Basic of machine learning');
INSERT INTO articleApp_reference VALUES(3,'Optimisation et l''analyse numerique');
INSERT INTO articleApp_reference VALUES(4,'Processus sous linux system');
INSERT INTO articleApp_reference VALUES(12,'Rayan Salah Hag Ali and Neamat El Gayar. 2019. Sentiment analysis using unla- beled email data. In 2019 International Conference on Computational Intelligence and Knowledge Economy (ICCIKE). IEEE, 328–333.');
INSERT INTO articleApp_reference VALUES(13,'Ali Shafigh Aski and Navid Khalilzadeh Sourati. 2016. Proposed efficient algo- rithm to filter spam using machine learning techniques. Pacific Science Review A: Natural Science and Engineering 18, 2 (2016), 145–149.');
INSERT INTO articleApp_reference VALUES(14,'Huwaida T Elshoush and Esraa A Dinar. 2019. Using adaboost and stochastic gradient descent (sgd) algorithms with R and orange software for filtering e-mail spam. In 2019 11th Computer Science and Electronic Engineering (CEEC). IEEE, 41–46.');
INSERT INTO articleApp_reference VALUES(15,'Weimiao Feng, Jianguo Sun, Liguo Zhang, Cuiling Cao, and Qing Yang. 2016. A support vector machine based naive Bayes algorithm for spam filtering. In 2016 IEEE 35th International Performance Computing and Communications Conference (IPCCC). IEEE, 1–8.');
INSERT INTO articleApp_reference VALUES(16,'Pranjul Garg and Nancy Girdhar. 2021. A Systematic Review on Spam Filtering Techniques based on Natural Language Processing Framework. In 2021 11th Inter- national Conference on Cloud Computing, Data Science & Engineering (Confluence). IEEE, 30–35.');
INSERT INTO articleApp_reference VALUES(17,'Adam Kavon Ghazi-Tehrani and Henry N Pontell. 2021. Phishing evolves: Ana- lyzing the enduring cybercrime. Victims & Offenders 16, 3 (2021), 316–342.');
INSERT INTO articleApp_reference VALUES(18,'Radicati Group et al. 2015. Email Statistics Report 2015–2019. Radicati Group. Accessed August 13 (2015), 2019.');
INSERT INTO articleApp_reference VALUES(19,'Maryam Hina, Mohsin Ali, and Javed. 2021. Sefaced: Semantic-based forensic analysis and classification of e-mail data using deep learning. IEEE Access 9 (2021), 98398–98411.');
INSERT INTO articleApp_reference VALUES(20,'Maryam Hina, Mohsin Ali, Abdul Rehman Javed, Fahad Ghabban, Liaqat Ali Khan, and Zunera Jalil. 2021. Sefaced: Semantic-based forensic analysis and classification of e-mail data using deep learning. IEEE Access 9 (2021), 98398– 98411.');
INSERT INTO articleApp_reference VALUES(21,'Weicong Kong, Zhao Yang Dong, Youwei Jia, David J Hill, Yan Xu, and Yuan Zhang. 2017. Short-term residential load forecasting based on LSTM recurrent neural network. IEEE transactions on smart grid 10, 1 (2017), 841–851.');
INSERT INTO articleApp_reference VALUES(22,'T Kumaresan and C Palanisamy. 2017. E-mail spam classification using S-cuckoo search and support vector machine. International Journal of Bio-Inspired Compu- tation 9, 3 (2017), 142–156.');
INSERT INTO articleApp_reference VALUES(23,'Nuha H Marza, Mehdi E Manaa, and Hussein A Lafta. 2021. Classification of spam emails using deep learning. In 2021 1st Babylon International Conference on Information Technology and Science (BICITS). IEEE, 63–68.');
INSERT INTO articleApp_reference VALUES(24,'Tomas Mikolov and Geoffrey Zweig. 2012. Context dependent recurrent neural network language model. In 2012 IEEE Spoken Language Technology Workshop (SLT). IEEE, 234–239.');
INSERT INTO articleApp_reference VALUES(25,'Sarwat Nizamani, Nasrullah Memon, Mathies Glasdam, and Dong Duong Nguyen. 2014. Detection of fraudulent emails by employing advanced feature abundance. Egyptian Informatics Journal 15, 3 (2014), 169–174.');
INSERT INTO articleApp_reference VALUES(26,'V Priya, I Sumaiya Thaseen, Thippa Reddy Gadekallu, Mohamed K Aboudaif, and Emad Abouel Nasr. 2021. Robust attack detection approach for IIoT using ensemble classifier. arXiv preprint arXiv:2102.01515 (2021).');
INSERT INTO articleApp_reference VALUES(27,'Justinas Rastenis, Simona Ramanauskait˙e, Justinas Janulevičius, Antanas Čenys, Asta Slotkien˙e, and Kęstutis Pakrijauskas. 2020. E-mail-based phishing attack taxonomy. Applied Sciences 10, 7 (2020), 2363.');
INSERT INTO articleApp_reference VALUES(28,'Karthika D Renuka and P Visalakshi. 2014. Latent semantic indexing based SVM model for email spam classification. (2014).');
INSERT INTO articleApp_reference VALUES(29,'Shuvendu Roy, Sk Imran Hossain, MAH Akhand, and N Siddique. 2018. Sequence modeling for intelligent typing assistant with Bangla and English keyboard. In 2018 International Conference on Innovation in Engineering and Technology (ICIET). IEEE, 1–6.');
INSERT INTO articleApp_reference VALUES(30,'Tara N Sainath, Oriol Vinyals, Andrew Senior, and Haşim Sak. 2015. Convolu- tional, long short-term memory, fully connected deep neural networks. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). Ieee, 4580–4584.');
INSERT INTO articleApp_reference VALUES(31,'Anuj Kumar Singh, Shashi Bhushan, and Sonakshi Vij. 2019. Filtering spam messages and mails using fuzzy C means algorithm. In 2019 4th International Conference on Internet of Things: Smart Innovation and Usages (IoT-SIU). IEEE, 1–5.');
INSERT INTO articleApp_reference VALUES(32,'Kristina Toutanova and Colin Cherry. 2009. A global model for joint lemmati- zation and part-of-speech prediction. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. 486–494.');
INSERT INTO articleApp_reference VALUES(33,'Tian Xia. 2020. A constant time complexity spam detection algorithm for boosting throughput on rule-based filtering systems. IEEE Access 8 (2020), 82653–82661.');
INSERT INTO articleApp_reference VALUES(34,'Yan Zhang, PengFei Liu, and JingTao Yao. 2019. Three-way email spam filtering with game-theoretic rough sets. In 2019 International conference on computing, networking and communications (ICNC). IEEE, 552–556. Received 15 April 2023 187');
INSERT INTO articleApp_reference VALUES(35,'Minimax. Wikipedia. [Online] [Cited: April 23, 2008.]  http://en.wikipedia.org/wiki/Minimax.');
INSERT INTO articleApp_reference VALUES(36,'Von Neumann, J: Zur theorie der gesellschaftsspiele Math.  Annalen. 100 (1928) 295-320');
INSERT INTO articleApp_reference VALUES(37,'Automated Planning. Wikipedia. [Online] [Cited: April 23,  2008.] http://en.wikipedia.org/wiki/Automated_planning.');
INSERT INTO articleApp_reference VALUES(38,'Sanchez-Ruiz, Antonio, et al. Game AI for a Turn-based  Strategy Game with Plan Adaptation and Ontology-based  retrieval.');
INSERT INTO articleApp_reference VALUES(39,'K. Erol, J. Hendler, and D. Nau (1994). Semantics for  hierarchical task-network planning. Technical Report TR-94- 31, UMIACS.');
INSERT INTO articleApp_reference VALUES(40,'Smith, S. J. J. and Dana S. Nau, T. A. Throp. A Planning  approach decrarer play in contract bridge. Computational  Intelligence. 1996, Vol. 12, 1.');
INSERT INTO articleApp_reference VALUES(41,'One Jump Ahead: Challenging Human Supremacy in  Checkers. J.Schaeffer. s.l. : Springer-Verlag, 1997.');
INSERT INTO articleApp_reference VALUES(42,'IBM. How Deep Blue works. [Online] 1997. [Cited: April  23, 2008.]  http://www.research.ibm.com/deepblue/meet/html/d.3.2.html');
INSERT INTO articleApp_reference VALUES(43,'Ghallab, Malik, Nau, Dana and Traverso, Paolo. Automated Planning theory and practice. s.l. : Morgan  Kaufmann Publishers, May 2004. ISBN 1-55860-856-7.');
INSERT INTO articleApp_reference VALUES(44,'Case Based Reasoning. Experiences, Lessons and Future.  Leake, David. s.l. : AAAI Press. MIT Press., 1997.');
INSERT INTO articleApp_reference VALUES(45,'Applying case-based reasoning: techniques for enterprise  systems. Watson, I. San Francisco, CA, USA : Morgan  Kaufmann Publishers Inc., 1998.');
INSERT INTO articleApp_reference VALUES(46,'Plaza, A. Aamodt and E. Case-based reasoning:  Foundational issues, methodological. AI Communications.  1994, 7(i).');
INSERT INTO articleApp_reference VALUES(47,'Tic-tac-toe. Wikipedia. [Online] [Cited: April 23, 2008.]  http://en.wikipedia.org/wiki/Tic-tac-toe.');
INSERT INTO articleApp_reference VALUES(48,'Díaz-Agudo, B. and González-Calero, P. A. An  architecture for knowledge intensive CBR systems. Advances  in Case-Based Reasoning – (EWCBR’00). New York :  Springer-Verlag, Berlin Heidelberg, 2000.');
INSERT INTO articleApp_reference VALUES(49,'Ilghami, Okhtay and Nau, Dana S. A General Approach to  Synthesize Problem-Specific Planners. 2003.  302 DIMEA 2008 3rd International Conference on Digital Interactive Media in Entertainment and Arts');
INSERT INTO articleApp_reference VALUES(50,'T. Tamai. Foundations of Software Engineering. Iwanami Shoten, Tokyo, Japan, 2004. in Japanese. 610');
INSERT INTO articleApp_reference VALUES(51,'describe a tool with a ﬁsheye zooming algorithm which lets the user view a model with varying amounts of detail depending on the context. It has to be investigated whether it is possible to combine the ﬁsh- eye zooming concept with the dimension-based navigation paradigm. While the KobrA 2.0 implementation of nAOMi heavily uses UML diagrams for developers, Glinz et al. use custom diagram types, e.g. for structural and behavioral views. An approach which also emphasizes the description of for- mal consistency rules (correspondences) between views is RM-ODP');
INSERT INTO articleApp_reference VALUES(52,'');
INSERT INTO articleApp_reference VALUES(53,'. However, this approach does not explic- itly mention the notion of a SUM and thus implies that consistency rules should be deﬁned in a pairwise fashion be- tween individual pairs of views. ArchiMate');
INSERT INTO articleApp_reference VALUES(54,', which com- plements TOGAF');
INSERT INTO articleApp_reference VALUES(55,', is an enterprise architecture mod- eling language which oﬀers two orthogonal ”dimensions” for modeling, (business, architecture, and technology) layers and (informational, behavioral and structural) aspects and also suggests two more dimensions, purpose and abstraction level. However, as many of these views span multiple choices of a single“dimension”, the intuitive dimension-based navigation metaphor of OSM can not be easily applied. There are also more general approaches for view-based modeling but they are less speciﬁc in terms of consistency rules between views and provide little guidance on how to manage and navigate views, for example the Zachman Framework');
INSERT INTO articleApp_reference VALUES(56,'. Regarding the practical use of OSM environments in the future, the biggest challenge is developing appropriate SUM metamodels which can accommodate all the types of views and services that software engineers are accustomed to to- day. For this ﬁrst prototypical SUM-based environment sup- porting the OSM approach we had a method at our disposal (KobrA) that already deﬁned a full set of orthogonal UML- based views. This allowed us to model the required SUM and view metamodels by simply adapting the UML meta- models, removing and adding model elements as needed. In doing so we were able to manually ensure that the meta- models fulﬁlled the two core requirements of SUM-based en- vironments — (1) being minimalistic and (2) redundancy free. If SUM-based software engineering environments are to take oﬀ, and to be introduced into existing, heteroge- neous environments, more sophisticated ways of integrating existing metamodels into a single uniﬁed metamodel will be required. 8. REFERENCES');
INSERT INTO articleApp_reference VALUES(57,'C. Atkinson, J. Bayer, C. Bunse, E. Kamsties, O. Laitenberger, R. Laqua, D. Muthig, B. Paech, J. W¨ust, and J. Zettel. Component-Based Product Line Engineering with UML. Addison Wesley, Reading, Massachusetts, USA, 1st edition, November 2001.');
INSERT INTO articleApp_reference VALUES(58,'C. Atkinson, D. Stoll, and P. Bostan. Orthographic Software Modeling: A Practical Approach to View-Based Development. In Evaluation of Novel Approaches to Software Engineering, volume 69 of Communications in Computer and Information Science, pages 206–219. Springer Berlin Heidelberg, 2010.');
INSERT INTO articleApp_reference VALUES(59,'C. Atkinson, D. Stoll, and C. Tunjic. Orthographic Service Modeling. In Proceedings of 15th IEEE EDOC Conference Workshops (EDOCW), Helsinki, Finland, 2011.');
INSERT INTO articleApp_reference VALUES(60,'Eclipse Foundation. UML2Tools. http://wiki.eclipse.org/MDT-UML2Tools, 2013.');
INSERT INTO articleApp_reference VALUES(61,'ISO/IEC and ITU-T. The Reference Model of Open Distributed Processing. RM-ODP, ITU-T Rec. X.901-X.904 / ISO/IEC 10746. http://standards.iso.org/ ittf/PubliclyAvailableStandards/index.html, 1998.');
INSERT INTO articleApp_reference VALUES(62,'J. I. J. Jose Raul Romero and A. Vallecillo. Realizing Correspondences in MultiViewpoint Speciﬁcations. In Proceedings of the Thirteenth IEEE International EDOC Conference, 1 - 4 September 2009, Auckland, New Zealand, September 2009.');
INSERT INTO articleApp_reference VALUES(63,'M. Lankhorst. Enterprise Architecture at Work. Springer Berlin Heidelberg, 2009.');
INSERT INTO articleApp_reference VALUES(64,'Object Management Group (OMG). OMG Uniﬁed Modeling Language (OMG UML), Superstructure, V2.1.2. http://www.omg.org/cgi-bin/doc?formal/07-11-02, November 2007.');
INSERT INTO articleApp_reference VALUES(65,'Object Management Group (OMG). Meta Object Facility (MOF) 2.0 Query/View/Transformation, v1.0. http://www.omg.org/spec/QVT/1.0/PDF/, April 2008.');
INSERT INTO articleApp_reference VALUES(66,'C. Seybold, M. Glinz, S. Meier, and N. Merlo-Schett. An eﬀective layout adaptation technique for a graphical modeling tool. In Proceedings of the 2003 International Conference on Software Engineering, Portland, 2003.');
INSERT INTO articleApp_reference VALUES(67,'The Atlas Transformation Language (ATL). Oﬃcial Website. http://www.eclipse.org/atl/, 2013.');
INSERT INTO articleApp_reference VALUES(68,'The Open Group. TOGAF Version 9 - The Open Group Architecture Framework. http://www.opengroup.org/architecture/ togaf9-doc/arch/index.html, Feb 2009.');
INSERT INTO articleApp_reference VALUES(69,'University of Mannheim - Software Engineering Group. nAOMi - opeN, Adaptable, Orthographic Modeling EnvIronment. http://eclipselabs.org/p/naomi.');
INSERT INTO articleApp_reference VALUES(70,'J. A. Zachman. The Zachman Framework: A Primer for Enterprise Engineering and Manufacturing. http://www.zachmaninternational.com, 2009.');
INSERT INTO articleApp_reference VALUES(71,'Justito Adiprasetio and Annissa Winda Larasati. 2020. Pandemic crisis in online media: Quantitative framing analysis on Detik. com’s coverage of Covid-19. Jurnal Ilmu Sosial Dan Ilmu Politik 24, 2 (2020), 153–170.');
INSERT INTO articleApp_reference VALUES(72,'Neel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur, Pegah Maham, C Jess Riedel, Emmie Hine, Carolyn Ashurst, Paul Sedille, Alexis Carlier, et al. 2021. RAFT: A real-world few-shot text classification benchmark. arXiv preprint arXiv:2109.14076 (2021).');
INSERT INTO articleApp_reference VALUES(73,'David Alonso del Barrio and Daniel Gatica-Perez. 2022. How Did Europe’s Press Cover Covid-19 Vaccination News? A Five-Country Analysis. (2022), 35–43. https://doi.org/10.1145/3512732.3533588');
INSERT INTO articleApp_reference VALUES(74,'Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? (2021), 610–623.');
INSERT INTO articleApp_reference VALUES(75,'Santosh Kumar Biswal and Nikhil Kumar Gouda. 2020. Artificial intelligence in journalism: A boon or bane? In Optimization in machine learning and applications. Springer, 155–167.');
INSERT INTO articleApp_reference VALUES(76,'Erik Bleich, Hannah Stonebraker, Hasher Nisar, and Rana Abdelhamid. 2015. Media portrayals of minorities: Muslims in British newspaper headlines, 2001– 2012. Journal of Ethnic and Migration Studies 41, 6 (2015), 942–962.');
INSERT INTO articleApp_reference VALUES(77,'Michael Bommarito and Daniel Martin Katz. 2022. GPT Takes the Bar Exam. https://doi.org/10.48550/ARXIV.2212.14402');
INSERT INTO articleApp_reference VALUES(78,'Meredith Broussard, Nicholas Diakopoulos, Andrea L Guzman, Rediet Abebe, Michel Dupagne, and Ching-Hua Chuan. 2019. Artificial intelligence and jour- nalism. Journalism & Mass Communication Quarterly 96, 3 (2019), 673–695.');
INSERT INTO articleApp_reference VALUES(79,'Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877–1901.');
INSERT INTO articleApp_reference VALUES(80,'Björn Burscher, Daan Odijk, Rens Vliegenthart, Maarten De Rijke, and Claes H De Vreese. 2014. Teaching the computer to code frames in news: Comparing two supervised machine learning approaches to frame analysis. Communication Methods and Measures 8, 3 (2014), 190–206.');
INSERT INTO articleApp_reference VALUES(81,'Bjorn Burscher, Rens Vliegenthart, and Claes H de Vreese. 2016. Frames beyond words: Applying cluster and sentiment analysis to news coverage of the nuclear power issue. Social Science Computer Review 34, 5 (2016), 530–545.');
INSERT INTO articleApp_reference VALUES(82,'Dallas Card, Amber Boydstun, Justin Gross, Philip Resnik, and Noah Smith. 2015. The Media Frames Corpus: Annotations of Frames Across Issues. 2 (01 2015), 438–444. https://doi.org/10.3115/v1/P15-2072');
INSERT INTO articleApp_reference VALUES(83,'Daniel Catalan-Matamoros and Carlos Elías. 2020. Vaccine hesitancy in the age of coronavirus and fake news: analysis of journalistic sources in the Spanish quality press. International Journal of Environmental Research and Public Health 17, 21 (2020), 8136.');
INSERT INTO articleApp_reference VALUES(84,'Daniel Catalán-Matamoros and Carmen Peñafiel-Saiz. 2019. Media and mistrust of vaccines: a content analysis of press headlines. Revista latina de comunicación social 74 (2019), 786–802.');
INSERT INTO articleApp_reference VALUES(85,'Mark Coddington. 2015. Clarifying journalism’s quantitative turn: A typology for evaluating data journalism, computational journalism, and computer-assisted reporting. Digital journalism 3, 3 (2015), 331–348.');
INSERT INTO articleApp_reference VALUES(86,'Stephen D Cooper. 2010. The oppositional framing of bloggers. In Doing News Framing Analysis. Routledge, 151–172.');
INSERT INTO articleApp_reference VALUES(87,'Robert Dale. 2021. GPT-3: What’s it good for? Natural Language Engineering 27, 1 (2021), 113–118.');
INSERT INTO articleApp_reference VALUES(88,'Astrid Dirikx and Dave Gelders. 2010. To frame is to explain: A de- ductive frame-analysis of Dutch and French climate change coverage dur- ing the annual UN Conferences of the Parties. Public Understanding of Science 19, 6 (2010), 732–742. https://doi.org/10.1177/0963662509352044 arXiv:https://doi.org/10.1177/0963662509352044 PMID: 21560546.');
INSERT INTO articleApp_reference VALUES(89,'Astrid Dirikx and Dave Gelders. 2010. To frame is to explain: A deductive frame- analysis of Dutch and French climate change coverage during the annual UN Conferences of the Parties. Public understanding of science 19, 6 (2010), 732–742. 634 Framing the News: From Human Perception to Large Language Model Inferences ICMR ’23, June 12–15, 2023, Thessaloniki, Greece');
INSERT INTO articleApp_reference VALUES(90,'Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig. 2020. Gsum: A general framework for guided neural abstractive summarization. arXiv preprint arXiv:2010.08014 (2020).');
INSERT INTO articleApp_reference VALUES(91,'Sumayya Ebrahim. 2022. The corona chronicles: Framing analysis of online news headlines of the COVID-19 pandemic in Italy, USA and South Africa. Health SA Gesondheid (Online) 27 (2022), 1–8.');
INSERT INTO articleApp_reference VALUES(92,'Hend Abdelgaber Ahmed El-Behary. 2021. A Feverish Spring: A Comparative Analysis of COVID-19 News Framing in Sweden, the UK, and Egypt. (2021).');
INSERT INTO articleApp_reference VALUES(93,'Robert M Entman. 1993. Framing: Towards clarification of a fractured paradigm. McQuail’s reader in mass communication theory 390 (1993), 397.');
INSERT INTO articleApp_reference VALUES(94,'Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723 (2020).');
INSERT INTO articleApp_reference VALUES(95,'Piyush Ghasiya and Koji Okamura. 2021. Investigating COVID-19 news across four nations: a topic modeling and sentiment analysis approach. Ieee Access 9 (2021), 36645–36656.');
INSERT INTO articleApp_reference VALUES(96,'Robert Gifford. 1994. A Lens-Mapping Framework for Understanding the En- coding and Decoding of Interpersonal Dispositions in Nonverbal Behavior. Journal of Personality and Social Psychology 66 (02 1994), 398–412. https: //doi.org/10.1037//0022-3514.66.2.398');
INSERT INTO articleApp_reference VALUES(97,'Quentin Grail, Julien Perez, and Eric Gaussier. 2021. Globalizing BERT-based transformer architectures for long document summarization. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 1792–1810.');
INSERT INTO articleApp_reference VALUES(98,'Anushka Gupta, Diksha Chugh, Rahul Katarya, et al. 2022. Automated news summarization using transformers. In Sustainable Advanced Computing. Springer, 249–259.');
INSERT INTO articleApp_reference VALUES(99,'Alfred Hermida and Mary Lynn Young. 2017. Finding the data unicorn: A hierar- chy of hybridity in data and computational journalism. Digital Journalism 5, 2 (2017), 159–176.');
INSERT INTO articleApp_reference VALUES(100,'Karoliina Isoaho, Daria Gritsenko, and Eetu Mäkelä. 2021. Topic modeling and text analysis for qualitative policy research. Policy Studies Journal 49, 1 (2021), 300–324.');
INSERT INTO articleApp_reference VALUES(101,'Carina Jacobi, Wouter Van Atteveldt, and Kasper Welbers. 2016. Quantitative analysis of large amounts of journalistic texts using topic modelling. Digital journalism 4, 1 (2016), 89–106.');
INSERT INTO articleApp_reference VALUES(102,'Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computa- tional Linguistics 8 (2020), 423–438.');
INSERT INTO articleApp_reference VALUES(103,'Shima Khanehzar, Andrew Turpin, and Gosia Mikołajczak. 2019. Modeling Political Framing Across Policy Issues and Contexts. In ALTA.');
INSERT INTO articleApp_reference VALUES(104,'Jeesun Kim and Wayne Wanta. 2018. News framing of the US immigration debate during election years: Focus on generic frames. The Communication Review 21, 2 (2018), 89–115.');
INSERT INTO articleApp_reference VALUES(105,'Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi- hiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 (2022).');
INSERT INTO articleApp_reference VALUES(106,'Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. (2021). https://doi.org/10. 48550/ARXIV.2107.13586');
INSERT INTO articleApp_reference VALUES(107,'Siyi Liu, Lei Guo, Kate Mays, Margrit Betke, and Derry Tanti Wijaya. 2019. Detecting frames in news headlines and its application to analyzing news framing trends surrounding US gun violence. In Proceedings of the 23rd conference on computational natural language learning (CoNLL).');
INSERT INTO articleApp_reference VALUES(108,'Jörg Matthes and Matthias Kohring. 2008. The Content Analysis of Media Frames: Toward Improving Reliability and Validity. Journal of Communication 58 (06 2008). https://doi.org/10.1111/j.1460-2466.2008.00384.x');
INSERT INTO articleApp_reference VALUES(109,'Selina Meyer, David Elsweiler, Bernd Ludwig, Marcos Fernandez-Pichel, and David E Losada. 2022. Do We Still Need Human Assessors? Prompt-Based GPT-3 User Simulation in Conversational AI. In Proceedings of the 4th Conference on Conversational User Interfaces. 1–6.');
INSERT INTO articleApp_reference VALUES(110,'Stuart E Middleton, Symeon Papadopoulos, and Yiannis Kompatsiaris. 2018. Social computing for verifying social media content in breaking news. IEEE Internet Computing 22, 2 (2018), 83–89.');
INSERT INTO articleApp_reference VALUES(111,'Marko Milosavljević and Igor Vobič. 2021. ‘Our task is to demystify fears’: Analysing newsroom management of automation in journalism. Journalism 22, 9 (2021), 2203–2221.');
INSERT INTO articleApp_reference VALUES(112,'R. Monarch. 2021. Human-in-the-Loop Machine Learning: Active Learning and Annotation for Human-centered AI. Manning. https://books.google.ch/books? id=LCh0zQEACAAJ');
INSERT INTO articleApp_reference VALUES(113,'Tom Nicholls and Pepper D Culpepper. 2021. Computational identification of media frames: Strengths, weaknesses, and opportunities. Political Communication 38, 1-2 (2021), 159–181.');
INSERT INTO articleApp_reference VALUES(114,'Zhongdang Pan and Gerald M Kosicki. 1993. Framing analysis: An approach to news discourse. Political communication 10, 1 (1993), 55–75.');
INSERT INTO articleApp_reference VALUES(115,'Raul Puri and Bryan Catanzaro. 2019. Zero-shot text classification with generative language models. arXiv preprint arXiv:1912.10165 (2019).');
INSERT INTO articleApp_reference VALUES(116,'Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv preprint arXiv:2104.06599 (2021).');
INSERT INTO articleApp_reference VALUES(117,'Rabindra Lamsal. 2021. Sentiment Analysis of English Tweets with BERTsent. https://huggingface.co/rabindralamsal/finetuned-bertweet-sentiment-analysis.');
INSERT INTO articleApp_reference VALUES(118,'Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.');
INSERT INTO articleApp_reference VALUES(119,'Nishant Rai, Deepika Kumar, Naman Kaushik, Chandan Raj, and Ahad Ali. 2022. Fake News Classification using transformer based enhanced LSTM and BERT. International Journal of Cognitive Computing in Engineering 3 (2022), 98–105. https://doi.org/10.1016/j.ijcce.2022.03.003');
INSERT INTO articleApp_reference VALUES(120,'Frida V Rodelo. 2021. Framing of the Covid-19 pandemic and its organizational predictors. Cuadernos. info 50 (2021), 91–112.');
INSERT INTO articleApp_reference VALUES(121,'Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022).');
INSERT INTO articleApp_reference VALUES(122,'Holli Semetko and Patti Valkenburg. 2000. Framing European Politics: A Content Analysis of Press and Television News. Journal of Communication 50 (06 2000), 93 – 109. https://doi.org/10.1111/j.1460-2466.2000.tb02843.x');
INSERT INTO articleApp_reference VALUES(123,'Richard Shin, Christopher H Lin, Sam Thomson, Charles Chen, Subhro Roy, Em- manouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin Van Durme. 2021. Constrained language models yield few-shot semantic parsers. arXiv preprint arXiv:2104.08768 (2021).');
INSERT INTO articleApp_reference VALUES(124,'Efstathios Sidiropoulos and Andreas Veglis. 2017. Computer Supported Collab- orative Work trends on Media Organizations: Mixing Qualitative and Quan- titative Approaches. Studies in Media and Communication 5 (04 2017), 63. https://doi.org/10.11114/smc.v5i1.2279');
INSERT INTO articleApp_reference VALUES(125,'Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019).');
INSERT INTO articleApp_reference VALUES(126,'Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding the capabilities, limitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503 (2021).');
INSERT INTO articleApp_reference VALUES(127,'Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847 (2018).');
INSERT INTO articleApp_reference VALUES(128,'Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. 2021. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems 34 (2021), 200–212.');
INSERT INTO articleApp_reference VALUES(129,'Sandra A Vannoy and Prashant Palvia. 2010. The social influence model of technology adoption. Commun. ACM 53, 6 (2010), 149–153.');
INSERT INTO articleApp_reference VALUES(130,'Tuukka Ylä-Anttila, Veikko Eranti, and Anna Kukkonen. 2022. Topic modeling for frame analysis: A study of media debates on climate change in India and USA. Global Media and Communication 18, 1 (2022), 91–112. 635');
INSERT INTO articleApp_reference VALUES(131,'train a custom query generation model on web-text datasets, more recent work has leveraged large language models for zero/few-shot question generation [7, 8, 15, 23]. In generating synthetic queries, this work indicates the effectiveness of smaller parameter LLMs (up to 6B parameters) for generating synthetic queries in simpler information-retrieval tasks [7, 8, 23], and finds larger models (100B parameters and above) to be necessary for harder tasks such as argument retrieval [15, 23]. Similar to this work, we explore the generation of synthetic queries with LLMs for a retrieval task. Un- like this work, we demonstrate a data augmentation method for creating effective training data from sets of user documents found in recommendation datasets rather than individual documents. Other work in this space has also explored training more efficient multi- vector models from synthetic queries instead of more expensive cross-encoder models');
INSERT INTO articleApp_reference VALUES(132,'and generating queries with a diverse range of intents than the ones available in implicit feedback datasets to enhance item retrievability');
INSERT INTO articleApp_reference VALUES(133,'. 3https://github.com/iesl/narrative-driven-rec-mint/ Besides creating queries for ad-hoc retrieval tasks, concurrent work of Leszczynski et al.');
INSERT INTO articleApp_reference VALUES(134,'has also explored the creation of syn- thetic conversational search datasets from music recommendation datasets with LLMs. The synthetic queries and user documents are then used to train bi-encoder retrieval models for conversational search. Our work resembles this in creating synthetic queries from sets of user items found in recommendation interaction datasets. However, it differs in the task of focus, creating long-form narra- tive queries for NDR. Finally, our work also builds on the recent perspective of Radlinski et al.');
INSERT INTO articleApp_reference VALUES(135,'who make a case for natural language user profiles driving recommenders – narrative requests tie closely to natural language user profiles. Our work presents a step toward these systems. Finally, while our work explores data augmentation from user- item interactions for a retrieval-oriented NDR task, prior work has also explored data augmentation of the user-item graph for training collaborative filtering models. This work has often explored aug- mentation to improve recommendation performance for minority [12, 47] or cold-start users [11, 28, 45]. And has leveraged genera- tive models [11, 45] and text similarity models');
INSERT INTO articleApp_reference VALUES(136,'for augmenting the user-item graph. Complex Queries in Information Access. With the advent of performant models for text understanding, focus on complex and interactive information access tasks has seen a resurgence [2, 29, 32, 48]. NDR presents an example of this – NDR was first formalized in Bogers and Koolen');
INSERT INTO articleApp_reference VALUES(137,'for the case of book recommen- dation and subsequently studied in other domains [3, 4, 6]. Bogers and Koolen');
INSERT INTO articleApp_reference VALUES(138,'systematically examined narrative requests posted by users on discussion forums. They defined NDR as a task requir- ing item recommendation based on a long-form narrative query and prior-user item interactions. While this formulation resembles personalized search');
INSERT INTO articleApp_reference VALUES(139,'and query-driven recommendation');
INSERT INTO articleApp_reference VALUES(140,', the length and complexity of requests differentiate these from NDR. Other work has also demonstrated the effectiveness of re-ranking initial recommendations from collaborative filtering approaches 778 Large Language Model Augmented Narrative Driven Recommendations RecSys ’23, September 18–22, 2023, Singapore, Singapore Figure 3: Mint re-purposes readily available user-item interaction datasets commonly used to train collaborative filtering models for narrative-driven recommendation. This is done by authoring narrative queries for sets of items liked by a user with a large language model. The data is filtered with a smaller language model and retrieval models are trained on the synthetic queries and user items. based on the narrative query');
INSERT INTO articleApp_reference VALUES(141,'. More recent work of Afzali et al.');
INSERT INTO articleApp_reference VALUES(142,'formulate the NDR task without access to the prior interactions of a user while also noting the value of contextual cues contained in the narrative request. In our work, we focus on this latter for- mulation of NDR, given the lack of focus on effectively using the rich narrative queries in most prior work. Further, we demonstrate the usefulness of data augmentation from LLMs and user-item interaction datasets lacking narrative queries. Besides this, a range of work has explored more complex, long- form, and interactive query formulations for information access; these resemble queries in NDR. Arguello et al.');
INSERT INTO articleApp_reference VALUES(143,'define the tip of tongue retrieval task, a known-item search task where user queries describe the rich context of items while being unable to recall item metadata itself. Mysore et al.');
INSERT INTO articleApp_reference VALUES(144,'formulate an aspect conditional query-by example task where results must match specific aspects of a long natural language query. And finally, a vibrant body of work has explored conversational critiquing of recommenders where nat- ural language feedback helps tune the recommendations received by users [30, 44, 49]. 3 METHOD 3.1 Problem Setup In our work, we define narrative-driven recommendation (NDR) to be a ranking task, where given a narrative query 𝑞 made by a user 𝑢, a ranking system 𝑓 must generate a ranking 𝑅 over a collection of items C. Further, we assume access to a user-item interaction dataset I consisting of user interactions with items (𝑢, {𝑑𝑖}𝑁𝑢 𝑖=1). We assume the items 𝑑𝑖 to be textual documents like reviews or item descriptions. While we don’t assume there to be any overlap in the users making narrative queries or the collection of items C and the user-items interaction dataset I, we assume them to be from the same broad domain, e.g., books, movies, points-of-interest. 3.2 Proposed Method Our proposed method, Mint, for NDR, re-purposes a dataset of abundantly available user-item interactions, I = {(𝑢, {𝑑𝑖}𝑁𝑢 𝑖=1)} into training data for retrieval models by using LLMs as query gener- ation models to author narrative queries 𝑞𝑢: D = {(𝑞𝑢, {𝑑𝑖}𝑁𝑢 𝑖=1)}. Then, retrieval models are trained on the synthetic dataset D (Fig- ure 3). 3.2.1 Narrative Queries from LLMs. To author a narrative query 𝑞𝑢 for a user in I, we make use of the 175B parameter InstructGPT4 model as our query generation model QGen. We include the text of interacted items {𝑑𝑖}𝑁𝑢 𝑖=1 in the prompt for QGen, and instruct it to author a narrative query (Figure 2). To improve the coherence of generated queries and obtain correctly formatted outputs, we manually author narrative queries for 3 topically diverse users based on their interacted items and include it in the prompt for QGen. The same three few shot examples are used for the whole dataset I, and the three users were chosen from I. Generating narrative queries based on user interactions may also be considered a form of multi-document summarization for generating a natural language user profile');
INSERT INTO articleApp_reference VALUES(145,'. 3.2.2 Filtering Items for Synthetic Queries. Since we expect user items to capture multiple aspects of their interests and generated queries to only capture a subset of these interests, we only retain some of the items present in {𝑑𝑖}𝑁𝑢 𝑖=1 before using it for training re- trieval models. For this, we use a pre-trained language model to com- pute the likelihood of the query given each user item, 𝑃𝐿𝑀 (𝑞𝑢|𝑑𝑖), and only retain the top 𝑀 highly scoring item for 𝑞𝑢, this re- sults in 𝑀 training samples per user for our NDR retrieval models: {(𝑞𝑢,𝑑𝑖)𝑀 𝑖=1}. In our experiments, we use FlanT5 with 3B parame- ters');
INSERT INTO articleApp_reference VALUES(146,'for computing and follow Sachan et al.');
INSERT INTO articleApp_reference VALUES(147,'for computing 𝑃𝐿𝑀 (𝑞𝑢|𝑑𝑖). Note that our use of 𝑃𝐿𝑀 (𝑞𝑢|𝑑𝑖) represents a query- likelihood model classically used for ad-hoc search and recently shown to be an effective unsupervised re-ranking method when used with large pre-trained language models');
INSERT INTO articleApp_reference VALUES(148,'. 3.2.3 Training Retrieval Models. We train bi-encoder and cross- encoder models for NDR on the generated synthetic dataset – com- monly used models in search tasks. Bi-encoders are commonly used as scalable first-stage rankers from a large collection of items. On the other hand, cross-encoders allow a richer interaction between query and item and are used as second-stage re-ranking models. For both models, we use a pre-trained transformer language model architec- ture with 110M parameters, MPnet, a model similar to Bert');
INSERT INTO articleApp_reference VALUES(149,'. Bi-encoder models embed the query and item independently into high dimensional vectors: q𝑢 = MPNet(𝑞𝑢), d𝑖 = MPNet(𝑑𝑖) and rank items for the user based on the minimum L2 distance between 4https://platform.openai.com/docs/models/gpt-3, text-davinci-003 779 RecSys ’23, September 18–22, 2023, Singapore, Singapore Mysore, McCallum, Zamani q𝑢 and d𝑖. Embeddings are obtained by averaging token embeddings from the final layer of MPNet, and the same model is used for both queries and items. Cross-encoder models input both the query and item and output a score to be used for ranking 𝑠 = 𝑓Cr([𝑞𝑢;𝑑𝑖]), where 𝑓Cr is parameterized as w𝑇 dropout  W𝑇 MPNet(·)  . We train our bi-encoder model with a margin ranking loss: L𝐵𝑖 = Í 𝑢 Í𝑀 𝑖=1 max[𝐿2(q𝑢, d𝑖) − 𝐿2(q𝑢, d ′ 𝑖) + 𝛿, 0] with randomly sam- pled negatives 𝑑 ′ and 𝛿 = 1. Our cross-encoders are trained with a cross-entropy loss: L𝐶𝑟 = Í 𝑢 Í𝑀 𝑖=1 log( Í 𝑒𝑠 𝑑′ 𝑒𝑠′ ). For training, 4 negative example items 𝑑′ are randomly sampled from ranks 100- 300 from our trained bi-encoder. At test time, we retrieve the top 200 items with our trained bi-encoder and re-rank them with the cross-encoder - we evaluate both these components in experiments and refer to them as BiEnc-Mint and CrEnc-Mint. 4 EXPERIMENTS AND RESULTS Next, we evaluate Mint on a publicly available test collection for NDR and present a series of ablations. 4.1 Experimental Setup 4.1.1 Datasets. We perform evaluations on an NDR dataset for point-of-interest (POI) recommendation Pointrec');
INSERT INTO articleApp_reference VALUES(150,'. Pointrec contains 112 realistic narrative queries (130 words long) obtained from discussion forums on Reddit and items pooled from baseline rankers. The items are annotated on a graded relevance scale by crowd-workers and/or discussion forum members and further vali- dated by the dataset authors. The item collection C in Pointrec contains 700k POIs with metadata (category, city) and noisy text snippets describing the POI obtained from the Bing search engine. For test time ranking, we only rank the candidate items in the city and request category (e.g., “Restaurants”) of the query available in Pointrec - this follows prior practice to exclude clearly irrelevant items [1, 26]. We use user-item interaction datasets from Yelp to generate synthetic queries for training.5 Note also that we limit our evaluations to Pointrec since it presents the only publicly avail- able, manually annotated, and candidate pooled test collection for NDR, to our knowledge. Other datasets for NDR use document col- lections that are no longer publicly accessible');
INSERT INTO articleApp_reference VALUES(151,', contain sparse and noisy relevance judgments due to them being determined with automatic rules applied to discussion threads [18, 24], lack pooling to gather candidates for judging relevance [18, 24], or lack realistic narrative queries');
INSERT INTO articleApp_reference VALUES(152,'. We leave the development of more robust test collections and evaluation methods for NDR to future work. 4.1.2 Implementation Details. Next, we describe important details for Mint and leave finer details of the model and training to our code release. To sample user interactions for generating synthetic queries from the Yelp dataset, we exclude POIs and users with fewer than ten reviews to ensure that users were regular users of the site with well represented interests. This follows common prior practice in preparing user-item interaction datasets for use');
INSERT INTO articleApp_reference VALUES(153,'. Then we retain users who deliver an average rating greater than 3/5 and with 10-30 above-average reviews. This desirably biases our data to users who commonly describe their likings (rather than 5https://www.yelp.com/dataset dislikes). It also retains the users whose interests are summarizable by QGen. In the Yelp dataset, this results in 45,193 retained users. Now, 10,000 randomly selected users are chosen for generating syn- thetic narrative queries. For these users, a single randomly selected sentence from 10 of their reviews is included in the prompt (Figure 2) to QGen, i.e., 𝑁𝑢 = 10. After generating synthetic queries, some items are filtered out (§3.2.2). Here, we exclude 40% of the items for a user. This results in about 60,000 training samples for training BiEnc-Mint and CrEnc-Mint. These decisions were made manu- ally by examining the resulting datasets and the cost of authoring queries. The expense of generating 𝑞𝑢 was about USD 230. 4.1.3 Baselines. We compare BiEnc-Mint and CrEnc-Mint mod- els against several standard and performant retrieval model base- lines. These span zero-shot/unsupervised rankers, supervised bi- encoders, unsupervised cross-encoders, and LLM baselines. BM25: A standard unsupervised sparse retrieval baseline based on term overlap between query and document, with strong generalization performance across tasks and domains');
INSERT INTO articleApp_reference VALUES(154,'. Contriver: A BERT-base bi-encoder model pre-trained for zero-shot retrieval with weakly su- pervised query-document pairs');
INSERT INTO articleApp_reference VALUES(155,'. MPNet-1B: A strong Sentence- Bert bi-encoder model initialized with MPNet-base and trained on 1 billion supervised query-document pairs aggregated from numer- ous domains');
INSERT INTO articleApp_reference VALUES(156,'. BERT-MSM: A BERT-base bi-encoder fine-tuned on supervised question-passage pairs from MSMarco. UPR: A two- stage approach that retrieves items with a Contriver bi-encoder and re-ranks the top 200 items with a query-likelihood model using a FlanT5 model with 3B parameters [14, 40]. This may be seen as an unsupervised “cross-encoder” model. Grounded LLM: A re- cently proposed two-stage approach which autoregressively gener- ates ten pseudo-relevant items using an LLM (175B InstructGPT) prompted with the narrative query and generates recommenda- tions grounded in C by retrieving the nearest neighbors for each generated item using a bi-encoder');
INSERT INTO articleApp_reference VALUES(157,'. We include one few-shot example of a narrative query and recommended items in the prompt to the LLM. We run this baseline three times and report average performance across runs. We report NDCG at 5 and 10, MAP, MRR, and Recall at 100 and 200. Finally, our reported results should be considered lower bounds on realistic performance due to the un- judged documents (about 70% at 𝑘 = 10) in our test collections');
INSERT INTO articleApp_reference VALUES(158,'. 4.2 Results Table 1 presents the performance of the proposed method compared against baselines. Here, bold numbers indicate the best-performing model, and superscripts indicate statistical significance computed with two-sided t-tests at 𝑝 < 0.05. Here, we first note the performance of baseline approaches. We see BM25 outperformed by Contriver, a transformer bi-encoder model trained for zero-shot retrieval; this mirrors prior work');
INSERT INTO articleApp_reference VALUES(159,'. Next, we see supervised bi-encoder models trained on similar pas- sage (MPNet-1B) and question-answer (BERT-MSM) pairs outper- form a weakly supervised model (Contriver) by smaller margins. Finally, the Grounded LLM outperforms all bi-encoder baselines, in- dicating strong few-shot generalization and mirroring prior results');
INSERT INTO articleApp_reference VALUES(160,'. Examining the Mint models, we first note that the BiEnc- Mint sees statistically significant improvement compared to BM25 780 Large Language Model Augmented Narrative Driven Recommendations RecSys ’23, September 18–22, 2023, Singapore, Singapore Table 1: Performance of the proposed method, Mint, for point-of-interest recommendation on Pointrec. The superscripts denote statistically significant improvements compared to specific baseline models. Pointrec Model Parameters NDCG@5 NDCG@10 MAP MRR Recall@100 Recall@200 1BM25 - 0.2682 0.2464 0.1182 0.2685 0.4194 0.5429 2Contriver 110M 0.2924 0.2776 0.1660 0.3355 0.4455 0.5552 3MPNet-1B 110M 0.3038 0.2842 0.1621 0.3566 0.4439 0.5657 4BERT-MSM 110M 0.3117 0.2886 0.1528 0.3320 0.4679 0.5816 5Grounded LLM 175B+110M 0.3558 0.3251 0.1808 0.3861 0.4797 0.5797 6UPR 110M+3B 0.3586 0.3242 0.1712 0.4013 0.4489 0.5552 BiEnc-Mint 110M 0.34891 0.32631 0.18901 0.39821 0.49141 0.6221 CrEnc-Mint 2×110M 0.372512 0.348912 0.219214 0.43171 0.5448123 0.6221 and outperforms the best bi-encoder baselines by 11-13% on preci- sion measures and 5-7% on recall measures. Specifically, we see a model trained for question-answering (BERT-MSM) underperform BiEnc-Mint, indicating the challenge of the NDR task. Further, BiEnc-Mint, trained on 5 orders of magnitude lesser data than MPNet-1B, sees improved performance – indicating the quality of data obtained from Mint. Furthermore, BiEnc-Mint also performs at par with a 175B LLM while offering the inference efficiency of a small-parameter bi-encoder. Next, we see CrEnc-Mint outperform the baseline bi-encoders, BiEnc-Mint, UPR, and Grounded LLM by 4-21% on precision measures and 7-13% on recall measures – demonstrating the value of Mint for training NDR models. 4.3 Ablations In Table 2, we ablate various design choices in Mint. Different choices result in different training sets for the BiEnc and CrEnc models. Also, note that in reporting ablation performance for CrEnc, we still use the performant BiEnc-Mint model for obtaining nega- tive examples for training and first-stage ranking. Without high- quality negative examples, we found CrEnc to result in much poorer performance. No item filtering. Since synthetic queries are unlikely to rep- resent all the items of a user, Mint excludes user items {𝑑𝑖}𝑁𝑢 𝑖=1 which have a low likelihood of being generated from the document (§3.2.2). Without this step, we expect the training set for training retrieval models to be larger and noisier. In Table 2, we see that excluding this step leads to a lower performance for BiEnc and CrEnc, indicating that the quality of data obtained is important for performance. 6B LLM for QGen. Mint relies on using an expensive 175B pa- rameter InstructGPT model for QGen. Here, we investigate the efficacy for generating𝑞𝑢 for {𝑑𝑖}𝑁𝑢 𝑖=1 with a 6B parameter Instruct- GPT model (text-curie-001). We use an identical setup to the 175B LLM for this. In Table 2, we see that training on the synthetic narrative queries of the smaller LLM results in worse models – of- ten underperforming the baselines in Table 1. This indicates the inability of a smaller model to generate complex narrative queries while conditioning on a set of user items. This necessity of a larger LLM for generating queries in complex retrieval tasks has been observed in prior work [15, 23]. 6B LLM for Item Queries. We find a smaller 6B LLM to result in poor quality data when used to generate narrative queries con- ditioned on {𝑑𝑖}𝑁𝑢 𝑖=1. Here we simplify the text generation task – using a 6B LLM to generate queries for individual items 𝑑𝑖. This experiment also mirrors the setup for generating synthetic queries for search tasks [7, 15]. Here, we use 3-few shot examples and sam- ple one item per user for generating 𝑞𝑢. Given the lower cost of using a smaller LLM, we use all 45,193 users in our Yelp dataset rather than a smaller random sample. From Table 2, we see that this results in higher quality queries than using smaller LLMs for gen- erating narrative queries from {𝑑𝑖}𝑁𝑢 𝑖=1. The resulting BiEnc model underperforms the BiEnc-Mint, indicating the value of generating complex queries conditioned on multiple items as in Mint for NDR. We see that CrEnc approaches the performance of CrEnc-Mint– note, however, that this approach uses the performant BiEnc-Mint for sampling negatives and first stage ranking. We leave further exploration of using small parameter LLMs for data augmentation for NDR models to future work. 5 CONCLUSIONS In this paper, we present Mint, a data augmentation method for the narrative-driven recommendation (NDR) task. Mint re-purposes historical user-item interaction datasets for NDR by using a 175B pa- rameter large language model to author long-form narrative queries while conditioning on the text of items liked by users. We evaluate bi-encoder and cross-encoder models trained on data from Mint on the publicly available Pointrec test collection for narrative-driven point of interest recommendation. We demonstrate that the result- ing models outperform several strong baselines and ablated models and match or outperform a 175B LLM directly used for NDR in a 1-shot setup. However, Mint also presents some limitations. Given our use of historical interaction datasets for generating synthetic training data and the prevalence of popular interests in these datasets longer, tailed interests are unlikely to be present in the generated syn- thetic datasets. In turn, causing retrieval models to likely see poorer performance on these requests. Our use of LLMs to generate syn- thetic queries also causes the queries to be repetitive in structure, likely causing novel longer-tail queries to be poorly served. These limitations may be addressed in future work. 781 RecSys ’23, September 18–22, 2023, Singapore, Singapore Mysore, McCallum, Zamani Table 2: Mint ablated for different design choices on Pointrec. Pointrec Ablation NDCG@5 NDCG@10 MAP MRR Recall@100 Recall@200 BiEnc-Mint 0.3489 0.3263 0.1890 0.3982 0.5263 0.6221 − No item filtering 0.2949 0.2766 0.1634 0.3505 0.4979 0.5951 − 6B LLM for QGen 0.2336 0.2293 0.1125 0.2287 0.426 0.5435 − 6B LLM for Item Queries 0.3012 0.2875 0.1721 0.3384 0.4800 0.5909 CrEnc-Mint 0.3725 0.3489 0.2192 0.4317 0.5448 0.6221 − No item filtering 0.3570 0.3379 0.2071 0.4063 0.5366 0.6221 − 6B LLM for QGen 0.2618 0.2421 0.1341 0.3118 0.4841 0.6221 − 6B LLM for Item Queries 0.3792 0.3451 0.2128 0.4098 0.5546 0.6221 Besides this, other avenues also present rich future work. While Mint leverages a 175B LLM for generating synthetic queries, smaller parameter LLMs may be explored for this purpose - perhaps by training dedicated QGen models. Mint may also be expanded to explore more active strategies for sampling items and users for whom narrative queries are authored - this may allow more effi- cient use of large parameter LLMs while ensuring higher quality training datasets. Next, the generation of synthetic queries from sets of documents may be explored for a broader range of retrieval tasks beyond NDR given its promise to generate larger training sets – a currently underexplored direction. Finally, given the lack of larger-scale test collections for NDR and the effectiveness of LLMs for authoring narrative queries from user-item interaction, fruitful future work may also explore the creation of larger-scale datasets in a mixed-initiative setup to robustly evaluate models for NDR. ACKNOWLEDGMENTS We thank anonymous reviewers for their invaluable feedback. This work was partly supported by the Center for Intelligent Informa- tion Retrieval, NSF grants IIS-1922090 and 2143434, the Office of Naval Research contract number N000142212688, an Amazon Alexa Prize grant, and the Chan Zuckerberg Initiative under the project Scientific Knowledge Base Construction. Any opinions, findings and conclusions or recommendations expressed here are those of the authors and do not necessarily reflect those of the sponsors. REFERENCES');
INSERT INTO articleApp_reference VALUES(161,'Jafar Afzali, Aleksander Mark Drzewiecki, and Krisztian Balog. 2021. POINTREC: A Test Collection for Narrative-Driven Point of Interest Recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR ’21). As- sociation for Computing Machinery, New York, NY, USA, 2478–2484. https: //doi.org/10.1145/3404835.3463243');
INSERT INTO articleApp_reference VALUES(162,'Jaime Arguello, Adam Ferguson, Emery Fine, Bhaskar Mitra, Hamed Zamani, and Fernando Diaz. 2021. Tip of the Tongue Known-Item Retrieval: A Case Study in Movie Identification. In Proceedings of the 6th international ACM SIGIR Conference on Human Information Interaction and Retrieval. ACM. https://dlnext.acm.org/ doi/10.1145/3406522.3446021');
INSERT INTO articleApp_reference VALUES(163,'Toine Bogers, Maria Gäde, Marijn Koolen, Vivien Petras, and Mette Skov. 2018. “What was this Movie About this Chick?” A Comparative Study of Relevance Aspects in Book and Movie Discovery. In Transforming Digital Worlds: 13th Inter- national Conference, iConference 2018, Sheffield, UK, March 25-28, 2018, Proceedings 13. Springer, 323–334.');
INSERT INTO articleApp_reference VALUES(164,'Toine Bogers, Maria Gäde, Marijn Koolen, Vivien Petras, and Mette Skov. 2019. “Looking for an amazing game I can relax and sink hours into...”: A Study of Relevance Aspects in Video Game Discovery. In Information in Contemporary Society: 14th International Conference, iConference 2019, Washington, DC, USA, March 31–April 3, 2019, Proceedings 14. Springer, 503–515.');
INSERT INTO articleApp_reference VALUES(165,'Toine Bogers and Marijn Koolen. 2017. Defining and Supporting Narrative-Driven Recommendation. In Proceedings of the Eleventh ACM Conference on Recommender Systems (Como, Italy) (RecSys ’17). Association for Computing Machinery, New York, NY, USA, 238–242. https://doi.org/10.1145/3109859.3109893');
INSERT INTO articleApp_reference VALUES(166,'Toine Bogers and Marijn Koolen. 2018. “I’m looking for something like...”: Combining Narratives and Example Items for Narrative-driven Book Recommen- dation. In Knowledge-aware and Conversational Recommender Systems Workshop. CEUR Workshop Proceedings.');
INSERT INTO articleApp_reference VALUES(167,'Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. InPars: Unsupervised Dataset Generation for Information Retrieval. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (Madrid, Spain) (SIGIR ’22). Association for Computing Machinery, New York, NY, USA, 2387–2392. https://doi.org/10.1145/3477495. 3531863');
INSERT INTO articleApp_reference VALUES(168,'Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu, Ramya Ramanathan, and Eric Nyberg. 2023. InPars-Light: Cost-Effective Unsu- pervised Training of Efficient Rankers. arXiv:2301.02998');
INSERT INTO articleApp_reference VALUES(169,'Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ran- zato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877–1901. https://proceedings.neurips.cc/paper_files/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf');
INSERT INTO articleApp_reference VALUES(170,'Chris Buckley and Ellen M. Voorhees. 2004. Retrieval Evaluation with Incomplete Information. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (Sheffield, United Kingdom) (SIGIR ’04). Association for Computing Machinery, New York, NY, USA, 25–32. https://doi.org/10.1145/1008992.1009000');
INSERT INTO articleApp_reference VALUES(171,'Dong-Kyu Chae, Jihoo Kim, Duen Horng Chau, and Sang-Wook Kim. 2020. AR- CF: Augmenting Virtual Users and Items in Collaborative Filtering for Addressing Cold-Start Problems. In Proceedings of the 43rd International ACM SIGIR Con- ference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR ’20). Association for Computing Machinery, New York, NY, USA, 1251–1260. https://doi.org/10.1145/3397271.3401038');
INSERT INTO articleApp_reference VALUES(172,'Lei Chen, Le Wu, Kun Zhang, Richang Hong, Defu Lian, Zhiqiang Zhang, Jun Zhou, and Meng Wang. 2023. Improving Recommendation Fairness via Data Augmentation. In Proceedings of the ACM Web Conference 2023 (Austin, TX, USA) (WWW ’23). Association for Computing Machinery, New York, NY, USA, 1012–1020. https://doi.org/10.1145/3543507.3583341');
INSERT INTO articleApp_reference VALUES(173,'Li Chen, Zhirun Zhang, Xinzhi Zhang, and Lehong Zhao. 2022. A Pilot Study for Understanding Users’ Attitudes Towards a Conversational Agent for News Recommendation. In Proceedings of the 4th Conference on Conversational User Interfaces (Glasgow, United Kingdom) (CUI ’22). Association for Computing Machinery, New York, NY, USA, Article 36, 6 pages. https://doi.org/10.1145/ 3543829.3544530');
INSERT INTO articleApp_reference VALUES(174,'Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022).');
INSERT INTO articleApp_reference VALUES(175,'Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith Hall, and Ming-Wei Chang. 2023. Promptagator: Few-shot 782 Large Language Model Augmented Narrative Driven Recommendations RecSys ’23, September 18–22, 2023, Singapore, Singapore Dense Retrieval From 8 Examples. In The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=gmL46YMpu2J');
INSERT INTO articleApp_reference VALUES(176,'Abhinandan S. Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. 2007. Google News Personalization: Scalable Online Collaborative Filtering. In Pro- ceedings of the 16th International Conference on World Wide Web (Banff, Alberta, Canada) (WWW ’07). Association for Computing Machinery, New York, NY, USA, 271–280. https://doi.org/10.1145/1242572.1242610');
INSERT INTO articleApp_reference VALUES(177,'James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, and Dasarathi Sampath. 2010. The YouTube Video Recommendation System. In Proceedings of the Fourth ACM Conference on Recommender Systems (Barcelona, Spain) (RecSys ’10). Association for Computing Machinery, New York, NY, USA, 293–296. https: //doi.org/10.1145/1864708.1864770');
INSERT INTO articleApp_reference VALUES(178,'Lukas Eberhard, Simon Walk, Lisa Posch, and Denis Helic. 2019. Evaluating Narrative-Driven Movie Recommendations on Reddit. In Proceedings of the 24th International Conference on Intelligent User Interfaces (Marina del Ray, California) (IUI ’19). Association for Computing Machinery, New York, NY, USA, 1–11. https: //doi.org/10.1145/3301275.3302287');
INSERT INTO articleApp_reference VALUES(179,'Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise Zero-Shot Dense Retrieval without Relevance Labels. arXiv preprint arXiv:2212.10496 (2022).');
INSERT INTO articleApp_reference VALUES(180,'Negar Hariri, Bamshad Mobasher, and Robin Burke. 2013. Query-Driven Context Aware Recommendation. In Proceedings of the 7th ACM Conference on Recom- mender Systems (Hong Kong, China) (RecSys ’13). Association for Computing Machinery, New York, NY, USA, 9–16. https://doi.org/10.1145/2507157.2507187');
INSERT INTO articleApp_reference VALUES(181,'Seyyed Hadi Hashemi, Jaap Kamps, Julia Kiseleva, Charles LA Clarke, and Ellen M Voorhees. 2016. Overview of the TREC 2016 Contextual Suggestion Track.. In TREC.');
INSERT INTO articleApp_reference VALUES(182,'Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo- janowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised Dense Infor- mation Retrieval with Contrastive Learning. Transactions on Machine Learning Research (2022). https://openreview.net/forum?id=jKN1pXi7b0');
INSERT INTO articleApp_reference VALUES(183,'Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo, Jakub Zavrel, and Rodrigo Nogueira. 2023. InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval. arXiv:2301.01820');
INSERT INTO articleApp_reference VALUES(184,'Marijn Koolen, Toine Bogers, Maria Gäde, Mark Hall, Iris Hendrickx, Hugo Huurdeman, Jaap Kamps, Mette Skov, Suzan Verberne, and David Walsh. 2016. Overview of the CLEF 2016 Social Book Search Lab. In Experimental IR Meets Mul- tilinguality, Multimodality, and Interaction, Norbert Fuhr, Paulo Quaresma, Teresa Gonçalves, Birger Larsen, Krisztian Balog, Craig Macdonald, Linda Cappellato, and Nicola Ferro (Eds.). Springer International Publishing, Cham, 351–370.');
INSERT INTO articleApp_reference VALUES(185,'Megan Leszczynski, Ravi Ganti, Shu Zhang, Krisztian Balog, Filip Radlinski, Fernando Pereira, and Arun Tejasvi Chaganty. 2023. Generating Synthetic Data for Conversational Music Recommendation Using Random Walks and Language Models. arXiv:2301.11489');
INSERT INTO articleApp_reference VALUES(186,'Xin Liu, Yong Liu, Karl Aberer, and Chunyan Miao. 2013. Personalized Point-of- Interest Recommendation by Mining Users’ Preference Transition. In Proceedings of the 22nd ACM International Conference on Information & Knowledge Manage- ment (San Francisco, California, USA) (CIKM ’13). Association for Computing Ma- chinery, New York, NY, USA, 733–738. https://doi.org/10.1145/2505515.2505639');
INSERT INTO articleApp_reference VALUES(187,'Yiding Liu, Tuan-Anh Nguyen Pham, Gao Cong, and Quan Yuan. 2017. An Experimental Evaluation of Point-of-Interest Recommendation in Location-Based Social Networks. Proc. VLDB Endow. 10, 10 (jun 2017), 1010–1021. https://doi. org/10.14778/3115404.3115407');
INSERT INTO articleApp_reference VALUES(188,'Federico López, Martin Scholz, Jessica Yung, Marie Pellat, Michael Strube, and Lucas Dixon. 2021. Augmenting the user-item graph with textual similarity models. arXiv preprint arXiv:2109.09358 (2021).');
INSERT INTO articleApp_reference VALUES(189,'Xing Han Lu, Siva Reddy, and Harm de Vries. 2023. The StatCan Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, Dubrovnik, Croatia, 2799–2829. https://aclanthology.org/2023.eacl-main.206');
INSERT INTO articleApp_reference VALUES(190,'Kai Luo, Scott Sanner, Ga Wu, Hanze Li, and Hojin Yang. 2020. Latent Linear Critiquing for Conversational Recommender Systems. In The Web Conference.');
INSERT INTO articleApp_reference VALUES(191,'Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. 2021. Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic Question Generation. In Proceedings of the 16th Conference of the European Chapter of the Associa- tion for Computational Linguistics: Main Volume. Association for Computational Linguistics, Online, 1075–1088. https://doi.org/10.18653/v1/2021.eacl-main.92');
INSERT INTO articleApp_reference VALUES(192,'Sheshera Mysore, Tim O’Gorman, Andrew McCallum, and Hamed Zamani. 2021. CSFCube - A Test Collection of Computer Science Research Articles for Faceted Query by Example. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). https://doi.org/10.48550/arXiv. 2103.12906');
INSERT INTO articleApp_reference VALUES(193,'Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 27730–27744. https://proceedings.neurips.cc/paper_files/paper/2022/file/ b1efde53be364a73914f58805a001731-Paper-Conference.pdf');
INSERT INTO articleApp_reference VALUES(194,'Andrea Papenmeier, Dagmar Kern, Daniel Hienert, Alfred Sliwa, Ahmet Aker, and Norbert Fuhr. 2021. Starting Conversations with Search Engines - Interfaces That Elicit Natural Language Queries. In Proceedings of the 2021 Conference on Human Information Interaction and Retrieval (Canberra ACT, Australia) (CHIIR ’21). Association for Computing Machinery, New York, NY, USA, 261–265. https: //doi.org/10.1145/3406522.3446035');
INSERT INTO articleApp_reference VALUES(195,'Gustavo Penha, Enrico Palumbo, Maryam Aziz, Alice Wang, and Hugues Bouchard. 2023. Improving Content Retrievability in Search with Controllable Query Generation. In Proceedings of the ACM Web Conference 2023 (Austin, TX, USA) (WWW ’23). Association for Computing Machinery, New York, NY, USA, 3182–3192. https://doi.org/10.1145/3543507.3583261');
INSERT INTO articleApp_reference VALUES(196,'Filip Radlinski, Krisztian Balog, Fernando Diaz, Lucas Dixon, and Ben Wedin. 2022. On Natural Language User Profiles for Transparent and Scrutable Rec- ommendation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (Madrid, Spain) (SIGIR ’22). Association for Computing Machinery, New York, NY, USA, 2863–2874. https://doi.org/10.1145/3477495.3531873');
INSERT INTO articleApp_reference VALUES(197,'Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Em- pirical Methods in Natural Language Processing. Association for Computational Linguistics. https://arxiv.org/abs/1908.10084');
INSERT INTO articleApp_reference VALUES(198,'Stephen Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance Framework: BM25 and Beyond. Found. Trends Inf. Retr. 3, 4 (apr 2009), 333–389. https://doi.org/10.1561/1500000019');
INSERT INTO articleApp_reference VALUES(199,'Jon Saad-Falcon, Omar Khattab, Keshav Santhanam, Radu Florian, Martin Franz, Salim Roukos, Avirup Sil, Md Arafat Sultan, and Christopher Potts. 2023. UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers. arXiv:2303.00807 [cs.IR]');
INSERT INTO articleApp_reference VALUES(200,'Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving Passage Retrieval with Zero-Shot Question Generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 3781–3797. https://aclanthology. org/2022.emnlp-main.249');
INSERT INTO articleApp_reference VALUES(201,'Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2020. MPNet: Masked and Permuted Pre-training for Language Understanding. In Advances in Neural Information Processing Systems, Vol. 33. https://proceedings.neurips.cc/paper_ files/paper/2020/file/c3a690be93aa602ee2dc0ccab5b7b67e-Paper.pdf');
INSERT INTO articleApp_reference VALUES(202,'Jaime Teevan, Susan T. Dumais, and Eric Horvitz. 2005. Personalizing Search via Automated Analysis of Interests and Activities. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (Salvador, Brazil) (SIGIR ’05). Association for Computing Machinery, New York, NY, USA, 449–456. https://doi.org/10.1145/1076034.1076111');
INSERT INTO articleApp_reference VALUES(203,'Mengting Wan and Julian McAuley. 2018. Item Recommendation on Monotonic Behavior Chains. In Proceedings of the 12th ACM Conference on Recommender Systems (Vancouver, British Columbia, Canada) (RecSys ’18). Association for Computing Machinery, New York, NY, USA, 86–94. https://doi.org/10.1145/ 3240323.3240369');
INSERT INTO articleApp_reference VALUES(204,'Haonan Wang, Chang Zhou, Carl Yang, Hongxia Yang, and Jingrui He. 2021. Controllable Gradient Item Retrieval. In Web Conference.');
INSERT INTO articleApp_reference VALUES(205,'Qinyong Wang, Hongzhi Yin, Hao Wang, Quoc Viet Hung Nguyen, Zi Huang, and Lizhen Cui. 2019. Enhancing Collaborative Filtering with Generative Aug- mentation. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD ’19). As- sociation for Computing Machinery, New York, NY, USA, 548–556. https: //doi.org/10.1145/3292500.3330873');
INSERT INTO articleApp_reference VALUES(206,'Jiajing Xu, Andrew Zhai, and Charles Rosenberg. 2022. Rethinking Personalized Ranking at Pinterest: An End-to-End Approach. In Proceedings of the 16th ACM Conference on Recommender Systems (Seattle, WA, USA) (RecSys ’22). Association for Computing Machinery, New York, NY, USA, 502–505. https://doi.org/10. 1145/3523227.3547394');
INSERT INTO articleApp_reference VALUES(207,'Yuxin Ying, Fuzhen Zhuang, Yongchun Zhu, Deqing Wang, and Hongwei Zheng. 2023. CAMUS: Attribute-Aware Counterfactual Augmentation for Minority Users in Recommendation. In Proceedings of the ACM Web Conference 2023 (Austin, TX, USA) (WWW ’23). Association for Computing Machinery, New York, NY, USA, 1396–1404. https://doi.org/10.1145/3543507.3583538');
INSERT INTO articleApp_reference VALUES(208,'Hamed Zamani, Johanne R Trippas, Jeff Dalton, and Filip Radlinski. 2022. Con- versational information seeking. arXiv preprint arXiv:2201.08808 (2022).');
INSERT INTO articleApp_reference VALUES(209,'Jie Zou, Yifan Chen, and Evangelos Kanoulas. 2020. Towards Question-Based Recommender Systems. In Proceedings of the 43rd International ACM SIGIR Confer- ence on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR ’20). Association for Computing Machinery, New York, NY, USA, 881–890. https://doi.org/10.1145/3397271.3401180 783');
CREATE TABLE IF NOT EXISTS "articleApp_article" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "titre" varchar(100) NOT NULL, "resume" text NOT NULL, "traiter" bool NOT NULL, "motsCles" varchar(1000) NOT NULL, "urlPdf" varchar(200) NOT NULL, "dateDePublication" date NULL, "texteIntegral" text NOT NULL);
INSERT INTO articleApp_article VALUES(1,'Analyse Numerique','test1',0,'systeme lineraire,systeme non lineraire, algebre','http://127.0.0.1:8000/admin/articleApp/article/add/',NULL,'');
INSERT INTO articleApp_article VALUES(3,'Synchronisation Processus','yyyyyyyyyyyyy',0,'processus, processeur, priorité','http://127.0.0.1:8000/admin/articleApp/article/add/',NULL,'');
INSERT INTO articleApp_article VALUES(4,'Preprocessing Methods','mmmmmmmmmmmmm',0,'AI, Preprocessing,data,ML','http://127.0.0.1:8000/admin/articleApp/article/add/','2024-01-25','nnnnnnnnnnnnnnnnnnnaddaaaaaaaaaaaaaaaaaa');
INSERT INTO articleApp_article VALUES(15,'AI Model for Computer games based on Case Based Reasoning and AI Planning','Making efficient AI models for games with imperfect information can be a particular challenge. Considering the large number of possible moves and the incorporated uncertainties building game trees for these games becomes very difficult due to the exponential growth of the number of nodes at each level. This effort is focused on presenting a method of combined Case Based Reasoning (CBR) with AI Planning which drastically reduces the size of game trees. Instead of looking at all possible combinations we can focus only on the moves that lead us to specific strategies in effect discarding meaningless moves. These strategies are selected by finding similarities to cases in the CBR database. The strategies are formed by a set of desired goals. The AI planning is responsible for creating a plan to reach these goals. The plan is basically a set of moves that brings the player to this goal. By following these steps and not regarding the vast number of other possible moves the model develops Game Trees which grows slower so they can be built with more feature moves restricted by the same amount of memory.',0,'Game AI, Case Based Reasoning, AI Planning, Game Trees','https://drive.google.com/file/d/1_yA8WKIPELvsObmed4DGcNggyQUOfvnc/view?usp=drivesdk',NULL,replace('Introduction The goal of this effort is to explore a model for design and implementation of an AI agent for turn based games. This model provides for building more capable computer opponents that rely on strategies that closely resemble human approach in solving problems opposed to classical computational centric heuristics in game AI. In this manner the computational resources can be focused on more sensible strategies for the game play.   With the advancement in computer hardware increasingly more computing power is left for executing AI algorithms in games. In the past AI in games was mainly a cheating set of instructions that simulated the increasing difficulty in the game environment so that the player had the illusion of real counterpart. Improvement in available memory and processing power allows implementation of more intelligent algorithms for building the game environment as well as direct interaction with the human players.    In this particular research the emphasis is put on the interaction between the AI agent and a computer player in the realm of the game rules. It is particularly focused on turn based games that have the elements of uncertainty like dice or concealed information. At the beginning a description of Game AI algorithms are given; such as Game Trees and Minimax. The following section describes an approach of using AI Planning to improve building Game Trees in games with imperfect information where Game Trees tend to be very large with high growth ratio. Section 4 discusses another approach that provides a significant reduction to the number of considered moves in order to find the favorable strategy of the AI player. This approach uses AI Planning techniques and Case Base Reasoning (CBR) to plan for different scenarios in predetermined strategies which would be analogous to human player experience in the particular game. The CBR database illustrates a set of past experiences for the AI problem and the AI Planning illustrates the procedure to deal with the given situation in the game. In the next two sections implementations and evaluations of both approaches are given. The AI Planning approach is implemented with the Tic-tac-toe game and the combined AI Planning and CBR approach is implemented with a model for the Monopoly game. The last part contains conclusions and future work ideas.   2. Game Trees and Minimax  Game Trees are common model for evaluating how different combinations of moves from the player and his opponents will affect the future position of the player and eventually the end result of the game. An algorithm that decides on the next move by evaluating the results from the built Game Tree is minimax [1]. Minimax assumes that the player at hand will always choose the best possible move for him, in other words the player will try to select the move that maximizes the result of the evaluation function over the game state. So basically the player at hand needs to choose the best move overall while taking into account that the next player(s) will try to do the same thing. Minimax tries to maximize the minimum gain. Minimax can be applied to multiple ... levels of nodes on the game tree, where the leaves bring the final  known (or considered) game state.   The minimax theorem states:  For every two-person, zero-sum game there is a mixed strategy  for each player, such that the expected payoff for both is the same  value V when the players use these strategies. Furthermore, V is  the best payoff each can expect to receive from a play of the  game; that is, these mixed strategies are the optimal strategies for  the two players.  This theorem was established by John von Neumann, who is  quoted as saying "As far as I can see, there could be no theory of  games … without that theorem … I thought there was nothing  worth publishing until the Minimax Theorem was proved" [2].  A simple example of minimax can be observed by building a  game tree of the tic-tac-toe game. The tic-tac-toe game is a simple  game which can end by the first player wining, the second player  wining or a tie. There are nine positions for each of the players in  which at each turn the player puts X or O sign. If the player has  three adjacent signs in a row, column or the two diagonals he or  she wins. This game has limited number of position and it is well  suited for building the whole game tree. The leaves of this tree  will be final positions in the game. A heuristics evaluation  function will also need to be written to evaluate the value of each  node along the way.  3. AI Planning for building Game Trees  3.1.1 AI Planning  AI Planning also referred as Automated Planning and  Scheduling is a branch of Artificial Intelligence that focuses on  finding strategies or sequences of actions that reach a predefined  goal [3]. Typical execution of AI Planning algorithms is by  intelligent agents, autonomous robots and unmanned vehicles.  Opposed to classical control or classification AI Planning results  with complex solutions that are derived from multidimensional  space.   AI Planning algorithms are also common in the video game  development. They solve broad range of problems from path  finding to action planning. A typical planner takes three inputs: a  description of the initial state of the world, a description of the  desired goal, and a set of possible actions. Some efforts for  incorporating planning techniques for building game trees have  also shown up, similar to the approach explored in this effort. In  addition Cased Based Reasoning [4] techniques are also gathering  popularity in developing strategies based in prior knowledge  about the problems in the games. One of the benefits from  Hierarchical Task Network (HTN) [5] planning is the possibility  to build Game Trees based on HTN plans; this method is  described in the following section.  3.2 Game Trees with AI Planning  An adaptation of the HTN planning can be used to build  much smaller and more efficient game trees. This idea has already  been implemented in the Bridge Baron a computer program for  the game of Contact Bridge [6].  Computer programs based on Game Tree search techniques  are now as good as or better than humans in many games like  Chess [7] and checkers [8], but there are some difficulties in  building a game tree for games that have imperfect information  and added uncertainty like card or games with dice. The main  problem is the enormous number of possibilities that the player  can choose from in making his move. In addition some of the  moves are accompanied with probabilities based on the random  elements in the games. The number of possible moves  exponentially grows with each move so the depth of the search  has to be very limited to accommodate for the memory  limitations.   The basic idea behind using HTN for building game trees is  that the HTN provides the means of expressing high level goals  and describing strategies how to reach those goals. These goals  may be decomposed in goals at lower level called sub-goals. This  approach closely resembles the way a human player usually  addresses a complex problem. It is also good for domains where  classical search for solution is not feasible due to the vastness of  the problem domain or uncertainties.  3.2.1 Hierarchical Task Networks  The Hierarchical Task Network, or HTN, is an approach to  automated planning in which the dependency among actions can  be given in the form of networks [9] [Figure 1].  A simple task network (or just a task network for short) is an  acyclic digraph     in which U is the node set, E is the  edge set, and each node 	    contains a task . The edges of  define a partial ordering of U. If the partial ordering is total, then  we say that  is totally ordered, in which case  can be written as  a sequence of tasks   \r    . Figure 1: Simple Hierarchical Task Network  A Simple Task Network (STN) method is a 4-tuple of its name,  task, precondition and a task network. The name of the method  lets us refer unambiguously to substitution instances of the  method, without having to write the preconditions and effects  explicitly. The task tells what kind of task can be applied if the  preconditions are met. The preconditions specify the conditions  that the current state needs to satisfy in order for the method to be  applied. And the network defines the specific subtasks to  accomplish in order to accomplish the task.  A method is relevant for a task if the current state satisfies the  preconditions of a method that implements that task. This task can  be then substituted with the instance of the method. The  substitution is basically giving the method network as a solution  for the task.  If there is a task “Go home” and the distance to home is 3km  [Figure 2] and there exists a method walk-to and this method has a  precondition that the distance is less than 5km, then a substation  to the task “Go home” can be made with this method instance.   Figure 2: HTN Method  Buy milk Go to (shop) Purchase  Go to (home) Go-to (from, to) Walk (to) If (to – from) < 5km  296 DIMEA 2008 3rd International Conference on Digital Interactive Media in Entertainment and Arts If the distance is larger than 5km another meth to be substituted [Figure 3].  Figure 3: HTN Method 2  An STN planning domain is a set of operatio methods M. A STN planning problem is a 4-tu state S0, the task network w called initial task STN domain. A plan   \r    is a soluti problem if there is a way to decompose w into π and each decomposition is applicable in the ap the world. The algorithm that is capable to  networks into plans is called Total-forward-deco [9] or Partial-forward-decomposition (PFD). H cases where one does not want to use a forwa procedure. HTN planning is generalization of S gives the planning procedure more freedom construct the task networks.   In order to provide this freedom, a bookke is needed to represent constraints that the plann not yet enforced. The bookkeeping is done by unenforced constraints explicitly in the task netw The HTN generalizes the definition of a STN. A task network is the pair     w task nodes and C is a set of constraints. Eac specifies a requirement that must be satisfied by a solution to a planning problem.   The definition of a method in HTN also definition used in STN planning. A HTN pla name, task, subtasks, and constraints. The s constraints form the task network. The HTN plan identical to STN planning domains except they u instead of STN methods.  Compared to classical planners the prim HTN planners is their sophisticated knowledge r reasoning capabilities. They can represent and  non-classical planning problems; with a good guide them, they can solve classical planning p magnitude more quickly than classical or neoc The primary disadvantage of HTN is the nee author to write not only a set of planning opera of methods.  3.2.2 HTN Planning in building Game  For a HTN planning algorithm to be adap trees we need to define the domain (set of H operators) which is the domain of the game. Thi a knowledge representation of the rules of the environments and possible strategies of game pla In this domain the game rules as well as kn tackle specific task are defined.   The implem Tree building with HTN is called Tign implementation  uses  a  procedure  simila decomposition, but adapted to build up a game  Drive(to If(t Go-to (from, to)  If(to – from) < 5km  Walk (to)  hod instance needs  ons O and a set of  uple of the initial  k network and the  ion for a planning  π if π is executable  ppropriate state of  decompose these  omposition (TFD)  However there are  ard-decomposition  STN planning that  m about how to  eeping mechanism  ning algorithm has  y representing the  work.  a task network in  where  is a set of  h constraint in C  y every plan that is  o generalizes the  an is a 4-tuple of  subtasks and the  nning domains are  use HTN methods  mary advantage of  representation and  solve a variety of  d set of HTNs to  problems orders of  classical planners.  ed of the domain  ators but also a set  Trees ted to build game  HTN methods and  is is in some sense  e game, the game  ay. nown strategies to  mentation of Game  num2 [9]. This  ar  to  forward- tree rather than a  plan. The branches of the game tree rep the methods. Tignum2 applies all met state of the world to produce new continues recursively until there are n have not already been applied to th world.   In the task network generated by Tignu actions will occur is determined by th By listing the actions in the order  network can be “serialized” into a gam 4. Case Based Reasoning in 4.1 Case Based Reasoning Case-based reasoning (CBR) is a  Artificial Intelligence (AI), both as  problems and as a basis for standalone  Case-based reasoning is a paradigm solving and learning that has became  applied subfield of AI of recent yea intuition that problems tend to recur. I are often similar to previously en therefore, that past solutions may be of [10].   CBR is particularly applicable to probl available, even when the domain is n for a deep domain model. Helpdesks, systems have been the most successfu to determine a fault or diagnostic  attributes, or to determine whether or repair is necessary given a set of past s Figure 5: Game Tree built fr Figure 4: HTN to Game Tr ) to – from) < 200km  present moves generated by  thods applicable to a given  w states of the world and  no applicable methods that  he appropriate state of the  um2, the order in which the  e total-ordering constraints.  they will occur, the task  me tree [Figure 4] [Figure 5].  n Game Strategies well established subfield of  a mean for addressing AI  AI technology. m for combining problem- one of the most successful  ars. CBR is based on the  It means that new problems  ncountered problems and,  f use in the current situation  lems where earlier cases are  not understood well enough  , diagnosis or classification  ul areas of application, e.g.,  an illness from observed  r not a certain treatment or  olved cases [11].  rom HTN ree Algorithm Interactive and Adaptable Media 297 3rd International Conference on Digital Interactive Media in Entertainment and Arts Central tasks that all CBR methods have to deal with are [12]: "to  identify the current problem situation, find a past case similar to  the new one, use that case to suggest a solution to the current  problem, evaluate the proposed solution, and update the system by  learning from this experience. How this is done, what part of the  process that is focused, what type of problems that drives the  methods, etc. varies considerably, however".   While the underlying ideas of CBR can be applied  consistently  across  application  domains,  the  specific  implementation of the CBR methods –in particular retrieval and  similarity functions– is highly customized to the application at  hand.  4.2 CBR and Games  Many different implementations of CBR exist in games.  CBR technology is nicely suited for recognizing complex  situations much easier and more elegant than traditional parameter  comparison or function evaluation. There are especially evident  cases in real time strategies where different attack and defense of  global strategies are nicely defined by CBR datasets and later used  in the running games. Also intelligent bots behavior is also  another typical example. Depending on the number of enemy bots  the layout of the terrain and position of human players the CBR  system finds the closest CBR case and employs that strategy  against the human players which in prior evaluation was proved to  be highly efficient.  5. Game Trees with AI Planning – Tic-tac-toe  In order to show the expressive power of AI Planning in  defining strategies for games, and the use of these plans to build  Game Trees I implemented an algorithm that builds Game Trees  for the Tic-Tac-Toe game.  The game tree of Tic-Tac-Toe shows 255,168 possible  games of which 131,184 are won by X (the first player), 77904  are won by O and the rest 46,080 are draw [13]. All these games  can be derived from building a complete Game Tree.   Even though it is possible to build a complete game tree of  Tic-tac-toe it is definitely not an optimal solution. Many of the  moves in this tree would be symmetrical and also there are a many  moves that would be illogical or at least a bad strategy to even  consider.   So what strategy should X (the first player) choose in order  to win the game?  There are few positions that lead to certain victory. These  positions involve simultaneous attack on two positions so the  other player could not defend, basically the only trick in Tic-Tac- Toe.  Figure 6: Tic-tac-toe winning strategy positions  Position 1 leads to victory if the two of the three fields: top  middle, bottom left corner and bottom right corner are free  [Figure 6].  Position 2 lead to victory if two of the three fields: top right  corner, bottom right corner and bottom middle are free [Figure ].  And in the third position if the two of center, middle top and  middle left are available the position is a certain victory.  There are many different arrangements of the player’s tokens  that give equivalent positions as these three positions. By using  planning we do not need to consider all possible layouts but just  consider these three similar to what a human would consider.   The game starts from an empty table.  The two relevant strategies that would lead to these positions  are to take one corner or to take the center [Figure 7].  Figure 7: Tic-tac-toe Two starting moves  The center position as we can see in the simulation results  lead to a bigger number of victorious endings but it is also a  straight forward strategy with obvious defense strategy.  At this point we need to consider the moves of the opponent.  If we take the left branch the opponent moves can be a center, a  corner or a middle field. We also need to differentiate with a  move to a corner adjacent with our like top left or bottom right or  across the center to bottom right [Figure 8].  Figure 8: Tic-tac-toe opponent response to corner move  In cases one and two, we have a clear path to executing  strategy 3 so we need to capture the diagonally opposite field.  And as for the third case the best way to go is to capture the center  and go for strategy 1 or 2 depending of the opponent’s next move.   Figure 9: Tic-tac-toe move 2 after corner opening  The first move leads to certain victory, O will have to go to  the center and X will achieve strategy 3 [Figure 9]. The second  move is a possible way to strategy 3 if O makes a mistake in the  next loop, so X goes to the opposite corner. For the third case  since O is playing a valid strategy the only move that leaves a  possible mistake from O would be to take the center and wait for  O to go to the middle and then achieve strategy 1 or 3 which will  be a symmetric situation to the one that we will find if we  branched with the center.  Figure 10: Tic-tac-toe opponent response to center move  If we go back to the second branch [Figure 10], a possible  way for the second player to engage is corner or middle. The first  298 DIMEA 2008 3rd International Conference on Digital Interactive Media in Entertainment and Arts move is a valid strategy for O and can be mee corner move from X to try a mistake from O in  the same as in the third case above from the pre another move would be go to the middle wh achieves strategy 1 or 2.   Figure 11: Tic-tac-toe Move 2 after cent The fist move will lead to win if O moves  draw if it goes for the corners [Figure 11]. In t has to block the lower left corner which leave middle left or corner left which are strategy 1 and To sum the strategies for the planning, first  corner strategy for the beginning. Then for the ce the corners with the particularly the one oppo holds. If the center is empty for the second strate we go for the opposite corner. After this point w opponent or try to implement strategies 1, 2 or victory.   Plan 1: Take center   Preconditions: Center empty  Plan 2: Take corner   Preconditions: All corners empty  Plan 3: Take corner after center  Preconditions: We have center take corner oppos opponent has  Plan 4: Take diagonal corner  Preconditions: We have a corner, the opponent ha  the corner opposite to the one we have is free.  Plan 5: Block  Precondition: The opponent has tree tokens in a r agonal  Plan 6: Win  Preconditions: We have two tokens in a row, colu nd the third place is free  Plan 7: Tie  Preconditions: If all places are taken, it’s a tie.  5.1 Hierarchical Task Network  Top level task is Play [Figure 12]. This is a  can be derived into: Win, Block, Tie or Sear Search for plan is derived to both Plan 1 and Pla Plan 4, which later leads to a call for the oppon recursive call to Play.  Figure 12: Tic-tac-toe HT et with a opposite  the future exactly  evious branch, and  here X eventually  ter opening to the middle or a  the second case O  es X to go for the  d 2. we have center or  enter we try to get  osite to the one O  egy we go for it or  we either block the  r 3 which lead to  site to the  one the  as the ce−nter and row, colu−mn or di mn or dia−gonal a a complex task and  rch for Plan. The  an 2 or Plan 3 and  nent’s move and a  TN This HTN when executed will re game scenarios. By creating nodes from them with branches with the move of t tree for the Tic-tac-toe game over whi algorithm.  This set up with 7 plans with 3 ta for Tic-tac-toe which considers all pos player with only 457 games, 281 of w and 0 where the second opponent w reduction over the 255, 168 possible g tree. These reductions can be very use computing capabilities but also we pr that planning can be very efficient if d trees by applying reasoning very  reasoning.  Further improvements to the gam the opponents moves are also planned all the meaningless and symmetrical m 6. Game AI in Monopoly  6.1 Overview of the AI Imp The AI agent is responsible for  players in the game. The core principle a Game Tree with all the sensible move make from the current point of time minimax algorithm the agent selects t would bring the computer player mo with the highest probability. Building  that would be big enough to consider  is obstructed by the vastness of poss with all the possible random landings  nodes of the game tree exponentially tackle this problem the AI agents  discussed technologies: Case Based Re The technologies are employed  First the agent searches the CBR datab largest similarity with the current state associated with a playing strategy. Th that the planner needs to build plans f consecutive player moves that bring th way only moves that are part of that str being a small fraction of the overall po edges of the game tree at each level dec At each level of the game tree the of a single player. After the strateg considered the response to those strate by the opponent(s). The move of the  probability distribution of the dice as  player. A more general strategy needs opponent’s (human player) moves sin the expertise of the opponent. This ge more plausible moves than the focused After covering all opponents t deducting a feature move of the com CBR selected plan strategy. After  strategies and reaching a reasonable s into account the memory limits an probabilities that the move is possible the dice the building of the Game Tre algorithm searches the Game Tree  favorable move for the AI player usi The process is repeated each time the A esult with plans for possible  m each position and linking  the player we create a game  ich we can run the minimax  arget strategies creates a tree  ssible moves for the second  which X wins 176 are draw  wins. This is a significant  ames with a complete game  eful for devices with limited  rove a very important point  designing meaningful game  similar to human player  me tree are also possible if  d, in other words if we drop  moves of the opponent.  plementation the moves of the artificial  e of the AI agent is building  es that all the players would  e forward. Then using the  the move that in the future  ost favorable game position  a Game Tree in this game  sufficient number of moves  sible moves in combination  of the dice. The number of  y grows at each level. To  incorporates two already  easoning and AI Planning.  in the following manner.  base to find the case with the  e of the board. This case is  he strategy consists of goal  for, and the plans consist of  he player to that goal. This  rategy are considered, those  ossible moves the number of  creases immensely.  e model considers the moves  gies of the AI player are  egies needs to be considered  opponent(s) depends of the  well as the strategy of the  s to be implemented for the  nce we cannot be aware of  eneral strategy would bring  d strategy of the AI player.   the agent comes back to  mputer player by using the  creating several loops of  size of a Game Tree taking  nd the rapidly decreasing  e due to the distribution of  ee stops. Then the minimax  and decides on the most  ing the minimax algorithm.  AI player is up.  Interactive and Adaptable Media 299 3rd International Conference on Digital Interactive Media in Entertainment and Arts Buying, auctioning and trading game moves are always  accompanied by return of investment calculations in making the  plans. These calculations represent adaptation of the more general  planning associated with the cases in the CBR database. These  adaptations are necessary due to the fact that the cases do not  identically correspond to the situation on the table. In addition  calculating the game position value of each node of the game tree  is done by heuristic functions that incorporate economic  calculations of net present value, cash, and strategic layout and so  on. For example railroads in monopoly are known to be  strategically effective because they bring constant income even  though the income can be smaller than building on other  properties.   6.2 Details on the CBR Implementation  The implementation of the CBR is by using the JColibri2  platform.  JColibri2 is an object-oriented framework in Java for  building CBR systems that is an evolution of previous work on  knowledge intensive CBR [14].   For this implementation we need to look into three particular  classes of the JColibri2 platform. The StandardCBRApplication,  Connector, CBRQuery. For a JColibri2 implementation the  StandardCBRApplication interface needs to be implemented.   The CBR cycle executed accepts an instance of CBRQuery.  This class represents a CBR query to the CBR database. The  description component (instance of CaseComponent) represents  the description of the case that will be looked up in the database.  All  cases  and  case  solutions  are  implementing  the  CaseComponent interface.  The JColibri2 platform connects to the CBR database via a  Connector class. Each connector implements all the necessary  methods for accessing the database, retrieval of cases, storing and  deletion of cases. This implementation uses a custom XML  structure for holding the CBR cases. Since the game will not  update the CBR database only read it, a XML solution satisfies  the needs. The XML file to a certain extent is similar to the XML  representation of the board. We are interested in finding one  CBRCase that is the most similar case to the situation in the game  at the time of the search. This procedure is done in the cycle  method of the CBRApplication. The JColibri2 CBR comparison is  done by Nearest Neighbor (NN) search method.   JColibri2 offers implementations for NN search algorithms  of simple attributes. These implementations are called local  similarities. For complex attributes like in our case global  customized similarity mechanisms need to be implemented.  The MonopolyDescription class [Figure 13] is basically a  serialization of the GameState. It holds all the information about  the state of the board, the players, their amount of cash etc.   Figure 13: Class diagram of the Monopoly Case component  models  On the other hand the MonopolySolution class holds the  three particular attributes that are needed for the planning, the  planning Domain, State and TaskList.  The game is implemented by using the Model-View- Controller software development pattern. The controller is  responsible for implementing the game rules and handling all of  the events in the game like roll of dice, input commands for  trading, auctioning and etc from the players. The View layer is  responsible for displaying the board and all of the input widgets  on to the game screen, and the models are data structures  representing the game state [Figure 14].  Figure 14: Class diagram of the Monopoly models  6.2.1 Complex Similarity representation in CBR  The similarity measurement part of the Nearest Neighbor  algorithm JColibri2 is implemented by implementing the  LocalSimiralrityFunction  and  the  GlobalSimiralityFunction  interface. A local similarity function is applied to simple attributes  by the NN algorithm, and a global similarity function is applied to  compound attributes. In the case of our implementation the  attributes of the MonopolyDescription are compound attributes  describing the state of the board, number of players, amount of  cash for every player and etc. Since MonopolyDescription is a  custom CaseComponent a global similarity function needs to be  implemented to accurately find the distance between different  CBR cases.  The similarity mechanism is inseparable core element of the  CBR system. This mechanism represents how the CBR decides  which strategy is best suited for the particular situation by  300 DIMEA 2008 3rd International Conference on Digital Interactive Media in Entertainment and Arts calculating the distance or similarity to other cases in the  database.   For the monopoly implementation we need to consider  several basic strategies. Monopoly is based on investing in  properties and receiving revenues from those investments. One of  the basic strategies of the game is to build a set of properties that  will bring constant income larger than the one of the opponents.  So in time the opponents will have to declare bankruptcy. But on  the other hand over investment can lead to too stretched resources  with low income that will eventually drove the player to  bankruptcy. To decide on these two we need a clear separation  into two groups of cases in the CBR database. The first group of  cases will represent a situation on the board where the player has  significant income per loop formed of one or more color group  properties, maybe railroads, some buildings on them and so on. It  is important to note that in this case the player is better situated  than his opponents so he only needs to survive long enough to win  the game. In the other group of cases either the opponent is not  well positioned on the board or its opponents are better situated.  In this case further investments are necessary to improve the  situation so the player can have a chance of winning in the long  run.   These metrics can be owning color groups, valuing groups of  railroads, evaluating the other opponents as well, and considering  the amount of cash. As it is obvious in monopoly the number of  streets is not as nearly as important as the combination of streets  the player owns. It is also important to note that one CBR case  does not hold only a single strategy in place, but its solution can  have multiple different strategic goals. For example one CBR case  might simultaneously say buy this land to form a color group but  also trade some other unimportant property to increase cash  amount.   The cases do not represent all possible combinations of board  positions. They are only representation of typical game scenarios.  The CBR Case solutions do not give exact instructions in general  but rather strategic goals. For example one CBR Solution might  say trade the streets that you only have one of each for the ones  that you have two of that color already. Then the planner based on  the situation on the board needs to decompose this high level task  to a low level operations. Like offer "Mediterranean Avenue" for  "Reading Railroad" and offer $50. The exact amounts and actual  streets are left to the planer to evaluate.   The monopoly CBR database is currently in development on  a monopoly clone game called Spaceopoly. The cases are  architected based on human player experience and knowledge.  There is a plan of making a number of slightly different strategies  that differ on the style of playing and then running simulation  tests that would determine the particular validity of each database  as well as validity of certain segments of the strategy or even  particular cases in the database.   The actual execution of the strategies will not differ from  strategy to strategy since the plan execution is more related to the  structure and rules of the game than to the actual playing strategy.  6.3 Details on the Planning Implementation  For the purpose of planning this implementation uses a  modification of the JSHOP2 planner. The Java Simple  Hierarchical Ordered Planner 2 is a domain independent HTN  planning system [15].   JSHOP2 uses ordered task decomposition in reducing the  HTN to list of primitive tasks which form the plans. An ordered  task decomposition planner is an HTN planner that plans for tasks  in the same order that they will be executed. This reduces the  complexity of reasoning by removing a great deal of uncertainty  about the world, which makes it easy to incorporate substantial  expressive power into the planning algorithm. In addition to the  usual HTN methods and operators, the planners can make use of  axioms, can do mixed symbolic/numeric conditions, and can do  external function calls.   In order for the JSHOP2 planer to generate plans it needs  tree crucial components: Domain, State and Tasks. The Domain  defines all the functionalities that the particular domain offers.  These are simple and complex tasks. The complex tasks also  called methods create the hierarchy with the fact that they can be  evaluated by simple tasks of other complex tasks. This is how a  hierarchical structure of tasks is formed. The problem reduction is  done by reducing the high level complex tasks to simpler until all  the tasks are primitive. The list of primitive tasks forms the plan.  The State represents the state of the system. It is a simple  database of facts that represent the state of the system. The State  is necessary to determine the way the problems or tasks are  reduced to their primitive level. The reduction is done by  satisfying different prerequisites set in the methods; these  prerequisites are defined in the state. The Tasks are high level  tasks or methods defined in the Domain. The planner based on the  State and the goals selects one or more high level tasks that need  to be reduced to plans [Figure  15].  Figure 15: Diagram of a Planner  The plans then generate the game moves. The number of  moves generated by the plans is just a fraction of the possible  moves at that point. This reduces the game tree providing the  opportunity to generate smaller and deeper game trees and making  more efficient decisions in general.   7. Conclusion  Even though the results from the CBR database are not  complete at this time partial strategies are implemented as cases  and recognized during game play by the CBR system. These  smaller local strategies coupled with more global higher level  strategies that are particularly important at the beginning of the  game would form a complete CBR database and represent a  knowledge engineered style of playing of the AI player.   The AI Planning approach is a proven method by the tic-tac- toe experiment and is suitable for implementing the strategies  associated with the CBR cases.  This approach in general benefits from both technologies,  CBR as well as AI Planning and comprises an elegant solution.  Even though AI Planning can be enough as a single technology  for some simpler problems like tic-tac-toe the complexity of  Monopoly would mean that the Planner would have to incorporate  Core Planner  Tasks Plan State Interactive and Adaptable Media 301 3rd International Conference on Digital Interactive Media in Entertainment and Arts large and complex domain and a very big state model. The CBR  application helps reduce this complexity by focusing the planning  on smaller domain of the game. Basically the CBR reduces the  overall goal of the play (wining the game) to smaller more  concrete goals suitable to the particular state of the game, thus  reducing the need for global planning strategies and complex  planning domain.   Furthermore this symbiosis of technologies gives way for  more precise and finely tuned strategies which can be difficult to  include into global plan for the whole game. One simple example  for the Monopoly game would be this: Sometimes it’s better to  stay in jail because rolling double increases the probability of  landing on some field (two, four, six, eight, ten or twelve steps  from the jail) that can be of great importance to the rest of the  game. These and similar small local strategies can be easily  recognized by similar cases in the CBR database.   In other words the system is flexible enough so that new  strategies can be incorporated easily missing strategies can be also  recognized by the distance metrics as well as wrong assumptions  in the strategies can be easily recognized.  One other important property of the system is that is highly  configurable. The game its self can be diversely different  depending on the configuration of the board. Even though the  platform is restricted to Monopoly type of games, changing the  layout and values of the fields effectively brings completely  different properties of the game. In addition the CBR database  represents the entire experience of the AI Player. It can be filled  with rich set of strategies or even configured with different flavors  of difficulties of play, this of course coupled with the domain of  the planner which can differ from a case to a case as well.   8. Future Work  Further exploration of this technology would go towards  complete implementation of an AI aware agent for monopoly.  Initial results from the local cases with more specific strategies  show CBR as a capable tool for representing expertise in playing  the game. Completing the more general strategies and coupling  them with the planning domain will give precise results on the  benefits from this architecture.  There is also need for exploring the planning of strategies of  opponents. This task is to some extent different because we  cannot always expect the opponent to select the best move we  think. In the Tic-tac-toe example all possible moves of the  opponent were taken into consideration, if we used the same  planner for the opponent only tie games would result from the  game tree. In other words mistakes of the players also need to be  considered.   The CBR Platform brings other functionalities well worth of  exploring as well. The revision stage of the JColibri2 platform is  basically capable of fine tuning strategies or even developing new  strategies for the games. A well written underlying AI planning  model with a capable feedback of the game tree evaluation back  to the CBR revision capability can be an interesting concept in  automatic experience acquisition for the AI model.  There are also many other fields were combined CBR and  planning approach can be incorporated into a problem solution.  This combination is analogous in a big extent to a human way of  reasoning. People in addition to logic of reasoning in situations  with lack of information rely to planning strategies and prior  experience, exactly the intuition behind CBR – AI Planning  architecture.   9. ACKNOWLEDGMENTS  We would like to thank Prof. Sofia Tsekeridou for her  involvement in the valuable discussions we had on the topic of  CBR.  10. ','\r',char(13)));
INSERT INTO articleApp_article VALUES(17,'How to Teach Software Modeling','To enhance motivation of students to study software engineering, some way of ﬁnding balance between the scientiﬁc aspect and the practical aspect of software engineering is required. In this paper, we claim that teaching multiple software modeling techniques from a uniﬁed viewpoint is a good way of obtaining the balance and attracting the students’ interest as well.',0,'software modeling, software engineering education, UML','https://drive.google.com/file/d/14Wb_J_ZKLlUDs47LaMQBHCaYJpg2ElCi/view?usp=drivesdk',NULL,' INTRODUCTION Software engineering education at universities faces a common problem; that is regular students do not usually have experience of developing software for practical use and thus are not motivated for software engineering aiming at high quality software production by a project team or a persistent organization. Software projects conducted by students simulating real scale software development may help enhance students’ motivation, although it requires a lot of efforts to prepare such projects and manage them. Another way of solving this problem is to teach those who already have real experience in industry. In our case, there are currently ﬁve Ph. D. students under the author’s supervision who are working at companies as well as doing research in our lab. As a by-product, interactions between the part-time students and the other regular students stimulate each other, particularly enlightening the regular students to practical software issues. However, too much emphasis on practicality may bring negligence to scienceand technology and may generate anti-intellectualism. A good balance between the scientiﬁc aspect and the practical aspect of software engineering should always be pursued. In our view, teaching various software modeling techniques is a good way to achieve balanced software engineering education. It is needless to say that model is a key concept and modeling is an essential skill in software engineering. There are a variety of modeling techniques; some are intuitive and quite accessible to novices, while some are highly sophisticated and attract theory oriented stu- dents and researchers. In this paper, we would like to show that it is effective to teach multiple modeling techniques from a uniﬁed viewpoint. It is based on our experience of teaching software engineering courses at several universities in Japan. Recently, the author published a textbook on software engineering, speciﬁcally focused on software model- ing (unfortunately, it is written in Japanese)[1]. The book covers the whole area of software engineering, including design, testing and evolution but the modeling part has a role of attracting interests of intelligent students, who may not have much experience in developing real scale software systems. It also gives a consistent viewpoint penetrating through various techniques employed in different stages of software engineering. MODELING TECHNIQUES In software engineering, models are used for various purposes, e.g. life cycle model, process model, project model, product model, quality model, domain model, requirements model, design model, object model, data model, etc. In the following, we basically focus on requirements and design models but most of the discussions will hold for other kinds of models. Teaching modeling is almost equal to teaching abstraction. Models are constructed through capturing the crucial properties and structure of the target, abstracting away irrelevant details. Thus, learning how to model is a good training for mastering abstraction. 2.1 Graph Representation of Models Many software models are represented with diagrams. Wide acceptance of UML symbolizes the trend that diagrams are often preferred to textual languages. Among many types of diagrams, graph structured diagrams are by far the most widely used. The reasons may be as follows. 1. A most fundamental way for human mind to understand the world is by regarding it as consisting of a set of conceptual units and a set of relations between them. Conceptual units can be naturally illustrated with boxes or circles or whatever closed ﬁgures and relations can be illustrated with lines or arrows connecting such ﬁgures, corresponding to vertices and edges of graphs, respectively.2. It is easy to draw graph structured diagrams by hand or with drawing tools. 3. Concepts and algorithms of the graph theory are available and often useful in analyzing models represented by graphs. A typical example is reasoning on transitive relations by trac- ing along paths of graphs. Also, the concept of subgraph is highly useful in decomposinghigher-level models or cluster- ing lower-level models. Accordingly, a number of models share the same structure of graphs. Table 1 shows graph structures of some typical models. Table 1: Graph structures of typical models model vertex edge Data ﬂow process data ﬂow ER entity relationship State transition state transition JSD process data stream connection state vector connection Activity activity control ﬂow Petri net place, transition ﬁre and token ﬂow 2.2 Commonality and Difference between Models It is pedagogical to let students notice the common structure shared by a number of models. However, the apparent resemblance often causes confusion. Such confusion can be observed not only in software modeling graphs but in many diagrams found in daily newspapers, magazines, reports, proposals and other documents. It is often the case that one vertex denotes a type of things and an- other denotes quite a different type on the same diagram or one type of edges co-exist with edges with different meaning. Thus, it is important to make students consciously aware the differences between different models. We often experience that when we let students draw data ﬂow diagrams who appear to have understood the data ﬂow model perfectly, the diagrams turn out to be some- thing like control ﬂow graphs. To show the difference, it is instructive to categorize models rep- resented by graphs. Basically, there are two categories. 1. Static models: An edge connecting vertex A and vertex B represents a rela- tion between A and B. When the edge is undirected, it means “A and B are in some relation” and when directed, it means “A has a relation with B”. Typical examples include entity relationship model, class diagram and semantic network. 2. Dynamic models: An edge from vertex A to B denotes a move from A to B. The edge in this case is always directed. There are two sub- categories: (a) The case where a view of control moves from A to B. Examples are control ﬂow model and state transition model. (b) The case where data or objects ﬂow from A to B. Exam- ples are data ﬂow model, work ﬂow model, and trans- portation ﬂow model. Static models and dynamic models may not be easily confused but confusion betweendifferent dynamic models are often observed, e.g. data ﬂow and control ﬂow or state transition and activity tran- sition. Since graphs are intuitively understandable, their semantics are apt to be understood ambiguously or misunderstood. 3. UML UML diagrams can also be viewed in terms of graph structures. Table 2 shows graph structures of ﬁve UML diagrams. Table 2: Graph structures of UML diagrams diagram vertex edge class diagram class generalization, composition, association state machine state transition activity diagram activity control ﬂow collaboration diagram object message ﬂow sequence diagram message message ﬂow anchor point It is usually not desirable to teach UML per se. UML is a col- lection of miscellaneous diagrams and its speciﬁcation is continu- ously changing. For the pedagogical purpose, UML had better be regarded as a catalogue of analysis and design know-how collected around diagrammatic representations. Diagrams should be selected according to the policy of how to teach modeling methods. Each UML diagram contains overly rich constructs, which some- times blur the essential property of the model. For example, the activity diagram is essentially a control ﬂow diagram but it also in- cludes a notation for data ﬂow description. From the stance of em- phasizing differences between various models, it is not appropriate to include such ad hoc constructs. By the same token, the collab- oration diagram (, renamed to “communication diagram” in UML 2) is explained to have the equivalent semantics as the sequence diagram. But if that is the case, signiﬁcance of the collaboration diagram is considerably limited. The author prefers to regard it as showing collaboration relations between objects, integrating a set of different sequence diagrams. 4. CONCLUSION Software modeling is important by itself but teaching modeling in the software engineering course has at least two additional mean- ings. One is to give a bird’s-eye view to the whole software engi- neering through the standpoint of modeling technology. The other is to attract interest of good students who may not have much expe- rience in developing a real-scale software but possess intelligence and will to attack complexity of modern software construction. 5. ');
INSERT INTO articleApp_article VALUES(32,'Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model','The emergence of novel types of communication, such as email, has been brought on by the development of the internet, which radically concentrated the way in that individuals communicate socially and with one another. It is now establishing itself as a crucial aspect of the communication network which has been adopted by a variety of commercial enterprises such as retail outlets. So in this research paper, we have built a unique spam-detection methodology based on email-body sentiment analysis. The proposed hybrid model is put into practice and preprocessing the data, extracting the properties, and categorizing data are all steps in the process. To examine the emotive and sequential aspects of texts, we use word embedding and a bi-directional LSTM network. this model frequently shortens the training period, then utilizes the Convolution Layer to extract text features at a higher level for the BiLSTM network. Our model performs better than previous versions, with an accuracy rate of 97–98%. In addition, we show that our model beats not just some well-known machine learning classifiers but also cutting-edge methods for identifying spam communications, demonstrating its superiority on its own. Suggested Ensemble model’s results are examined in terms of recall, accuracy, and precision.',0,'Dataset, KNN, Gaussian Naive Bayes, LSTM, SVM, Bidirectional LSTM, GRU, Word-Embeddings, CNN','https://drive.google.com/file/d/1woqbRPySWFF3Zrm7Ws20xy9er5JYrpVr/view?usp=drivesdk',NULL,'Introduction Over the past few years, a clear surge of both the amount of spammers as well as spam emails. This is likely due to a fact that the investment necessary for engaging in the spamming industry is relatively low. As a result of this, we currently have a system that identifies every email as suspicious, which has caused major expenditures in the investment of defense systems [12]. Emails are used for online crimes like fraud, hacking, phishing, E-mail bombing, bullying, and spamming. [16]. Algorithms that are based on machine learning (ML) are now the most effective and often used approach to the recognition of spam. Phishing, which is defined as a fraudulent attempt to acquire private information by masquerading as a trustworthy party in electronic communication, has rapidly advanced past use of simple techniques and the tactic of casting a wide net; instead, spear phishing uses a variety of sophisticated techniques to target a single high-value individual. Other researchers used NB, Decision Trees, and SVM to compare the performance of supervised ML algorithms for spam identification [6]. Spam emails clog up recipients'' inboxes with unsolicited communications, which frustrate them and push them into the attacker''s planned traps [7]. As a result, spam messages unquestionably pose a risk to both email users and the Internet community. In addition, Users may occasionally read the entire text of an unsolicited message that is delivered to the target users'' inboxes without realizing that the message is junk and then choosing to avoid it. Building a framework for email spam detection is the aim of this project. In this approach, we combine the Word-Embedding Network with the CNN layer, Bi-LSTM, and GRU (BiLSTM+GRU). CNN layers are used to speed up training time before the Bi-LSTM network, and more advanced textual characteristics are extracted with the use of this network in comparison to the straight LSTM network, in less time. Gated recurrent neural networks (GRUs) are then added because they train more quickly and perform better for language modeling. To evaluate and investigate various machine learning algorithms for predicting email spam, and develop a hybrid classification algorithm to filter email spam before employing an ensemble classification algorithm to forecast it. To put an innovative technique into practice and compare it to the current method in terms of various metrics. Ensemble learning, a successful machine learning paradigm, combines a group of learners rather than a single learner to forecast unknown target attributes. Bagging, boosting, voting, and stacking are the four main types of ensemble learning techniques. To increase performance, an integrated method and the combining of two or three algorithms are also suggested. Extraction of text-based features takes a long time. Furthermore, it can be challenging to extract all of the crucial information from a short text. Over the span associated with this 181.IC3 2023, August 03–05, 2023, Noida, India Sachan et al. research, we utilize Bidirectional Large Short-Term Memories (Bi- LSTM) in conjunction with Convolutional Neural Networks (CNN) to come up with an innovative method to the detection of spam. Bagging and boosting approaches were widely preferred in this study. Contribution and paper organization is as follows: section 1.1 describes literature study, section 1.2 describe motivation for this research work, section 2 sketches procedure of details implemen- tation, Section 3 present experimental setup, dataset description and evaluation metrics, and section 4 summarizing outcomes of the experiment. 1.1 Related Work Email is indeed the second most frequently utilized Internet appli- cation as well as the third most common method of cyberbullying, claims one study. Cybercriminals exploit it in a number of ways, including as sending obscene or abusive messages, adding viruses to emails, snatching the private information of victims, and ex- posing it to a broad audience. Spam letters made up 53.95% of all email traffic in March 2020. We examine three main types of un- lawful emails in our study. First are fake emails, which are sent to manipulate recipients to submit sensitive information. The sec- ond as being cyberbullying’s use of harassing emails to threaten individuals. Suspicious emails that describe illegal activities belong to the third category. Many researchers have earlier contributed massively to this subject. The researcher claims there is some proof that suspicious emails were sent before to the events of 9/11. [14]. When it comes to data labeling, there are also convinced rule-based approaches and technologies ( like VADER) that are used, even though their efficiency of the are together is adversely affected. A hidden layer, which itself is essential for vectorization, is the top layer of the model. We use oversampling methods for this minority class because of the absence of data. Sampling techniques can help with multicollinearity, but they have an impact on simulation re- sults. Oversampling causes data to be randomly repeated, which affects test data because dividing data may result in duplicates. Un- dersampling may result in the loss of some strong information. In order to advance email research, it is crucial to provide datasets on criminal activity. P. Garg et al. (2021) [5], which revealed that spam in an email was detected in 70 percent of business emails, spam was established as an obstacle for email administrators. Recognizing spam and getting rid of it were the primary concerns, as spam can be offensive, may lead to other internet sites being tricked, which can offer harmful data, and can feature those who are not particu- lar with their content using NLP. To select the best-trained model, each mail transmission protocol requires precise and effective email classification, a machine learning comparison is done. Our study has suggested that innovative deep learning outperforms learning algorithms like SVM and RF. Current studies on the classification of emails use a variety of machine learning (ML) techniques, with a few of them focusing on the study of the sentiments consisted of within email databases. The lack of datasets is a significant obstacle to email classification. There are few publicly accessible E-mail datasets, thus researchers must use these datasets to test their hy- potheses or gather data on their own. Authors[15] describe supplied two-phased outlier detection models to enhance the IIOT network’s dependability. Artificial Neural Network, SVM, Gaussian NB, and RF (random forest) ensemble techniques were performed to forecast class labels, and the outputs were input into a classifying unit to increase accuracy. A method for content-based phishing detection was presented by the authors in [2], to classify phishing emails, they employed RF. They categorize spam and phishing emails. They enhanced phishing email classifiers with more accurate predictions by extracting features. They showed some effective Machine learn- ing spam filtering techniques. When the PCA method is used, it will lower the number of features in the dataset. The collected features go through the PCA algorithm to reduce the number of features. The PCA method is used to make a straightforward representation of the information which illustrates the amount of variability there is in the data. The authors of [20] presented the Fuzzy C-means method for classifying spam email. To stop spam, they implemented a membership threshold value. A methodology to identify unla- beled data was put forth by the authors of [1] and applied motive analysis to the Enron data collection. They divided the data into categories that were favorable, negative, and neutral. They grouped the data using k-means clustering, an unsupervised ML technique and then classified it using the supervised ML techniques SVM and NB. Hina, Maryam, and colleagues (2021) implemented Sefaced: Deep learning-based semantic analysis and categorization of e-mail data using a forensic technique. For multiclass email classification, SeFACED employs a Gated Recurrent Neural Network (GRU) based on Long Short-Term Memory (LSTM). Different random weight ini- tializations affect LSTMs [9]. Zhang, Yan, et al.(2019) Experiments on three-way game-theoretic rough set (GTRS) email spam filter- ing show that it is feasible to significantly boost coverage without decreasing accuracy [23]. According to Xia et al. [22], SMS spam has been identified using machine learning model such as naive bayes , vector-space modeling, support vector machines (SVM), long selective memory machines (LSTM), and convolutional neural networks including every instance of a method for categorizing data. Elshoush, Huwaida, et al. (2019) Using adaboost and stochastic gradient descent (sgd) algorithms for e-mail filtering with R and orange software spam [3]. Orange software was used to create the classifications, which included Adaboost and SGD. The majority of researchers focused on text-based email spam classification meth- ods because image-based spam can be filtered in the early stages of pre-processing. There are widely used word bag (BoW) model, which believes that documents are merely unordered collections of words, is the foundation for these techniques. Kumaresan [11] explains SVM with a cuckoo search algorithm was used to extract textual features for spam detection. Renuka and Visalakshi made use of svm [17] spam email identification, followed by selecting features using Latent Semantic Indexing (LSI). Here we have used labeled dataset to train the hybrid classifier. We used TF-IDF for feature extraction [20] and Textual features for spam detection were extracted using SVM and a cuckoo search algorithm. [4] for filtering out the spam email. Combining the integrated strategy to the pure SVM and NB methods, overall accuracy is really improved. Moreover, accurate detection for spam email has been proposed using the Negative Selection Algorithm (NSA) and Particle Swarm Optimization’s (PSO) algorithm. PSO is used in this instance to improve the effectiveness of the classifier. 182 Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model IC3 2023, August 03–05, 2023, Noida, India 1.2 Motivation and Novelty Email is most common form of communication between people in this digital age. Many users have been victims of spam emails, and their personal information has been compromised. The email Classification technique is employed to identify and filter junk mail, junk, and virus-infected emails prior to reach a user’s inbox. Existing email classification methods result in irrelevant emails and/or the loss of valuable information. Keeping these constraints in mind, the following contributions are made in this paper: • Text-based feature extraction is a lengthy process. Further- more, extracting every important feature from text is difficult. In this paper, we show how to employ GRU with Convo- lutional Neural Networks and Bidirectional-LSTM to find spam. • Used Word-Embeddings, BiLSTM, and Gated Recurrent Neu- ral Networks to examine the relationships, sentimental con- tent, and sequential way of email contents. • Applied CNN before the Bi-LSTM network, training time can be sped up. This network can also extract more advanced textual features faster than the Bi-LSTM network alone when combined with the GRU network. • We use Enorn Corpora datasets and compute precision, re- call, and f-score to assess how well the suggested technique performs. Our model outperforms several well-known ma- chine learning techniques as well as more contemporary methods for spam message detection. 2 PROPOSED SYSTEM ARCHITECTURE AND MODEL E-mail is a valuable tool for communicating with other users. Email allows the sender to efficiently forward millions of advertisements at no cost. Unfortunately, this scheme is now being used in a variety of organizations. As a result, a massive amount of redundant emails is known as spam or junk mail, many people are confused about the emails in their E- Mailboxes. Each learning sequence is given for- ward as well as backward to two different LSTM networks that are attached to the same outputs layer in order for bidirectional Lstms to function. This indicates that the Bi-LSTM has detailed sequential information about all points before and following each point in a specific sequence. In other words, we concatenate the outputs from both the forward and the backward LSTM at each time step rather than just encoding the sequence in the forward direction. Each word’s encoded form now comprehends the words that come before and after it. This is a problem for the Internet community. The di- agram depicts various stages that aid in the prediction of email spam: Because real-world data is messy and contains unnecessary infor- mation and duplication, data preprocessing is critical in natural language processing (NLP). The major preprocessing steps are de- picted below. 2.1 NLP Tokenization Tokenization of documents into words follows predefined rules. The tokenization step is carried out in Python with spacy library. 2.2 Stop Words Removal Stop words appear infrequently or frequently in the document, but they are less significant in terms of importance. As a result, these are removed to improve data processing. 2.3 Text Normalization A word’s lexicon form or order may differ. Thus, they must all be changed to their root word to be correctly analyzed. Lemmatization and stemming are the two methods that can be used for normal- ization. When a word’s final few characters are removed to create a shorter form, even if that form has no meaning, the procedure is known as stemming. lemmatization [21] is a mixture of corpus- based an rule-based methods, and it retains the context of a term while changing it back to its root. 2.4 Feature Extraction feature extraction which transforms the initial text into its features so that it may be used for modeling after being cleaned up and normalized. Before predicting them, we use a specific way to give weights to specific terms in our document. While it is simple for a computer to process numbers, we choose to represent individual words numerically. In such cases, we choose word embeddings. IDF is the count of documents containing the term divided by the total number of documents, and occurrence is the amount of instances a word appears in a document. We derive characteristics based on equations. 1,2,3,4,5, and 6. We use equations to derive properties. 𝑇 𝑓 𝐼𝑑𝑓 = 𝑡𝑓 ∗  1 𝑑𝑓  (1) 𝑇 𝑓 𝐼𝑑𝑓 = 𝑡𝑓 ∗ Inverse(𝑑𝑓 ) (2) 𝑇 𝑓 𝐼𝑑𝑓 (𝑡,𝑑, 𝐷) = 𝑇 𝑓 (𝑡,𝑑).𝐼𝑑𝑓 (𝑡, 𝐷) (3) 𝑇𝐼𝑑𝑓 (𝑡,𝑑) = log 𝑁 |𝑑𝜖𝐷𝑡𝜖𝐷| (4) A word2vec neural network-based approach is the method that is utilized for this goal as the tool. The following equation, referred to as 5, shows how word2vec handles word context through the use of probability-accurate measurements. Here letter D stands for the paired-wise display of a set of words, while the letters w and c0 or c1 represent paired word context that originated from a larger collection of set D. 𝑃 (𝐷 = 1 | 𝑤,𝑐11:𝑘) = 1 1 + 𝑒−(𝑤·𝑐11+𝑤·𝑐12+...+𝑤·𝑐1𝑘) (5) 𝑃 (𝐷 = 1 | 𝑤,𝑐1:𝑘) = 1 1 + 𝑒−(𝑤·𝑐0) (6) 183 IC3 2023, August 03–05, 2023, Noida, India Sachan et al. 2.5 Word-Embeddings Word-Embedding helps to improve on the typical "bag-of-words" worldview, which requires a massive sparse feature vector to score every word individually to represent this same entire vocabulary. This perception is sparse because the vocabulary is large, and each word or document is defined by a massive vector. Using a word map-based dictionary, word embedding needs to be converted terms (words) into real value feature vectors. There are two basic issues with standard feature engineering techniques for deep learning. Data is represented using sparse vectors, and the second is that some of the meanings of words are not taken into consideration. Similar phrases will have values in embedding vectors that are almost real-valued. The Input length in our proposed study is set to 700 for our suggested model. If the texts seemed to be integer encoded with value systems between 10 and 20, the vocabulary distance would be 11. Our data is encoded as integers, and the input and output dimensions are both set to 50,000. The embedding layer outcome will be used in successive layers and for BiLSTM and GRU layers. 2.6 Machine Learning Model Within the scope of the research, we are using the subsequent ma- chine learning techniques, to examine and compare the overall efficacy of our suggested Bi-LSTM strategy: Support Vector Ma- chine, Gaussian NB, Logistic Regression, K - nearest neighbors, and Random Forest (RF). 2.7 Convolution Network The popular RNN model generally performs well but takes too long to train the model incorporating the textual sequential data. When a layer is added after the RNN layer, the model’s learning duration is considerably decreased. Higher-level feature extraction is another benefit. [19] additionally possible using the convolutional layer. In essence, the convolution layer looks for combinations of the various words or paragraphs in the document that involve the filters. We use features with 128 dimensions and a size 10 for each. For this task, the Relu activation function is utilized. After that, the one-dimensional largest pooling layers with a pooling size of 4 are put on the data in order to obtain higher-level features. 2.8 BiLSTM Network with GRU Recurrent Neural Network (RNN) technique of text sentiment anal- ysis is particularly well-liked and frequently applied. Recurrent neural networks (RNN) surpass conventional neural networks. be- cause it can remember the information from earlier time steps thanks to its memory. A state vector is combined with an RNN’s data to create a new state vector. The resulting state vector uses the present to recollect past knowledge. The RNN is straightforward and is based on the following equations: ℎ𝑡 = tanh (𝑊ℎℎℎ𝑡−1 +𝑊𝜋ℎ𝑥𝑡) (7) 𝑦𝑡 = 𝑊ℎ𝑦ℎ𝑡 (8) The vanilla RNN[18]is not very good at remembering previous sequences. In addition to that, RNN struggles with diminishing gradient descent. A kind of RNN is a long short-term recall network (LSTM), solves a vanishing gradient descent problem and learns long-term dependencies[10]. LSTM was actually created to address the problem of long-term reliance. LSTM has the unique ability to recall. The cell state is the LSTM model’s central concept. With only a small amount of linear interaction, the cell state follows the sequence essentially unmodified from beginning to end. gate of an LSTM is also significant. Under the command of these gates, information is safely inserted to or eliminated from the cell stated. The following equations are used by the LSTM model to update each cell: 𝑓𝑡 = 𝜎  𝑊𝑓 · [ℎ𝑡−1,𝑥𝑡] + 𝑏𝑓  (9) In this case, Xt denotes input, and ht is the hidden state at the t time step. The following is the revised cell state Ct: 𝑖t = 𝜎 (𝑊𝑖 [ℎ𝑡−1,𝑥𝑡] + 𝑏𝑖) (10) 𝐶𝑇 = tanh (𝑊𝑐 [ℎ𝑡−1,𝑥𝑡] + 𝑏𝑐𝑡) (11) 𝐶𝑡 = 𝑓𝑡 ∗ 𝐶𝑡−1 + 𝑖𝑡 ∗ 𝐶𝑇 (12) Here, we may compute the output and hidden state at t time steps using the point-wise multiplication operator *. 𝑜𝑡 = 𝜎 (𝑊𝑜 · [ℎ𝑡−1,𝑥𝑡] + 𝑏𝑜) (13) ℎ𝑡 = 𝑜𝑡 ∗ tanh (𝐶𝑡) (14) Due to the reality it only considers all prior contexts from the present one, LSTM does have a few drawbacks. As a result of this, it may accept data from preceding time steps through LSTM as well as RNN. Therefore, in order to avoid this issue, further improve- ments are carried out with the help of a bidirectional recurrent neural network(Bi-RNN). BiRNN [13] can handle two pieces of in- formation from both the front and the back. Bi-LSTM is created by combining the Bi-RNN and LSTM. As a result, operating LSTM has advantages such as cell state storage so that BiRNN have way to acknowledge from the context before and after. As a conse- quence of this, it provides the Bi-LSTM with the advantages of an LSTM with feedback for the next layer. Remembering long-term dependencies is a significant new benefit of Bi-LSTM. The output, which is a feature vector, will be based on the call state. Finally, we forecast the probability of email content as Normal, Fraudu- lent, Harassment, and Suspicious Emails using as an input to the softmax activation function, which is a weighted sum of the dense layer’s outputs. To regulate the information flow, GRU employs the point-wise multiplying function and logistic sigmoid activation. The GRU has hidden states of storage memory and does not have distinct memory cells or units for state control. The W, U, and b vectors, which stand for weights, gates, and biases, respectively, are crucial variables that must be calculated during the creation of the GRU model. For training reasons, the pre-trained word embedding known as the Glove vector is used. They made it clear that GRU is the superior model when there is a large amount of training data for textual groups and word embedding is available. BiLSTM, CNN, and GRU is required so as to compensate for the deletion of the document’s long-term and short-term connections. In our case, the embedding dimension, maximum sequence length, and lexicon size were used to start the LSTM embedding layer in three separate LSTM models. The input vector was modified to make it appropriate for such a Conv1D layer, prior situations’ sequences are returned by LSTM layer. The "return sequences" of the LSTM layer must be set to False when the subsequent state is free of the gated 184 Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model IC3 2023, August 03–05, 2023, Noida, India architecture. Quantity of learning parameters must be taken into consideration. A 350-unit LSTM layer was set - up, and different LSTM unit combinations were tested. More importantly, because it has more parts, the model made with BiLSTM will take longer to train. Bidirectional LSTM is the name of a particular kind of recurrent neural network that is primarily used for the processing of natural languages. (BiLSTM). It is able to use data from both sides, and, in contrast to regular LSTM, it enables input flow in both directions. It is an effective instrument for demonstrating the logical relationships between words and phrases, and this involves both the forward and backward directions of the sequence. In con- clusion, BiLSTM works by adding one extra layer of LSTM, causing the information flow to travel in the other direction. It only denotes that the input sequence runs in reverse at the next LSTM layer. Mul- tiple operations, including averaging, summation, multiplication, and concatenation, are then applied to the results of the two LSTM layers. The gated design of Bi-LSTM and GRU networks solves the disappearing gradient and exploding problems. A good way to handle more long sequences is to use Bi-LSMT and GRU together. GRU works well with datasets that don’t have text. In two to three rounds, the complicated CNN+BiLSTM+GRU model learns the long sequence of email text well. We have used word embedding, cnn, bidirectional lstm and gru networks as our three building blocks to separate email messages based on their sentiment and text’s sequential features. Also, we succinctly demonstrate below why these blocks help identify email spam: • First, We have used the Sequence - to - sequence Lstm as the current block in the networks since it can retrieve both the previous and next sequences from the current. More so than a straightforward LSTM network, it can also recognize and extract text sentiment and sequential properties. • Second, we extract the more complex and advanced charac- teristics for Bi-LSTM network using Convolutional Network block, which is the network’s second block after the Bi-LSTM block. Bi-LSTM takes a long time to extract text-based fea- tures, hence one of the reasons for using this block is to reduce the network’s overall training time. 3 EXPERIMENTAL EVALUATION 3.1 Experimental Setup We divided the information into training and testing groups of 80/20. We divided the remaining 20% of the 80 percent training data into test data for the model. Construct, compute, and evaluate the efficacy of the suggested method using the Pythonic packages Keras, as TensorFlow and Scikit learn. 3.2 Dataset Description Email spam detection is the foundation of this research project. The dataset includes normal emails from the Enron corpora, deceptive emails from phished email corpora, harassment emails chosen from hate speech, and the offensive dataset. Only the content of the email body is used for analysis; all header information, including sender, topic, CC, and BCC, are eliminated. Word2vector, TF-IDF, and Word Embedding are used to extract characteristics from the email mes- sage and classify them. This dataset[8] is publicly available. The presented model is implemented using Python, and several metrics, including accuracy, precision, and recall, are used to examine the outcomes. 3.3 Evaluation Metrics and Results Classifier performance is assessed Using metrics such as accuracy, precision, and recall. Four terms make up a confusion matrix that is used to calculate these metrics. • True positives (TP) are positive values that have been accu- rately assigned the positive label. • The negative values that are accurately identified as negative are known as True Negatives (TN). • True Negative values are those that can be accurately identi- fied as being negative (TN). • Positive readings that have been mistakenly labeled as nega- tive are known as False Negatives (FN). Assess the efficacy of the suggested model is listed below: 3.3.1 Accuracy. Accuracy reveals how frequently the ML model was overall correct. Accuracy = 𝑇𝑃 +𝑇𝑁 𝑇𝑃 +𝑇𝑁 + 𝐹𝑃 + 𝐹𝑁 (15) 3.3.2 Precision. The accuracy of the model gauges how effectively it can predict a specific category. Precision = 𝑇𝑃 𝑇𝑃 + 𝐹𝑃 (16) 3.3.3 Recall. Recall tells us how often the model was able to rec- ognize a specific category. Recall = 𝑇𝑃 𝑇𝑃 + 𝐹𝑁 (17) Model Accuracy Precision Recall Gaussian NB 91.3 90.1 91.8 Random Forest 88.41 90 88 KNN 86.6 89 87 SVM 92.4 91 92 LSTM 95.2 95 95.7 Proposed Ensemble (CNN,BiLSTM+GRU) 97.32 95.6 95.3 Table 1: Differet Model’s Score on Test Data Accuracy, Precision, and Recall metrics are computed. In the given Table 1 where six different classifiers are Gaussian NB, Ran- dom Forest, KNN, SVM, LSTM, and Propose Ensemble Hybrid Model (CNN+BiLSTM+GRU) have been used in this work. In the CNN, Bi-LSTM, and GRU architectures which enable sequence pre- diction, CNN strands for feature extraction on data input which are combined with LSTM. It requires less time training and a higher expandable model. Any bottlenecks are created by predictions and the increasing number of distinct units of information. This model is useful for dealing with issue-related classifications that consist of two or more than two classes. So suggested Ensemble model, out of these six classifiers, produces more accurate findings. 185 IC3 2023, August 03–05, 2023, Noida, India Sachan et al. Figure 1: Performance Analysis 3.4 Comparative Analysis A model’s ability to fit new data is measured by the validation loss, whereas its ability to fit training data is determined by the training loss. The two main variables that decide whether in which learning is efficient or not are validation loss and training loss. LSTM and Suggested Ensemble hybrid Models have equivalent loss and accuracy. In this context, we are contrasting the LSTM with the proposed model (CNN, Bilstm, and GRU) in terms of their respective validation accuracies and losses. The model’s accuracy was at its highest after 14 epochs of operation when it achieved an accuracy of roughly 97-98% while minimizing model loss. Figure 2: LSTM Model Training and Validation Accuracy Figure 3: LSTM Model Training and Validation Loss Figure 4: Ensemble Model (CNN,BiLSTM+GRU) Training and Validation Accuracy Figure 5: Ensemble Model (CNN,BiLSTM+GRU)Training and Validation Loss 186 Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model IC3 2023, August 03–05, 2023, Noida, India In this Proposed ensemble hybrid model’s train accuracy is 98.7% Validation accuracy is 97.32% and LSTM has train accuracy of 97.41% and validation accuracy is 95.2%. So based on figures 3 and 5 indicate the validation loss for LSTM and the proposed ensemble hybrid model to be 0.93 and 0.84, respectively, and figures 2 and 4 show the validation accuracy to be 95.2% and 97.3%, respectively. LSTM and the proposed hybrid model used ensemble artificial intelligence, with the proposed hybrid model outperforming the LSTM. We decide on dense architecture as the final model for identifying the text messages as spam or nonspam based on loss, accuracy, and the aforementioned charts. The loss and accuracy over epochs are more stable than LSTM, and the Proposed classifier has a straightforward structure. 4 CONCLUSION The model is composed of four networks Word-Embeddings, CNN, Bi-LSTM, and GRU. We may train the model more quickly by using the convolutional layer first, followed by the word-embedding layer, and then the BiLSTM network. The Bidirectional LSTM network also has higher-level properties that we can extract. We have used a bidirectional LSTM(BiLSTM)and GRU network to memorize a sentence’s contextual meaning and sequential structure, which im- proves the model’s performance accuracy to roughly 97.32 percent. ');
INSERT INTO articleApp_article VALUES(33,'Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model','The emergence of novel types of communication, such as email, has been brought on by the development of the internet, which radically concentrated the way in that individuals communicate socially and with one another. It is now establishing itself as a crucial aspect of the communication network which has been adopted by a variety of commercial enterprises such as retail outlets. So in this research paper, we have built a unique spam-detection methodology based on email-body sentiment analysis. The proposed hybrid model is put into practice and preprocessing the data, extracting the proper- ties, and categorizing data are all steps in the process. To examine the emotive and sequential aspects of texts, we use word embed- ding and a bi-directional LSTM network. this model frequently shortens the training period, then utilizes the Convolution Layer to extract text features at a higher level for the BiLSTM network. Our model performs better than previous versions, with an accuracy rate of 97–98%. In addition, we show that our model beats not just some well-known machine learning classifiers but also cutting-edge methods for identifying spam communications, demonstrating its superiority on its own. Suggested Ensemble model’s results are examined in terms of recall, accuracy, and precision.',0,'Dataset, KNN, Gaussian Naive Bayes, LSTM, SVM, Bidirectional LSTM, GRU, Word-Embeddings, CNN','https://drive.google.com/file/d/1KnIDIx-QMeq3R5U_nbPTSJe9NfZ6tFja/view?usp=drivesdk',NULL,'   Introduction    Over the past few years, a clear surge of both the amount of spammers as well as spam emails. This is likely due to a fact that the investment necessary for engaging in the spamming industry is relatively low. As a result of this, we currently have a system that identifies every email as suspicious, which has caused major expenditures in the investment of defense systems [12]. Emails are used for online crimes like fraud, hacking, phishing, E-mail bombing, bullying, and spamming. [16]. Algorithms that are based on machine learning (ML) are now the most effective and often used approach to the recognition of spam. Phishing, which is defined as a fraudulent attempt to acquire private information by masquerading as a trustworthy party in electronic communication, has rapidly advanced past use of simple techniques and the tactic of casting a wide net; instead, spear phishing uses a variety of sophisticated techniques to target a single high-value individual. Other researchers used NB, Decision Trees, and SVM to compare the performance of supervised ML algorithms for spam identification [6]. Spam emails clog up recipients'' inboxes with unsolicited communications, which frustrate them and push them into the attacker''s planned traps [7]. As a result, spam messages unquestionably pose a risk to both email users and the Internet community. In addition, Users may occasionally read the entire text of an unsolicited message that is delivered to the target users'' inboxes without realizing that the message is junk and then choosing to avoid it. Building a framework for email spam detection is the aim of this project. In this approach, we combine the Word-Embedding Network with the CNN layer, Bi-LSTM, and GRU (BiLSTM+GRU). CNN layers are used to speed up training time before the Bi-LSTM network, and more advanced textual characteristics are extracted with the use of this network in comparison to the straight LSTM network, in less time. Gated recurrent neural networks (GRUs) are then added because they train more quickly and perform better for language modeling. To evaluate and investigate various machine learning algorithms for predicting email spam, and develop a hybrid classification algorithm to filter email spam before employing an ensemble classification algorithm to forecast it. To put an innovative technique into practice and compare it to the current method in terms of various metrics. Ensemble learning, a successful machine learning paradigm, combines a group of learners rather than a single learner to forecast unknown target attributes. Bagging, boosting, voting, and stacking are the four main types of ensemble learning techniques. To increase performance, an integrated method and the combining of two or three algorithms are also suggested. Extraction of text-based features takes a long time. Furthermore, it can be challenging to extract all of the crucial information from a short text. Over the span associated with this 181.IC3 2023, August 03–05, 2023, Noida, India Sachan et al. research, we utilize Bidirectional Large Short-Term Memories (Bi- LSTM) in conjunction with Convolutional Neural Networks (CNN) to come up with an innovative method to the detection of spam. Bagging and boosting approaches were widely preferred in this study. Contribution and paper organization is as follows: section 1.1 describes literature study, section 1.2 describe motivation for this research work, section 2 sketches procedure of details implemen- tation, Section 3 present experimental setup, dataset description and evaluation metrics, and section 4 summarizing outcomes of the experiment. 1.1 Related Work Email is indeed the second most frequently utilized Internet appli- cation as well as the third most common method of cyberbullying, claims one study. Cybercriminals exploit it in a number of ways, including as sending obscene or abusive messages, adding viruses to emails, snatching the private information of victims, and ex- posing it to a broad audience. Spam letters made up 53.95% of all email traffic in March 2020. We examine three main types of un- lawful emails in our study. First are fake emails, which are sent to manipulate recipients to submit sensitive information. The sec- ond as being cyberbullying’s use of harassing emails to threaten individuals. Suspicious emails that describe illegal activities belong to the third category. Many researchers have earlier contributed massively to this subject. The researcher claims there is some proof that suspicious emails were sent before to the events of 9/11. [14]. When it comes to data labeling, there are also convinced rule-based approaches and technologies ( like VADER) that are used, even though their efficiency of the are together is adversely affected. A hidden layer, which itself is essential for vectorization, is the top layer of the model. We use oversampling methods for this minority class because of the absence of data. Sampling techniques can help with multicollinearity, but they have an impact on simulation re- sults. Oversampling causes data to be randomly repeated, which affects test data because dividing data may result in duplicates. Un- dersampling may result in the loss of some strong information. In order to advance email research, it is crucial to provide datasets on criminal activity. P. Garg et al. (2021) [5], which revealed that spam in an email was detected in 70 percent of business emails, spam was established as an obstacle for email administrators. Recognizing spam and getting rid of it were the primary concerns, as spam can be offensive, may lead to other internet sites being tricked, which can offer harmful data, and can feature those who are not particu- lar with their content using NLP. To select the best-trained model, each mail transmission protocol requires precise and effective email classification, a machine learning comparison is done. Our study has suggested that innovative deep learning outperforms learning algorithms like SVM and RF. Current studies on the classification of emails use a variety of machine learning (ML) techniques, with a few of them focusing on the study of the sentiments consisted of within email databases. The lack of datasets is a significant obstacle to email classification. There are few publicly accessible E-mail datasets, thus researchers must use these datasets to test their hy- potheses or gather data on their own. Authors[15] describe supplied two-phased outlier detection models to enhance the IIOT network’s dependability. Artificial Neural Network, SVM, Gaussian NB, and RF (random forest) ensemble techniques were performed to forecast class labels, and the outputs were input into a classifying unit to increase accuracy. A method for content-based phishing detection was presented by the authors in [2], to classify phishing emails, they employed RF. They categorize spam and phishing emails. They enhanced phishing email classifiers with more accurate predictions by extracting features. They showed some effective Machine learn- ing spam filtering techniques. When the PCA method is used, it will lower the number of features in the dataset. The collected features go through the PCA algorithm to reduce the number of features. The PCA method is used to make a straightforward representation of the information which illustrates the amount of variability there is in the data. The authors of [20] presented the Fuzzy C-means method for classifying spam email. To stop spam, they implemented a membership threshold value. A methodology to identify unla- beled data was put forth by the authors of [1] and applied motive analysis to the Enron data collection. They divided the data into categories that were favorable, negative, and neutral. They grouped the data using k-means clustering, an unsupervised ML technique and then classified it using the supervised ML techniques SVM and NB. Hina, Maryam, and colleagues (2021) implemented Sefaced: Deep learning-based semantic analysis and categorization of e-mail data using a forensic technique. For multiclass email classification, SeFACED employs a Gated Recurrent Neural Network (GRU) based on Long Short-Term Memory (LSTM). Different random weight ini- tializations affect LSTMs [9]. Zhang, Yan, et al.(2019) Experiments on three-way game-theoretic rough set (GTRS) email spam filter- ing show that it is feasible to significantly boost coverage without decreasing accuracy [23]. According to Xia et al. [22], SMS spam has been identified using machine learning model such as naive bayes , vector-space modeling, support vector machines (SVM), long selective memory machines (LSTM), and convolutional neural networks including every instance of a method for categorizing data. Elshoush, Huwaida, et al. (2019) Using adaboost and stochastic gradient descent (sgd) algorithms for e-mail filtering with R and orange software spam [3]. Orange software was used to create the classifications, which included Adaboost and SGD. The majority of researchers focused on text-based email spam classification meth- ods because image-based spam can be filtered in the early stages of pre-processing. There are widely used word bag (BoW) model, which believes that documents are merely unordered collections of words, is the foundation for these techniques. Kumaresan [11] explains SVM with a cuckoo search algorithm was used to extract textual features for spam detection. Renuka and Visalakshi made use of svm [17] spam email identification, followed by selecting features using Latent Semantic Indexing (LSI). Here we have used labeled dataset to train the hybrid classifier. We used TF-IDF for feature extraction [20] and Textual features for spam detection were extracted using SVM and a cuckoo search algorithm. [4] for filtering out the spam email. Combining the integrated strategy to the pure SVM and NB methods, overall accuracy is really improved. Moreover, accurate detection for spam email has been proposed using the Negative Selection Algorithm (NSA) and Particle Swarm Optimization’s (PSO) algorithm. PSO is used in this instance to improve the effectiveness of the classifier. 182 Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model IC3 2023, August 03–05, 2023, Noida, India 1.2 Motivation and Novelty Email is most common form of communication between people in this digital age. Many users have been victims of spam emails, and their personal information has been compromised. The email Classification technique is employed to identify and filter junk mail, junk, and virus-infected emails prior to reach a user’s inbox. Existing email classification methods result in irrelevant emails and/or the loss of valuable information. Keeping these constraints in mind, the following contributions are made in this paper: • Text-based feature extraction is a lengthy process. Further- more, extracting every important feature from text is difficult. In this paper, we show how to employ GRU with Convo- lutional Neural Networks and Bidirectional-LSTM to find spam. • Used Word-Embeddings, BiLSTM, and Gated Recurrent Neu- ral Networks to examine the relationships, sentimental con- tent, and sequential way of email contents. • Applied CNN before the Bi-LSTM network, training time can be sped up. This network can also extract more advanced textual features faster than the Bi-LSTM network alone when combined with the GRU network. • We use Enorn Corpora datasets and compute precision, re- call, and f-score to assess how well the suggested technique performs. Our model outperforms several well-known ma- chine learning techniques as well as more contemporary methods for spam message detection. 2 PROPOSED SYSTEM ARCHITECTURE AND MODEL E-mail is a valuable tool for communicating with other users. Email allows the sender to efficiently forward millions of advertisements at no cost. Unfortunately, this scheme is now being used in a variety of organizations. As a result, a massive amount of redundant emails is known as spam or junk mail, many people are confused about the emails in their E- Mailboxes. Each learning sequence is given for- ward as well as backward to two different LSTM networks that are attached to the same outputs layer in order for bidirectional Lstms to function. This indicates that the Bi-LSTM has detailed sequential information about all points before and following each point in a specific sequence. In other words, we concatenate the outputs from both the forward and the backward LSTM at each time step rather than just encoding the sequence in the forward direction. Each word’s encoded form now comprehends the words that come before and after it. This is a problem for the Internet community. The di- agram depicts various stages that aid in the prediction of email spam: Because real-world data is messy and contains unnecessary infor- mation and duplication, data preprocessing is critical in natural language processing (NLP). The major preprocessing steps are de- picted below. 2.1 NLP Tokenization Tokenization of documents into words follows predefined rules. The tokenization step is carried out in Python with spacy library. 2.2 Stop Words Removal Stop words appear infrequently or frequently in the document, but they are less significant in terms of importance. As a result, these are removed to improve data processing. 2.3 Text Normalization A word’s lexicon form or order may differ. Thus, they must all be changed to their root word to be correctly analyzed. Lemmatization and stemming are the two methods that can be used for normal- ization. When a word’s final few characters are removed to create a shorter form, even if that form has no meaning, the procedure is known as stemming. lemmatization [21] is a mixture of corpus- based an rule-based methods, and it retains the context of a term while changing it back to its root. 2.4 Feature Extraction feature extraction which transforms the initial text into its features so that it may be used for modeling after being cleaned up and normalized. Before predicting them, we use a specific way to give weights to specific terms in our document. While it is simple for a computer to process numbers, we choose to represent individual words numerically. In such cases, we choose word embeddings. IDF is the count of documents containing the term divided by the total number of documents, and occurrence is the amount of instances a word appears in a document. We derive characteristics based on equations. 1,2,3,4,5, and 6. We use equations to derive properties. 𝑇 𝑓 𝐼𝑑𝑓 = 𝑡𝑓 ∗  1 𝑑𝑓  (1) 𝑇 𝑓 𝐼𝑑𝑓 = 𝑡𝑓 ∗ Inverse(𝑑𝑓 ) (2) 𝑇 𝑓 𝐼𝑑𝑓 (𝑡,𝑑, 𝐷) = 𝑇 𝑓 (𝑡,𝑑).𝐼𝑑𝑓 (𝑡, 𝐷) (3) 𝑇𝐼𝑑𝑓 (𝑡,𝑑) = log 𝑁 |𝑑𝜖𝐷𝑡𝜖𝐷| (4) A word2vec neural network-based approach is the method that is utilized for this goal as the tool. The following equation, referred to as 5, shows how word2vec handles word context through the use of probability-accurate measurements. Here letter D stands for the paired-wise display of a set of words, while the letters w and c0 or c1 represent paired word context that originated from a larger collection of set D. 𝑃 (𝐷 = 1 | 𝑤,𝑐11:𝑘) = 1 1 + 𝑒−(𝑤·𝑐11+𝑤·𝑐12+...+𝑤·𝑐1𝑘) (5) 𝑃 (𝐷 = 1 | 𝑤,𝑐1:𝑘) = 1 1 + 𝑒−(𝑤·𝑐0) (6) 183 IC3 2023, August 03–05, 2023, Noida, India Sachan et al. 2.5 Word-Embeddings Word-Embedding helps to improve on the typical "bag-of-words" worldview, which requires a massive sparse feature vector to score every word individually to represent this same entire vocabulary. This perception is sparse because the vocabulary is large, and each word or document is defined by a massive vector. Using a word map-based dictionary, word embedding needs to be converted terms (words) into real value feature vectors. There are two basic issues with standard feature engineering techniques for deep learning. Data is represented using sparse vectors, and the second is that some of the meanings of words are not taken into consideration. Similar phrases will have values in embedding vectors that are almost real-valued. The Input length in our proposed study is set to 700 for our suggested model. If the texts seemed to be integer encoded with value systems between 10 and 20, the vocabulary distance would be 11. Our data is encoded as integers, and the input and output dimensions are both set to 50,000. The embedding layer outcome will be used in successive layers and for BiLSTM and GRU layers. 2.6 Machine Learning Model Within the scope of the research, we are using the subsequent ma- chine learning techniques, to examine and compare the overall efficacy of our suggested Bi-LSTM strategy: Support Vector Ma- chine, Gaussian NB, Logistic Regression, K - nearest neighbors, and Random Forest (RF). 2.7 Convolution Network The popular RNN model generally performs well but takes too long to train the model incorporating the textual sequential data. When a layer is added after the RNN layer, the model’s learning duration is considerably decreased. Higher-level feature extraction is another benefit. [19] additionally possible using the convolutional layer. In essence, the convolution layer looks for combinations of the various words or paragraphs in the document that involve the filters. We use features with 128 dimensions and a size 10 for each. For this task, the Relu activation function is utilized. After that, the one-dimensional largest pooling layers with a pooling size of 4 are put on the data in order to obtain higher-level features. 2.8 BiLSTM Network with GRU Recurrent Neural Network (RNN) technique of text sentiment anal- ysis is particularly well-liked and frequently applied. Recurrent neural networks (RNN) surpass conventional neural networks. be- cause it can remember the information from earlier time steps thanks to its memory. A state vector is combined with an RNN’s data to create a new state vector. The resulting state vector uses the present to recollect past knowledge. The RNN is straightforward and is based on the following equations: ℎ𝑡 = tanh (𝑊ℎℎℎ𝑡−1 +𝑊𝜋ℎ𝑥𝑡) (7) 𝑦𝑡 = 𝑊ℎ𝑦ℎ𝑡 (8) The vanilla RNN[18]is not very good at remembering previous sequences. In addition to that, RNN struggles with diminishing gradient descent. A kind of RNN is a long short-term recall network (LSTM), solves a vanishing gradient descent problem and learns long-term dependencies[10]. LSTM was actually created to address the problem of long-term reliance. LSTM has the unique ability to recall. The cell state is the LSTM model’s central concept. With only a small amount of linear interaction, the cell state follows the sequence essentially unmodified from beginning to end. gate of an LSTM is also significant. Under the command of these gates, information is safely inserted to or eliminated from the cell stated. The following equations are used by the LSTM model to update each cell: 𝑓𝑡 = 𝜎  𝑊𝑓 · [ℎ𝑡−1,𝑥𝑡] + 𝑏𝑓  (9) In this case, Xt denotes input, and ht is the hidden state at the t time step. The following is the revised cell state Ct: 𝑖t = 𝜎 (𝑊𝑖 [ℎ𝑡−1,𝑥𝑡] + 𝑏𝑖) (10) 𝐶𝑇 = tanh (𝑊𝑐 [ℎ𝑡−1,𝑥𝑡] + 𝑏𝑐𝑡) (11) 𝐶𝑡 = 𝑓𝑡 ∗ 𝐶𝑡−1 + 𝑖𝑡 ∗ 𝐶𝑇 (12) Here, we may compute the output and hidden state at t time steps using the point-wise multiplication operator *. 𝑜𝑡 = 𝜎 (𝑊𝑜 · [ℎ𝑡−1,𝑥𝑡] + 𝑏𝑜) (13) ℎ𝑡 = 𝑜𝑡 ∗ tanh (𝐶𝑡) (14) Due to the reality it only considers all prior contexts from the present one, LSTM does have a few drawbacks. As a result of this, it may accept data from preceding time steps through LSTM as well as RNN. Therefore, in order to avoid this issue, further improve- ments are carried out with the help of a bidirectional recurrent neural network(Bi-RNN). BiRNN [13] can handle two pieces of in- formation from both the front and the back. Bi-LSTM is created by combining the Bi-RNN and LSTM. As a result, operating LSTM has advantages such as cell state storage so that BiRNN have way to acknowledge from the context before and after. As a conse- quence of this, it provides the Bi-LSTM with the advantages of an LSTM with feedback for the next layer. Remembering long-term dependencies is a significant new benefit of Bi-LSTM. The output, which is a feature vector, will be based on the call state. Finally, we forecast the probability of email content as Normal, Fraudu- lent, Harassment, and Suspicious Emails using as an input to the softmax activation function, which is a weighted sum of the dense layer’s outputs. To regulate the information flow, GRU employs the point-wise multiplying function and logistic sigmoid activation. The GRU has hidden states of storage memory and does not have distinct memory cells or units for state control. The W, U, and b vectors, which stand for weights, gates, and biases, respectively, are crucial variables that must be calculated during the creation of the GRU model. For training reasons, the pre-trained word embedding known as the Glove vector is used. They made it clear that GRU is the superior model when there is a large amount of training data for textual groups and word embedding is available. BiLSTM, CNN, and GRU is required so as to compensate for the deletion of the document’s long-term and short-term connections. In our case, the embedding dimension, maximum sequence length, and lexicon size were used to start the LSTM embedding layer in three separate LSTM models. The input vector was modified to make it appropriate for such a Conv1D layer, prior situations’ sequences are returned by LSTM layer. The "return sequences" of the LSTM layer must be set to False when the subsequent state is free of the gated 184 Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model IC3 2023, August 03–05, 2023, Noida, India architecture. Quantity of learning parameters must be taken into consideration. A 350-unit LSTM layer was set - up, and different LSTM unit combinations were tested. More importantly, because it has more parts, the model made with BiLSTM will take longer to train. Bidirectional LSTM is the name of a particular kind of recurrent neural network that is primarily used for the processing of natural languages. (BiLSTM). It is able to use data from both sides, and, in contrast to regular LSTM, it enables input flow in both directions. It is an effective instrument for demonstrating the logical relationships between words and phrases, and this involves both the forward and backward directions of the sequence. In con- clusion, BiLSTM works by adding one extra layer of LSTM, causing the information flow to travel in the other direction. It only denotes that the input sequence runs in reverse at the next LSTM layer. Mul- tiple operations, including averaging, summation, multiplication, and concatenation, are then applied to the results of the two LSTM layers. The gated design of Bi-LSTM and GRU networks solves the disappearing gradient and exploding problems. A good way to handle more long sequences is to use Bi-LSMT and GRU together. GRU works well with datasets that don’t have text. In two to three rounds, the complicated CNN+BiLSTM+GRU model learns the long sequence of email text well. We have used word embedding, cnn, bidirectional lstm and gru networks as our three building blocks to separate email messages based on their sentiment and text’s sequential features. Also, we succinctly demonstrate below why these blocks help identify email spam: • First, We have used the Sequence - to - sequence Lstm as the current block in the networks since it can retrieve both the previous and next sequences from the current. More so than a straightforward LSTM network, it can also recognize and extract text sentiment and sequential properties. • Second, we extract the more complex and advanced charac- teristics for Bi-LSTM network using Convolutional Network block, which is the network’s second block after the Bi-LSTM block. Bi-LSTM takes a long time to extract text-based fea- tures, hence one of the reasons for using this block is to reduce the network’s overall training time. 3 EXPERIMENTAL EVALUATION 3.1 Experimental Setup We divided the information into training and testing groups of 80/20. We divided the remaining 20% of the 80 percent training data into test data for the model. Construct, compute, and evaluate the efficacy of the suggested method using the Pythonic packages Keras, as TensorFlow and Scikit learn. 3.2 Dataset Description Email spam detection is the foundation of this research project. The dataset includes normal emails from the Enron corpora, deceptive emails from phished email corpora, harassment emails chosen from hate speech, and the offensive dataset. Only the content of the email body is used for analysis; all header information, including sender, topic, CC, and BCC, are eliminated. Word2vector, TF-IDF, and Word Embedding are used to extract characteristics from the email mes- sage and classify them. This dataset[8] is publicly available. The presented model is implemented using Python, and several metrics, including accuracy, precision, and recall, are used to examine the outcomes. 3.3 Evaluation Metrics and Results Classifier performance is assessed Using metrics such as accuracy, precision, and recall. Four terms make up a confusion matrix that is used to calculate these metrics. • True positives (TP) are positive values that have been accu- rately assigned the positive label. • The negative values that are accurately identified as negative are known as True Negatives (TN). • True Negative values are those that can be accurately identi- fied as being negative (TN). • Positive readings that have been mistakenly labeled as nega- tive are known as False Negatives (FN). Assess the efficacy of the suggested model is listed below: 3.3.1 Accuracy. Accuracy reveals how frequently the ML model was overall correct. Accuracy = 𝑇𝑃 +𝑇𝑁 𝑇𝑃 +𝑇𝑁 + 𝐹𝑃 + 𝐹𝑁 (15) 3.3.2 Precision. The accuracy of the model gauges how effectively it can predict a specific category. Precision = 𝑇𝑃 𝑇𝑃 + 𝐹𝑃 (16) 3.3.3 Recall. Recall tells us how often the model was able to rec- ognize a specific category. Recall = 𝑇𝑃 𝑇𝑃 + 𝐹𝑁 (17) Model Accuracy Precision Recall Gaussian NB 91.3 90.1 91.8 Random Forest 88.41 90 88 KNN 86.6 89 87 SVM 92.4 91 92 LSTM 95.2 95 95.7 Proposed Ensemble (CNN,BiLSTM+GRU) 97.32 95.6 95.3 Table 1: Differet Model’s Score on Test Data Accuracy, Precision, and Recall metrics are computed. In the given Table 1 where six different classifiers are Gaussian NB, Ran- dom Forest, KNN, SVM, LSTM, and Propose Ensemble Hybrid Model (CNN+BiLSTM+GRU) have been used in this work. In the CNN, Bi-LSTM, and GRU architectures which enable sequence pre- diction, CNN strands for feature extraction on data input which are combined with LSTM. It requires less time training and a higher expandable model. Any bottlenecks are created by predictions and the increasing number of distinct units of information. This model is useful for dealing with issue-related classifications that consist of two or more than two classes. So suggested Ensemble model, out of these six classifiers, produces more accurate findings. 185 IC3 2023, August 03–05, 2023, Noida, India Sachan et al. Figure 1: Performance Analysis 3.4 Comparative Analysis A model’s ability to fit new data is measured by the validation loss, whereas its ability to fit training data is determined by the training loss. The two main variables that decide whether in which learning is efficient or not are validation loss and training loss. LSTM and Suggested Ensemble hybrid Models have equivalent loss and accuracy. In this context, we are contrasting the LSTM with the proposed model (CNN, Bilstm, and GRU) in terms of their respective validation accuracies and losses. The model’s accuracy was at its highest after 14 epochs of operation when it achieved an accuracy of roughly 97-98% while minimizing model loss. Figure 2: LSTM Model Training and Validation Accuracy Figure 3: LSTM Model Training and Validation Loss Figure 4: Ensemble Model (CNN,BiLSTM+GRU) Training and Validation Accuracy Figure 5: Ensemble Model (CNN,BiLSTM+GRU)Training and Validation Loss 186 Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model IC3 2023, August 03–05, 2023, Noida, India In this Proposed ensemble hybrid model’s train accuracy is 98.7% Validation accuracy is 97.32% and LSTM has train accuracy of 97.41% and validation accuracy is 95.2%. So based on figures 3 and 5 indicate the validation loss for LSTM and the proposed ensemble hybrid model to be 0.93 and 0.84, respectively, and figures 2 and 4 show the validation accuracy to be 95.2% and 97.3%, respectively. LSTM and the proposed hybrid model used ensemble artificial intelligence, with the proposed hybrid model outperforming the LSTM. We decide on dense architecture as the final model for identifying the text messages as spam or nonspam based on loss, accuracy, and the aforementioned charts. The loss and accuracy over epochs are more stable than LSTM, and the Proposed classifier has a straightforward structure. 4 CONCLUSION The model is composed of four networks Word-Embeddings, CNN, Bi-LSTM, and GRU. We may train the model more quickly by using the convolutional layer first, followed by the word-embedding layer, and then the BiLSTM network. The Bidirectional LSTM network also has higher-level properties that we can extract. We have used a bidirectional LSTM(BiLSTM)and GRU network to memorize a sentence’s contextual meaning and sequential structure, which im- proves the model’s performance accuracy to roughly 97.32 percent. ');
INSERT INTO articleApp_article VALUES(34,'AI Model for Computer games based on Case Based Reasoning and AI Planning','Making efficient AI models for games with imperfect information can be a particular challenge. Considering the large number of possible moves and the incorporated uncertainties building game trees for these games becomes very difficult due to the exponential growth of the number of nodes at each level. This effort is focused on presenting a method of combined Case Based Reasoning (CBR) with AI Planning which drastically reduces the size of game trees. Instead of looking at all possible combinations we can focus only on the moves that lead us to specific strategies in effect discarding meaningless moves. These strategies are selected by finding similarities to cases in the CBR database. The strategies are formed by a set of desired goals. The AI planning is responsible for creating a plan to reach these goals. The plan is basically a set of moves that brings the player to this goal. By following these steps and not regarding the vast number of other possible moves the model develops Game Trees which grows slower so they can be built with more feature moves restricted by the same amount of memory.',0,'Game AI, Case Based Reasoning, AI Planning, Game Trees','https://drive.google.com/file/d/1AAbu12sL0_lv0CPHluKlar0RhQkujG8J/view?usp=drivesdk',NULL,replace('The goal of this effort is to explore a model for design and implementation of an AI agent for turn based games. This model provides for building more capable computer opponents that rely on strategies that closely resemble human approach in solving problems opposed to classical computational centric heuristics in game AI. In this manner the computational resources can be focused on more sensible strategies for the game play.  With the advancement in computer hardware increasingly more computing power is left for executing AI algorithms in games. In the past AI in games was mainly a cheating set of instructions that simulated the increasing difficulty in the game environment so that the player had the illusion of real counterpart. Improvement in available memory and processing power allows implementation of more intelligent algorithms for building the game environment as well as direct interaction with the human players.   In this particular research the emphasis is put on the interaction between the AI agent and a computer player in the realm of the game rules. It is particularly focused on turn based games that have the elements of uncertainty like dice or concealed information. At the beginning a description of Game AI algorithms are given; such as Game Trees and Minimax. The following section describes an approach of using AI Planning to improve building Game Trees in games with imperfect information where Game Trees tend to be very large with high growth ratio. Section 4 discusses another approach that provides a significant reduction to the number of considered moves in order to find the favorable strategy of the AI player. This approach uses AI Planning techniques and Case Base Reasoning (CBR) to plan for different scenarios in predetermined strategies which would be analogous to human player experience in the particular game. The CBR database illustrates a set of past experiences for the AI problem and the AI Planning illustrates the procedure to deal with the given situation in the game. In the next two sections implementations and evaluations of both approaches are given. The AI Planning approach is implemented with the Tic-tac-toe game and the combined AI Planning and CBR approach is implemented with a model for the Monopoly game. The last part contains conclusions and future work ideas.  Game Trees are common model for evaluating how different combinations of moves from the player and his opponents will affect the future position of the player and eventually the end result of the game. An algorithm that decides on the next move by evaluating the results from the built Game Tree is minimax [1]. Minimax assumes that the player at hand will always choose the best possible move for him, in other words the player will try to select the move that maximizes the result of the evaluation function over the game state. So basically the player at hand needs to choose the best move overall while taking into account that the next player(s) will try to do the same thing. Minimax tries to maximize the minimum gain. Minimax can be applied to multiple algorithms, one of them is the following: when a player is about to make his move, all of the resulting game states are projected in a game tree and every branch of the tree is evaluated by the evaluation function. The same is done for the opponents moves. For every possible move the result of the evaluation function is observed and the move that maximizes this result is chosen to be the optimal move. A common case of using the minimax algorithm is chess. Instead of a game evaluation function for our Minimax algorithm we will use CBR [1]. Our Case Base will consist of past situations and the result of the case contains the move that solved the situation most favorable for us.  levels of nodes on the game tree, where the leaves bring the final  known (or considered) game state.   The minimax theorem states:  For every two-person, zero-sum game there is a mixed strategy  for each player, such that the expected payoff for both is the same  value V when the players use these strategies. Furthermore, V is  the best payoff each can expect to receive from a play of the  game; that is, these mixed strategies are the optimal strategies for  the two players.  This theorem was established by John von Neumann, who is  quoted as saying "As far as I can see, there could be no theory of  games … without that theorem … I thought there was nothing  worth publishing until the Minimax Theorem was proved" [2].  A simple example of minimax can be observed by building a  game tree of the tic-tac-toe game. The tic-tac-toe game is a simple  game which can end by the first player wining, the second player  wining or a tie. There are nine positions for each of the players in  which at each turn the player puts X or O sign. If the player has  three adjacent signs in a row, column or the two diagonals he or  she wins. This game has limited number of position and it is well  suited for building the whole game tree. The leaves of this tree  will be final positions in the game. A heuristics evaluation  function will also need to be written to evaluate the value of each  node along the way.  3. AI Planning for building Game Trees  3.1.1 AI Planning  AI Planning also referred as Automated Planning and  Scheduling is a branch of Artificial Intelligence that focuses on  finding strategies or sequences of actions that reach a predefined  goal [3]. Typical execution of AI Planning algorithms is by  intelligent agents, autonomous robots and unmanned vehicles.  Opposed to classical control or classification AI Planning results  with complex solutions that are derived from multidimensional  space.   AI Planning algorithms are also common in the video game  development. They solve broad range of problems from path  finding to action planning. A typical planner takes three inputs: a  description of the initial state of the world, a description of the  desired goal, and a set of possible actions. Some efforts for  incorporating planning techniques for building game trees have  also shown up, similar to the approach explored in this effort. In  addition Cased Based Reasoning [4] techniques are also gathering  popularity in developing strategies based in prior knowledge  about the problems in the games. One of the benefits from  Hierarchical Task Network (HTN) [5] planning is the possibility  to build Game Trees based on HTN plans; this method is  described in the following section.  3.2 Game Trees with AI Planning  An adaptation of the HTN planning can be used to build  much smaller and more efficient game trees. This idea has already  been implemented in the Bridge Baron a computer program for  the game of Contact Bridge [6].  Computer programs based on Game Tree search techniques  are now as good as or better than humans in many games like  Chess [7] and checkers [8], but there are some difficulties in  building a game tree for games that have imperfect information  and added uncertainty like card or games with dice. The main  problem is the enormous number of possibilities that the player  can choose from in making his move. In addition some of the  moves are accompanied with probabilities based on the random  elements in the games. The number of possible moves  exponentially grows with each move so the depth of the search  has to be very limited to accommodate for the memory  limitations.   The basic idea behind using HTN for building game trees is  that the HTN provides the means of expressing high level goals  and describing strategies how to reach those goals. These goals  may be decomposed in goals at lower level called sub-goals. This  approach closely resembles the way a human player usually  addresses a complex problem. It is also good for domains where  classical search for solution is not feasible due to the vastness of  the problem domain or uncertainties.  3.2.1 Hierarchical Task Networks  The Hierarchical Task Network, or HTN, is an approach to  automated planning in which the dependency among actions can  be given in the form of networks [9] [Figure 1].  A simple task network (or just a task network for short) is an  acyclic digraph     in which U is the node set, E is the  edge set, and each node 	    contains a task . The edges of  define a partial ordering of U. If the partial ordering is total, then  we say that  is totally ordered, in which case  can be written as  a sequence of tasks   \r    . Figure 1: Simple Hierarchical Task Network  A Simple Task Network (STN) method is a 4-tuple of its name,  task, precondition and a task network. The name of the method  lets us refer unambiguously to substitution instances of the  method, without having to write the preconditions and effects  explicitly. The task tells what kind of task can be applied if the  preconditions are met. The preconditions specify the conditions  that the current state needs to satisfy in order for the method to be  applied. And the network defines the specific subtasks to  accomplish in order to accomplish the task.  A method is relevant for a task if the current state satisfies the  preconditions of a method that implements that task. This task can  be then substituted with the instance of the method. The  substitution is basically giving the method network as a solution  for the task.  If there is a task “Go home” and the distance to home is 3km  [Figure 2] and there exists a method walk-to and this method has a  precondition that the distance is less than 5km, then a substation  to the task “Go home” can be made with this method instance.   Figure 2: HTN Method  Buy milk Go to (shop) Purchase  Go to (home) Go-to (from, to) Walk (to) If (to – from) < 5km  296 DIMEA 2008 3rd International Conference on Digital Interactive Media in Entertainment and Arts If the distance is larger than 5km another meth to be substituted [Figure 3].  Figure 3: HTN Method 2  An STN planning domain is a set of operatio methods M. A STN planning problem is a 4-tu state S0, the task network w called initial task STN domain. A plan   \r    is a soluti problem if there is a way to decompose w into π and each decomposition is applicable in the ap the world. The algorithm that is capable to  networks into plans is called Total-forward-deco [9] or Partial-forward-decomposition (PFD). H cases where one does not want to use a forwa procedure. HTN planning is generalization of S gives the planning procedure more freedom construct the task networks.   In order to provide this freedom, a bookke is needed to represent constraints that the plann not yet enforced. The bookkeeping is done by unenforced constraints explicitly in the task netw The HTN generalizes the definition of a STN. A task network is the pair     w task nodes and C is a set of constraints. Eac specifies a requirement that must be satisfied by a solution to a planning problem.   The definition of a method in HTN also definition used in STN planning. A HTN pla name, task, subtasks, and constraints. The s constraints form the task network. The HTN plan identical to STN planning domains except they u instead of STN methods.  Compared to classical planners the prim HTN planners is their sophisticated knowledge r reasoning capabilities. They can represent and  non-classical planning problems; with a good guide them, they can solve classical planning p magnitude more quickly than classical or neoc The primary disadvantage of HTN is the nee author to write not only a set of planning opera of methods.  3.2.2 HTN Planning in building Game  For a HTN planning algorithm to be adap trees we need to define the domain (set of H operators) which is the domain of the game. Thi a knowledge representation of the rules of the environments and possible strategies of game pla In this domain the game rules as well as kn tackle specific task are defined.   The implem Tree building with HTN is called Tign implementation  uses  a  procedure  simila decomposition, but adapted to build up a game  Drive(to If(t Go-to (from, to)  If(to – from) < 5km  Walk (to)  hod instance needs  ons O and a set of  uple of the initial  k network and the  ion for a planning  π if π is executable  ppropriate state of  decompose these  omposition (TFD)  However there are  ard-decomposition  STN planning that  m about how to  eeping mechanism  ning algorithm has  y representing the  work.  a task network in  where  is a set of  h constraint in C  y every plan that is  o generalizes the  an is a 4-tuple of  subtasks and the  nning domains are  use HTN methods  mary advantage of  representation and  solve a variety of  d set of HTNs to  problems orders of  classical planners.  ed of the domain  ators but also a set  Trees ted to build game  HTN methods and  is is in some sense  e game, the game  ay. nown strategies to  mentation of Game  num2 [9]. This  ar  to  forward- tree rather than a  plan. The branches of the game tree rep the methods. Tignum2 applies all met state of the world to produce new continues recursively until there are n have not already been applied to th world.   In the task network generated by Tignu actions will occur is determined by th By listing the actions in the order  network can be “serialized” into a gam 4. Case Based Reasoning in 4.1 Case Based Reasoning Case-based reasoning (CBR) is a  Artificial Intelligence (AI), both as  problems and as a basis for standalone  Case-based reasoning is a paradigm solving and learning that has became  applied subfield of AI of recent yea intuition that problems tend to recur. I are often similar to previously en therefore, that past solutions may be of [10].   CBR is particularly applicable to probl available, even when the domain is n for a deep domain model. Helpdesks, systems have been the most successfu to determine a fault or diagnostic  attributes, or to determine whether or repair is necessary given a set of past s Figure 5: Game Tree built fr Figure 4: HTN to Game Tr ) to – from) < 200km  present moves generated by  thods applicable to a given  w states of the world and  no applicable methods that  he appropriate state of the  um2, the order in which the  e total-ordering constraints.  they will occur, the task  me tree [Figure 4] [Figure 5].  n Game Strategies well established subfield of  a mean for addressing AI  AI technology. m for combining problem- one of the most successful  ars. CBR is based on the  It means that new problems  ncountered problems and,  f use in the current situation  lems where earlier cases are  not understood well enough  , diagnosis or classification  ul areas of application, e.g.,  an illness from observed  r not a certain treatment or  olved cases [11].  rom HTN ree Algorithm Interactive and Adaptable Media 297 3rd International Conference on Digital Interactive Media in Entertainment and Arts Central tasks that all CBR methods have to deal with are [12]: "to  identify the current problem situation, find a past case similar to  the new one, use that case to suggest a solution to the current  problem, evaluate the proposed solution, and update the system by  learning from this experience. How this is done, what part of the  process that is focused, what type of problems that drives the  methods, etc. varies considerably, however".   While the underlying ideas of CBR can be applied  consistently  across  application  domains,  the  specific  implementation of the CBR methods –in particular retrieval and  similarity functions– is highly customized to the application at  hand.  4.2 CBR and Games  Many different implementations of CBR exist in games.  CBR technology is nicely suited for recognizing complex  situations much easier and more elegant than traditional parameter  comparison or function evaluation. There are especially evident  cases in real time strategies where different attack and defense of  global strategies are nicely defined by CBR datasets and later used  in the running games. Also intelligent bots behavior is also  another typical example. Depending on the number of enemy bots  the layout of the terrain and position of human players the CBR  system finds the closest CBR case and employs that strategy  against the human players which in prior evaluation was proved to  be highly efficient.  5. Game Trees with AI Planning – Tic-tac-toe  In order to show the expressive power of AI Planning in  defining strategies for games, and the use of these plans to build  Game Trees I implemented an algorithm that builds Game Trees  for the Tic-Tac-Toe game.  The game tree of Tic-Tac-Toe shows 255,168 possible  games of which 131,184 are won by X (the first player), 77904  are won by O and the rest 46,080 are draw [13]. All these games  can be derived from building a complete Game Tree.   Even though it is possible to build a complete game tree of  Tic-tac-toe it is definitely not an optimal solution. Many of the  moves in this tree would be symmetrical and also there are a many  moves that would be illogical or at least a bad strategy to even  consider.   So what strategy should X (the first player) choose in order  to win the game?  There are few positions that lead to certain victory. These  positions involve simultaneous attack on two positions so the  other player could not defend, basically the only trick in Tic-Tac- Toe.  Figure 6: Tic-tac-toe winning strategy positions  Position 1 leads to victory if the two of the three fields: top  middle, bottom left corner and bottom right corner are free  [Figure 6].  Position 2 lead to victory if two of the three fields: top right  corner, bottom right corner and bottom middle are free [Figure ].  And in the third position if the two of center, middle top and  middle left are available the position is a certain victory.  There are many different arrangements of the player’s tokens  that give equivalent positions as these three positions. By using  planning we do not need to consider all possible layouts but just  consider these three similar to what a human would consider.   The game starts from an empty table.  The two relevant strategies that would lead to these positions  are to take one corner or to take the center [Figure 7].  Figure 7: Tic-tac-toe Two starting moves  The center position as we can see in the simulation results  lead to a bigger number of victorious endings but it is also a  straight forward strategy with obvious defense strategy.  At this point we need to consider the moves of the opponent.  If we take the left branch the opponent moves can be a center, a  corner or a middle field. We also need to differentiate with a  move to a corner adjacent with our like top left or bottom right or  across the center to bottom right [Figure 8].  Figure 8: Tic-tac-toe opponent response to corner move  In cases one and two, we have a clear path to executing  strategy 3 so we need to capture the diagonally opposite field.  And as for the third case the best way to go is to capture the center  and go for strategy 1 or 2 depending of the opponent’s next move.   Figure 9: Tic-tac-toe move 2 after corner opening  The first move leads to certain victory, O will have to go to  the center and X will achieve strategy 3 [Figure 9]. The second  move is a possible way to strategy 3 if O makes a mistake in the  next loop, so X goes to the opposite corner. For the third case  since O is playing a valid strategy the only move that leaves a  possible mistake from O would be to take the center and wait for  O to go to the middle and then achieve strategy 1 or 3 which will  be a symmetric situation to the one that we will find if we  branched with the center.  Figure 10: Tic-tac-toe opponent response to center move  If we go back to the second branch [Figure 10], a possible  way for the second player to engage is corner or middle. The first  298 DIMEA 2008 3rd International Conference on Digital Interactive Media in Entertainment and Arts move is a valid strategy for O and can be mee corner move from X to try a mistake from O in  the same as in the third case above from the pre another move would be go to the middle wh achieves strategy 1 or 2.   Figure 11: Tic-tac-toe Move 2 after cent The fist move will lead to win if O moves  draw if it goes for the corners [Figure 11]. In t has to block the lower left corner which leave middle left or corner left which are strategy 1 and To sum the strategies for the planning, first  corner strategy for the beginning. Then for the ce the corners with the particularly the one oppo holds. If the center is empty for the second strate we go for the opposite corner. After this point w opponent or try to implement strategies 1, 2 or victory.   Plan 1: Take center   Preconditions: Center empty  Plan 2: Take corner   Preconditions: All corners empty  Plan 3: Take corner after center  Preconditions: We have center take corner oppos opponent has  Plan 4: Take diagonal corner  Preconditions: We have a corner, the opponent ha  the corner opposite to the one we have is free.  Plan 5: Block  Precondition: The opponent has tree tokens in a r agonal  Plan 6: Win  Preconditions: We have two tokens in a row, colu nd the third place is free  Plan 7: Tie  Preconditions: If all places are taken, it’s a tie.  5.1 Hierarchical Task Network  Top level task is Play [Figure 12]. This is a  can be derived into: Win, Block, Tie or Sear Search for plan is derived to both Plan 1 and Pla Plan 4, which later leads to a call for the oppon recursive call to Play.  Figure 12: Tic-tac-toe HT et with a opposite  the future exactly  evious branch, and  here X eventually  ter opening to the middle or a  the second case O  es X to go for the  d 2. we have center or  enter we try to get  osite to the one O  egy we go for it or  we either block the  r 3 which lead to  site to the  one the  as the ce−nter and row, colu−mn or di mn or dia−gonal a a complex task and  rch for Plan. The  an 2 or Plan 3 and  nent’s move and a  TN This HTN when executed will re game scenarios. By creating nodes from them with branches with the move of t tree for the Tic-tac-toe game over whi algorithm.  This set up with 7 plans with 3 ta for Tic-tac-toe which considers all pos player with only 457 games, 281 of w and 0 where the second opponent w reduction over the 255, 168 possible g tree. These reductions can be very use computing capabilities but also we pr that planning can be very efficient if d trees by applying reasoning very  reasoning.  Further improvements to the gam the opponents moves are also planned all the meaningless and symmetrical m 6. Game AI in Monopoly  6.1 Overview of the AI Imp The AI agent is responsible for  players in the game. The core principle a Game Tree with all the sensible move make from the current point of time minimax algorithm the agent selects t would bring the computer player mo with the highest probability. Building  that would be big enough to consider  is obstructed by the vastness of poss with all the possible random landings  nodes of the game tree exponentially tackle this problem the AI agents  discussed technologies: Case Based Re The technologies are employed  First the agent searches the CBR datab largest similarity with the current state associated with a playing strategy. Th that the planner needs to build plans f consecutive player moves that bring th way only moves that are part of that str being a small fraction of the overall po edges of the game tree at each level dec At each level of the game tree the of a single player. After the strateg considered the response to those strate by the opponent(s). The move of the  probability distribution of the dice as  player. A more general strategy needs opponent’s (human player) moves sin the expertise of the opponent. This ge more plausible moves than the focused After covering all opponents t deducting a feature move of the com CBR selected plan strategy. After  strategies and reaching a reasonable s into account the memory limits an probabilities that the move is possible the dice the building of the Game Tre algorithm searches the Game Tree  favorable move for the AI player usi The process is repeated each time the A esult with plans for possible  m each position and linking  the player we create a game  ich we can run the minimax  arget strategies creates a tree  ssible moves for the second  which X wins 176 are draw  wins. This is a significant  ames with a complete game  eful for devices with limited  rove a very important point  designing meaningful game  similar to human player  me tree are also possible if  d, in other words if we drop  moves of the opponent.  plementation the moves of the artificial  e of the AI agent is building  es that all the players would  e forward. Then using the  the move that in the future  ost favorable game position  a Game Tree in this game  sufficient number of moves  sible moves in combination  of the dice. The number of  y grows at each level. To  incorporates two already  easoning and AI Planning.  in the following manner.  base to find the case with the  e of the board. This case is  he strategy consists of goal  for, and the plans consist of  he player to that goal. This  rategy are considered, those  ossible moves the number of  creases immensely.  e model considers the moves  gies of the AI player are  egies needs to be considered  opponent(s) depends of the  well as the strategy of the  s to be implemented for the  nce we cannot be aware of  eneral strategy would bring  d strategy of the AI player.   the agent comes back to  mputer player by using the  creating several loops of  size of a Game Tree taking  nd the rapidly decreasing  e due to the distribution of  ee stops. Then the minimax  and decides on the most  ing the minimax algorithm.  AI player is up.  Interactive and Adaptable Media 299 3rd International Conference on Digital Interactive Media in Entertainment and Arts Buying, auctioning and trading game moves are always  accompanied by return of investment calculations in making the  plans. These calculations represent adaptation of the more general  planning associated with the cases in the CBR database. These  adaptations are necessary due to the fact that the cases do not  identically correspond to the situation on the table. In addition  calculating the game position value of each node of the game tree  is done by heuristic functions that incorporate economic  calculations of net present value, cash, and strategic layout and so  on. For example railroads in monopoly are known to be  strategically effective because they bring constant income even  though the income can be smaller than building on other  properties.   6.2 Details on the CBR Implementation  The implementation of the CBR is by using the JColibri2  platform.  JColibri2 is an object-oriented framework in Java for  building CBR systems that is an evolution of previous work on  knowledge intensive CBR [14].   For this implementation we need to look into three particular  classes of the JColibri2 platform. The StandardCBRApplication,  Connector, CBRQuery. For a JColibri2 implementation the  StandardCBRApplication interface needs to be implemented.   The CBR cycle executed accepts an instance of CBRQuery.  This class represents a CBR query to the CBR database. The  description component (instance of CaseComponent) represents  the description of the case that will be looked up in the database.  All  cases  and  case  solutions  are  implementing  the  CaseComponent interface.  The JColibri2 platform connects to the CBR database via a  Connector class. Each connector implements all the necessary  methods for accessing the database, retrieval of cases, storing and  deletion of cases. This implementation uses a custom XML  structure for holding the CBR cases. Since the game will not  update the CBR database only read it, a XML solution satisfies  the needs. The XML file to a certain extent is similar to the XML  representation of the board. We are interested in finding one  CBRCase that is the most similar case to the situation in the game  at the time of the search. This procedure is done in the cycle  method of the CBRApplication. The JColibri2 CBR comparison is  done by Nearest Neighbor (NN) search method.   JColibri2 offers implementations for NN search algorithms  of simple attributes. These implementations are called local  similarities. For complex attributes like in our case global  customized similarity mechanisms need to be implemented.  The MonopolyDescription class [Figure 13] is basically a  serialization of the GameState. It holds all the information about  the state of the board, the players, their amount of cash etc.   Figure 13: Class diagram of the Monopoly Case component  models  On the other hand the MonopolySolution class holds the  three particular attributes that are needed for the planning, the  planning Domain, State and TaskList.  The game is implemented by using the Model-View- Controller software development pattern. The controller is  responsible for implementing the game rules and handling all of  the events in the game like roll of dice, input commands for  trading, auctioning and etc from the players. The View layer is  responsible for displaying the board and all of the input widgets  on to the game screen, and the models are data structures  representing the game state [Figure 14].  Figure 14: Class diagram of the Monopoly models  6.2.1 Complex Similarity representation in CBR  The similarity measurement part of the Nearest Neighbor  algorithm JColibri2 is implemented by implementing the  LocalSimiralrityFunction  and  the  GlobalSimiralityFunction  interface. A local similarity function is applied to simple attributes  by the NN algorithm, and a global similarity function is applied to  compound attributes. In the case of our implementation the  attributes of the MonopolyDescription are compound attributes  describing the state of the board, number of players, amount of  cash for every player and etc. Since MonopolyDescription is a  custom CaseComponent a global similarity function needs to be  implemented to accurately find the distance between different  CBR cases.  The similarity mechanism is inseparable core element of the  CBR system. This mechanism represents how the CBR decides  which strategy is best suited for the particular situation by  300 DIMEA 2008 3rd International Conference on Digital Interactive Media in Entertainment and Arts calculating the distance or similarity to other cases in the  database.   For the monopoly implementation we need to consider  several basic strategies. Monopoly is based on investing in  properties and receiving revenues from those investments. One of  the basic strategies of the game is to build a set of properties that  will bring constant income larger than the one of the opponents.  So in time the opponents will have to declare bankruptcy. But on  the other hand over investment can lead to too stretched resources  with low income that will eventually drove the player to  bankruptcy. To decide on these two we need a clear separation  into two groups of cases in the CBR database. The first group of  cases will represent a situation on the board where the player has  significant income per loop formed of one or more color group  properties, maybe railroads, some buildings on them and so on. It  is important to note that in this case the player is better situated  than his opponents so he only needs to survive long enough to win  the game. In the other group of cases either the opponent is not  well positioned on the board or its opponents are better situated.  In this case further investments are necessary to improve the  situation so the player can have a chance of winning in the long  run.   These metrics can be owning color groups, valuing groups of  railroads, evaluating the other opponents as well, and considering  the amount of cash. As it is obvious in monopoly the number of  streets is not as nearly as important as the combination of streets  the player owns. It is also important to note that one CBR case  does not hold only a single strategy in place, but its solution can  have multiple different strategic goals. For example one CBR case  might simultaneously say buy this land to form a color group but  also trade some other unimportant property to increase cash  amount.   The cases do not represent all possible combinations of board  positions. They are only representation of typical game scenarios.  The CBR Case solutions do not give exact instructions in general  but rather strategic goals. For example one CBR Solution might  say trade the streets that you only have one of each for the ones  that you have two of that color already. Then the planner based on  the situation on the board needs to decompose this high level task  to a low level operations. Like offer "Mediterranean Avenue" for  "Reading Railroad" and offer $50. The exact amounts and actual  streets are left to the planer to evaluate.   The monopoly CBR database is currently in development on  a monopoly clone game called Spaceopoly. The cases are  architected based on human player experience and knowledge.  There is a plan of making a number of slightly different strategies  that differ on the style of playing and then running simulation  tests that would determine the particular validity of each database  as well as validity of certain segments of the strategy or even  particular cases in the database.   The actual execution of the strategies will not differ from  strategy to strategy since the plan execution is more related to the  structure and rules of the game than to the actual playing strategy.  6.3 Details on the Planning Implementation  For the purpose of planning this implementation uses a  modification of the JSHOP2 planner. The Java Simple  Hierarchical Ordered Planner 2 is a domain independent HTN  planning system [15].   JSHOP2 uses ordered task decomposition in reducing the  HTN to list of primitive tasks which form the plans. An ordered  task decomposition planner is an HTN planner that plans for tasks  in the same order that they will be executed. This reduces the  complexity of reasoning by removing a great deal of uncertainty  about the world, which makes it easy to incorporate substantial  expressive power into the planning algorithm. In addition to the  usual HTN methods and operators, the planners can make use of  axioms, can do mixed symbolic/numeric conditions, and can do  external function calls.   In order for the JSHOP2 planer to generate plans it needs  tree crucial components: Domain, State and Tasks. The Domain  defines all the functionalities that the particular domain offers.  These are simple and complex tasks. The complex tasks also  called methods create the hierarchy with the fact that they can be  evaluated by simple tasks of other complex tasks. This is how a  hierarchical structure of tasks is formed. The problem reduction is  done by reducing the high level complex tasks to simpler until all  the tasks are primitive. The list of primitive tasks forms the plan.  The State represents the state of the system. It is a simple  database of facts that represent the state of the system. The State  is necessary to determine the way the problems or tasks are  reduced to their primitive level. The reduction is done by  satisfying different prerequisites set in the methods; these  prerequisites are defined in the state. The Tasks are high level  tasks or methods defined in the Domain. The planner based on the  State and the goals selects one or more high level tasks that need  to be reduced to plans [Figure  15].  Figure 15: Diagram of a Planner  The plans then generate the game moves. The number of  moves generated by the plans is just a fraction of the possible  moves at that point. This reduces the game tree providing the  opportunity to generate smaller and deeper game trees and making  more efficient decisions in general.   7. Conclusion  Even though the results from the CBR database are not  complete at this time partial strategies are implemented as cases  and recognized during game play by the CBR system. These  smaller local strategies coupled with more global higher level  strategies that are particularly important at the beginning of the  game would form a complete CBR database and represent a  knowledge engineered style of playing of the AI player.   The AI Planning approach is a proven method by the tic-tac- toe experiment and is suitable for implementing the strategies  associated with the CBR cases.  This approach in general benefits from both technologies,  CBR as well as AI Planning and comprises an elegant solution.  Even though AI Planning can be enough as a single technology  for some simpler problems like tic-tac-toe the complexity of  Monopoly would mean that the Planner would have to incorporate  Core Planner  Tasks Plan State Interactive and Adaptable Media 301 3rd International Conference on Digital Interactive Media in Entertainment and Arts large and complex domain and a very big state model. The CBR  application helps reduce this complexity by focusing the planning  on smaller domain of the game. Basically the CBR reduces the  overall goal of the play (wining the game) to smaller more  concrete goals suitable to the particular state of the game, thus  reducing the need for global planning strategies and complex  planning domain.   Furthermore this symbiosis of technologies gives way for  more precise and finely tuned strategies which can be difficult to  include into global plan for the whole game. One simple example  for the Monopoly game would be this: Sometimes it’s better to  stay in jail because rolling double increases the probability of  landing on some field (two, four, six, eight, ten or twelve steps  from the jail) that can be of great importance to the rest of the  game. These and similar small local strategies can be easily  recognized by similar cases in the CBR database.   In other words the system is flexible enough so that new  strategies can be incorporated easily missing strategies can be also  recognized by the distance metrics as well as wrong assumptions  in the strategies can be easily recognized.  One other important property of the system is that is highly  configurable. The game its self can be diversely different  depending on the configuration of the board. Even though the  platform is restricted to Monopoly type of games, changing the  layout and values of the fields effectively brings completely  different properties of the game. In addition the CBR database  represents the entire experience of the AI Player. It can be filled  with rich set of strategies or even configured with different flavors  of difficulties of play, this of course coupled with the domain of  the planner which can differ from a case to a case as well.   8. Future Work  Further exploration of this technology would go towards  complete implementation of an AI aware agent for monopoly.  Initial results from the local cases with more specific strategies  show CBR as a capable tool for representing expertise in playing  the game. Completing the more general strategies and coupling  them with the planning domain will give precise results on the  benefits from this architecture.  There is also need for exploring the planning of strategies of  opponents. This task is to some extent different because we  cannot always expect the opponent to select the best move we  think. In the Tic-tac-toe example all possible moves of the  opponent were taken into consideration, if we used the same  planner for the opponent only tie games would result from the  game tree. In other words mistakes of the players also need to be  considered.   The CBR Platform brings other functionalities well worth of  exploring as well. The revision stage of the JColibri2 platform is  basically capable of fine tuning strategies or even developing new  strategies for the games. A well written underlying AI planning  model with a capable feedback of the game tree evaluation back  to the CBR revision capability can be an interesting concept in  automatic experience acquisition for the AI model.  There are also many other fields were combined CBR and  planning approach can be incorporated into a problem solution.  This combination is analogous in a big extent to a human way of  reasoning. People in addition to logic of reasoning in situations  with lack of information rely to planning strategies and prior  experience, exactly the intuition behind CBR – AI Planning  architecture.   9. ACKNOWLEDGMENTS  We would like to thank Prof. Sofia Tsekeridou for her  involvement in the valuable discussions we had on the topic of  CBR.  10. ','\r',char(13)));
INSERT INTO articleApp_article VALUES(35,'A Prototype Implementation of an Orthographic Software Modeling Environment','Orthographic Software Modeling (OSM) is a view-centric software engineering approach that aims to leverage the orthographic projection metaphor used in the visualization of physical objects to visualize software systems. Although the general concept of OSM does not prescribe specific sets of views, a concrete OSM environment has to be specific about the particular views to be used in a particular project. At the University of Mannheim we are developing a prototype OSM environment, nAOMi, that supports the views defined by the KobrA 2.0 method, a version of KobrA adapted for OSM. In this paper we provide an overview of the KobrA 2.0 metamodel underpinning nAOMi and give a small example of its use to model a software system.',0,'Orthographic Software Modeling, View-based Modeling','https://drive.google.com/file/d/1gz-lb-VjQ-GP3VHHSYhVsdWJC-80C6bs/view?usp=drivesdk',NULL,'Orthographic Software Modeling (OSM) is based on three fundamental hypotheses - (a) that it is feasible to integrate the many different kinds of artifacts used in contemporary software engineering methods within a single coherent methodology in which they are treated as views, (b) that it is feasible to create an efficient and scalable way of supporting these views by generating them dynamically, on-the-fly, from a Single Underlying Model (SUM) using model-based transformations and (c) that it is feasible to provide an intuitive metaphor for navigating around these many views by adapting the orthographic projection technique underpinning the CAD tools used in other engineering disciplines.  As shown in Figure 1, the main advantages of using the idea of orthographic projection to define the views used to visualize and describe a system are that they (a) can be organized according to a simple and easy-to-understand metaphor and (b) collectively represent all the properties of a system with minimal overlap and redundancy. In practice, this translates into a set of "dimensions", each containing well-defined choices (or so-called "dimension elements") that can be used to select individual views.  As shown in Figure 2, the main advantage of making the artifacts used to describe a software system views of a SUM is that the number of pairwise coherence relationships that have to be maintained is reduced and new views can be introduced by simply defining their relationship to the SUM. Moreover, the importance of this advantage grows quickly as the size of the system and the complexity of the deployed development methodology increase. Another important advantage is that the dominance of one particular kind of view over the development process (e.g. code) at the expense of other kinds of views (e.g. graphical models) is reduced so that any appropriate type of views can be used to enrich the underlying description of the system, depending on the needs and skills of the stakeholder involved. This makes it possible to subsume all view types under the same, overarching concept, i.e., the view.SUM SUM / View Centric Environment Artifact / Tools Centric Environment Figure 2: Consistency Dependencies in Artifact-oriented versus View-oriented Environments. ing development process and methodology (e.g. agile-driven, focusing on small development cycles, or model-driven de- velopment, based on transformations between abstraction levels). Although the details of how the views are created from the SUM and how the SUM is updated from the views are not central to the approach, a natural implementation is to use the visualization and transformation technologies oﬀered by model driven software engineering (MDSE). To explore the validity of these hypotheses at the Uni- versity of Mannheim we have been developing a prototype OSM modeling environment based on an enhanced version of the KobrA method for model-driven, component-oriented development, KobrA 2.0 [1]. This was chosen as a basis for the prototype, known as the Open, Adaptable, Orthographic Modeling Environment (nAOMi) [13] because its views were designed with the precise goals of being (a) genuine pro- jections of a subject containing carefully selected subsets of information about that subject, (b) minimalistic in the sense that they should overlap to the smallest extent possible and contain the minimum necessary models elements, and (c) selectable via a set of independent “dimensions” which reﬂect diﬀerent fundamental concerns of development (i.e. abstraction levels, composition or variants). In other words, KobrA already provided one of the “most orthogonal” sets of views for visualizing software systems of any contempo- rary method. More details about the actual views and di- mensions deﬁned in KobrA are presented in the following sections. More information on OSM can be found in [2] and [3]. nAOMi is implemented as an Eclipse plugin using the Eclipse Modeling Framework (EMF) as the underlying mod- eling platform and UML 2.0 tools [4] to generate and edit views. The KobrA 2.0 metamodel on which the current version of nAOMi is based is a specialization of the UML metamodel composed of three separate packages — one for the SUM, one for the views and one for the transformations (Figure 3). The UML was chosen as the base language be- cause of its maturity and widespread acceptance, making the environment usable to the largest possible body of develop- ers. UML elements not needed in KobrA 2.0 are excluded using OCL constraints while new elements or properties are KobrA2 Transformation SUM Views Figure 3: KobrA 2.0 Top Level Packages. introduced by specializing existing elements. The unique contribution of this paper is to elaborate on the structure of the KobrA 2.0 metamodel and how it is used to drive nAOMi. The three following sections each focus on one of the three main components of the metamodel — the SUM, the views and the transformations . This is followed by a brief overview of the OSM navigation paradigm in Sec- tion 5 before a small example of the approach is presented in Section 6. Section 7 then concludes the paper with related and future work. 2. SUM PACKAGE Figure 4 depicts the internal structure of the SUM pack- age which is based on the UML metamodel. There are three main subpackages, two containing the structural and behav- ioral constructs respectively, and one containing the con- straints that ensure that the metaclasses are used according to the KobrA conventions and rules. The Classes subpackage of the Structure package contains some of the most fundamental elements of the KobrA meta- model, such as Class and ComponentClass. The internal structure of this package is illustrated in Figure 5. Com- ponentClass represents objects with complex and reusable behaviors, while Class captures simple “data type” objects that have only very simple or non-reusable behaviors. The modeler has to decide whether it is necessary to model a speciﬁc part of the system as a ComponentClass and include state charts and activity diagrams, or whether it is suﬃcient to use a Class (which is limited to using OCL constraints). ComponentClass inherits (indirectly via Class) from Com- munications so it also has the isActive attribute. This makes KobrA2::SUM::Constraint::Behavioral KobrA2::SUM::Constraint::Structural KobrA2::SUM::Constraint KobrA2::SUM::Constraint::Common KobrA2::SUM::Behavior::ProtocolStateMachines KobrA2::SUM::Behavior::Common KobrA2::SUM::Behavior::Activities KobrA2::SUM::Behavior::Actions KobrA2::SUM::Behavior KobrA2::SUM::Structure::Classes KobrA2::SUM::Structure::Types KobrA2::SUM::Structure::Instances KobrA2::SUM::Structure::Elements KobrA2::SUM::Structure KobrA2::SUM::Constraint::OclExpressions <<merge>> <<merge>> <<merge>> <<merge>> <<merge>> <<merge>> <<merge>> <<merge>> <<merge>> <<merge>> <<merge>> <<merge>> Figure 4: KobrA 2.0 SUM Package. it possible to model whether its instances are active or pas- sive. Active objects, which can be used to model threads and processes ([8] p. 438), start to execute their behavior as soon as they are created and perform operations spontaneously. A ComponentClass may exhibit complex behavior. In Ko- brA, this behavior may be speciﬁed in the form of UML State Diagrams (deﬁning acceptable operation invocation sequences), and in the form of Activities (deﬁning algorithms of operations). UML Interaction elements (in sequence dia- grams) can be derived from the activity elements and thus are not included in the SUM. As KobrA aims to facilitate automatic checking of allowed sequences of operation calls, Protocol State Machines are supported instead of general state machines. Since the latter include a large variety of elements not needed for specifying acceptable operation se- quences or automatic checking, OCL constraints are used to prohibit the use of unwanted features. context ComponentClass -- only allow Activity elements or ProtocolStateMachines inv: ownedBehavior ->forAll( oclIsKindOf( Actitivity) or oclIsKindOf ( ProtocolStateMachine )) For example, since KobrA has no concept of roles for com- ponents, the use of role also needs to be prohibited. The part association refers to owned properties of components whose attribute isComposite is true. As KobrA uses associations like nests and creates for components, part, required and provided are not needed. Connectors (i.e. delegation and assembly) are not used in KobrA either so ownedConnector is excluded. Class KobrA2::SUM::Structure::Classes GeneralizationSet AssociationClass ComponentClass Property Usage Association Operation Packageable Element Parameter Acquires Creates Nests UML::Component::PackagingComponents::Component UML::CommonBehaviors::Communications::Class +ownedOperation * +class 0..1 +supplier 1..* {subsets supplierDependency} +supplierUsage * +client 1..* {subsets clientDependency} +clientUsage * +ownedAttribute * +class 0..1 +powertype 0..1 +powertypeExtent * +packagedElement * {subsets component} +componentClass 0..1 +/superClass Figure 5: KobrA 2.0 Classes Package. context ComponentClass inv: role ->union(part)->union( ownedConnector ) ->union( collaborationUse )-> union( representation ) ->union( realization)->union(required) ->union(provided)->isEmpty () 3. VIEWS PACKAGE The structure of the Views package is illustrated in Figure 6. Again, since most of the views deﬁned in KobrA 2.0 are based on UML diagrams, the view metamodels have similar elements to the SUM metamodel. The big diﬀerence to the SUM is that there are no restrictions on the use of the view metamodel elements. For instance, views for a particular purpose such as supporting model checkers can be supported by adding elements unrelated to the UML. The substructure of the Views package reﬂects the types and organization of the KobrA views according to the view “dimensions” supported in nAOMi (cf. example in Section 6). At the top level, the Views package is thus decomposed into the Speciﬁcation and Realization options of the encap- sulation dimension. These, in turn are both decomposed into the Structural, Behavioral and Operational options of the Projection dimension. Finally, with the exception of the behavioral option, these are also all subdivided into the Service and Type options of the granularity dimension. This dimension, with its two options, is an addition to the original version of KobrA. The Service view shows the direct, publicly visible rela- tionships of the subject ComponentClass to other Compo- nentClasses, while the Type view shows the publicly visi- ble relationships of the subject to simple Classes. As with the SUM, constraints have been deﬁned to control what can go into each view and when they are well formed. For ev- ery view, a constraint enumerates all allowed elements (not shown in this paper). In the following, some of the other constraints for the Service view are elaborated. Since this view is a black-box view, the internals of ComponentClasses (nestedClassiﬁer) are not shown. context ComponentClass -- no nested classifiers , no protocol inv: nestedClassifier ->union(protocol)->isEmpty () Classes are only allowed if they are generalizations of Com- ponentClasses, (or any of its superclasses, since a Compo- nentClass may inherit from a class as shown in the con- straints with context Class. The following invariants ensure that only publicly visible attributes and operations are in this view, for both classes and ComponentClasses (which inherit from Class). Class Service Type Instance Service Type Structural Specification Operational Service Type Protocol Behavioral KobrA2::Views::Derived ComponentClassDependencies OperationDependencies Instance Service Type Class Service Type Structural Realization Operational Service Type Behavioral Algorithm Views ConcreteSyntax Subject <<import>> <<merge>> <<merge>> <<import>> <<merge>> <<import>> Figure 6: KobrA 2.0 Views package nesting. context Class -- only allow classes that are direct or indirect generalizations of ComponentClasses in this view def: ccGeneralization : generalization .specific -> exists( oclIsKindOf ( ComponentClass )) inv: generalization .specific ->select( oclIsTypeOf ( Class))->exists(s|s. ccGeneralization ) or ccGeneralization -- only public attributes in this view inv: ownedAttribute ->forAll(visibility =# public) -- only public Operations are allowed in the specification inv: ownedOperation ->forAll(visibility =# public) Only operation signatures are shown in this view, so pre-, post- and bodyconditions, as well as activities are omitted, which is reﬂected in the last constraint. context Operation -- only the signature of the Operation is shown , not its behavior (role name "method" refers to the Activities of the operation), or dependencies inv: method ->union( precondition )->union(body)->union( postcondition )->isEmpty () 4. TRANSFORMATIONS PACKAGE The package AllViews provides the foundation for speci- fying the transformations between the SUM and the views in both directions. Part of the package’s contents are shown in Figure 7. The Abstraction concept (which is in fact a KobrA2::Transformation::Common::AllViews Abstraction TransformationExpression ViewElement SumElement View KobrA2::SUM::Structure::Elements::Element KobrA2::Views::ConcreteSyntax::Element KobrA2::SUM::Constraint::Behavioral::Exp ressionInOcl KobrA2::Views::Subject::View {subsets mapping} 0..1 0..1 {subsets clientDependency} +abstraction 1 {subsets client} +ve 1 1..* 1 {subsets supplier} +se 1 {subsets supplierDependency} +abstraction 1..* Figure 7: Transformation abstractions. dependency reused from the UML but with additional con- straints) plays the key role in relating elements from the SUM to elements of a view. Abstraction is actually mapped to ExpressionInOcl. When appearing in transformations, the equals sign links elements in the SUM to the respective elements in the view, and vice versa. For instance, equal- ity of the general meta-association of a Generalization in a transformation invariant means that, when following gen- eral, there must be an element in the SUM and in the view for which similar transformation expressions are speciﬁed. In the case of KobrA 2.0, which has many projections that just select a subset of elements using one-to-one abstrac- tions, this allows concise declarative TransformationExpres- sions. Together with the view constraints, a CASE tool can be implemented which uses a transformation language of the implementor’s choice, for instance the Atlas Transformation Language (ATL) [11] or QVT [9]. The role names se and ve are short for SumElement and ViewElement, respectively. These roles subset the client and supplier roles from the UML. SUM elements are translated into UML elements with stereotypes, so that the views are easy to manage for de- velopers familiar with the UML. The bidirectional mappings between stereotyped view elements and non-stereotyped SUM elements are expressed in the constraints of the Association- Abstraction, a subclass of the Abstraction from the AllViews package. This is also an example of a transformation which is reused in other views. context AssociationAbstraction inv: ve.memberEnd = se.memberEnd inv: ve.ownedEnd = se.ownedEnd ivn: ve. navigableOwnedEnd = se. navigableOwnedEnd inv: se. oclIsKindOf(Acquires) implies ve. hasStereotype (’acquires ’) inv: ve. hasStereotype (’acquires ’) implies se. oclIsKindOf (Aquires) inv: se. oclIsKindOf(Nests) implies ve. hasStereotype (’ nests ’) inv: ve. hasStereotype (’nests ’) implies se. oclIsKindOf (Nests) inv: se. oclIsKindOf (Creates) implies ve. hasStereotype (’creates ’) inv: ve. hasStereotype (’creates ’) implies se. oclIsKindOf (Creates) Figure 8 shows the main elements involved in the trans- formation of the black box structural view for Component- Classes. The ﬁrst transformation constraint is on the view and declares the starting point for the transformation. It states that the subject ComponentClass and its generaliza- tions (using a SUM utility function, superClosure) are in the view. The following transformation rules illustrate how to create the output (i.e. view) elements from the input (i.e. SUM) el- ements, such as the publicly visible attributes and operations of the ComponentClass and the acquired ComponentClasses. The ﬁrst constraint for ComponentClassAbstraction states that ');
INSERT INTO articleApp_article VALUES(36,'Framing the News: From Human Perception to Large Language Model Inferences','Identifying the frames of news is important to understand the articles’ vision, intention, message to be conveyed, and which aspects of the news are emphasized. Framing is a widely studied concept in journalism, and has emerged as a new topic in computing, with the potential to automate processes and facilitate the work of journalism professionals. In this paper, we study this issue with articles related to the Covid-19 anti-vaccine movement. First, to understand the perspectives used to treat this theme, we developed a protocol for human labeling of frames for 1786 headlines of No-Vax movement articles of European newspapers from 5 countries. Headlines are key units in the written press, and worth of analysis as many people only read headlines (or use them to guide their decision for further reading.) Second, considering advances in Natural Language Processing (NLP) with large language models, we investigated two approaches for frame inference of news headlines: first with a GPT-3.5 fine-tuning approach, and second with GPT-3.5 prompt-engineering. Our work contributes to the study and analysis of the performance that these models have to facilitate journalistic tasks like classification of frames, while understanding whether the models are able to replicate human perception in the identification of these frames.',0,'Covid-19 no-vax, news framing, GPT-3, prompt-engineering, transformers, large language models','https://drive.google.com/file/d/1M1-YXwXWFMY0U4Gd4vBsUTuwCmIgwJ1p/view?usp=drivesdk',NULL,'  INTRODUCTION In recent years, there has been a proliferation in the use of concepts such as data journalism, computational journalism, and computer-assisted reporting [15] [29], which all share the vision of bridging journalism and technology. The progress made in NLP has been gradually integrated into the journalistic field [5][8][54]. More specifically, machine learning models based on transformers have been integrated in the media sector in different tasks [41] such as the creation of headlines with generative languages models [17], summarization of news articles [28][27], false news detection [49], and topic modeling and sentiment analysis [25]. The development of large language models such as GPT-3 [9], BLOOM [51] or ChatGPT show a clear trend towards human-machine interaction becoming easier and more intuitive, opening up a wide range of research possibilities. At the same time, the use of these models is also associated with a lack of transparency regarding how these models work, but efforts are being made to bring some transparency to these models, and to analyze use cases where they can be useful and where they cannot [35]. Based on the premises that these models open up a wide range of research directions [7], and that at the same time (and needless to say) they are not the solution to all problems, we are interested in identifying use cases and tasks where they can be potentially useful, while acknowledging and systematically documenting their limitations [56]. More specifically, the aim of this work is to analyze the performance of GPT-3.5 for a specific use case, namely the analysis of frames in news, from an empirical point of view, with the objective of shedding light on a potential use of generative models in journalistic tasks.  Frame analysis is a concept from journalism, which consists of studying the way in which news stories are presented on an issue, and what aspects are emphasized: Is a merely informative vision given in an article? Or is it intended to leave a moral lesson? Is a news article being presented from an economic point of view? Or from a more human, emotional angle? The examples above correspond to different frames with which an article can be written. The concept of news framing has been studied in computing as a step beyond topic modeling and sentiment analysis, and for this purpose, in recent years, pre-trained language models have been used for fine-tuning the classification process of these frames [60] [10], but the emergence of generative models opens the possibility of doing prompt-engineering of these classification tasks, instead of the fine-tuning approach investigated so far.  Our work aims to address this research gap by posing the following research questions: RQ1: What are the main frames in the news headlines about the anti-vaccine movement, as reported in newspapers across 5 European countries? 627ICMR ’23, June 12–15, 2023, Thessaloniki, Greece David Alonso del Barrio and Daniel Gatica-Perez RQ2: Can prompt engineering be used for classification of head- lines according to frames? By addressing the above research questions, our work makes the following contributions: Contribution 1. We implemented a process to do human an- notation of the main frame of 1786 headlines of articles about the Covid-19 no-vax movement, as reported in 19 newspapers from 5 European countries (France, Italy, Spain, Switzerland and United Kingdom.) At the headline level, we found that the predominant frame was human interest, where this frame corresponds to a per- sonification of an event, either through a statement by a person, or the explanation of a specific event that happened to a person. Furthermore, we found a large number of headlines annotated as containing no frame, as they simply present information without entering into evaluations. We also found that for all the countries involved, the distribution of frame types was very similar, i.e., hu- man interest and no frame are the two predominant frames. Finally, the generated annotations allowed to subsequently study the per- formance of a large language model. Contribution 2. We studied the performance of GPT-3.5 on the task of frame classification of headlines. In addition to using the fine-tuning approach from previous literature, we propose an alternative approach for frame classification that requires no labeled data for training, namely prompt-engineering using GPT-3.5. The results show that fine-tuning with GPT-3.5 produces 72% accuracy (slightly higher than other smaller models), and that the prompt- engineering approach results in lower performance (49% accuracy.) Our analysis also shows that the subjectivity of the human labeling task has an effect on the obtained accufracy. The paper is organized as follows. In Section 2, we discuss related work. In Section 3, we describe the news dataset. In Section 4, we describe the methodology for both human labeling and machine classification of news frames. We present and discuss results for RQ1 and RQ2 in Sections 5 and 6, respectively. Finally, we provide conclusions in Section 7. 2 RELATED WORK Framing has been a concept widely studied in journalism, with a definition that is rooted in the study of this domain [23]: “To frame is to select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a par- ticular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described.” For frame recognition, there are two main approaches: the induc- tive approach [16], where one can extract the frames after reading the article, and the deductive approach [38], where a predefined list of frames exists and the goal is to interpret if any of them ap- pears in the article. In the deductive case, there are generic frames and subject-specific frames, and the way to detect them typically involves reading and identifying one frame at a time, or through answers to yes/no questions that represent the frames. Semetko et al. [52] used 5 types of generic frames (attribution of responsibility, human interest, conflict, morality, and economic consequences) based on previous literature, and they defined a list of 20 yes/no questions to detect frames in articles. For instance, the questions about morality are the following: "Does the story contain any moral message? Does the story make reference to morality, God, and other religious tenets? Does the story offer specific social prescriptions about how to behave?", and so on for each of the frame types. This categorization of frames has been used in various topics such as climate change [18] [19], vaccine hesitance [13], or immigration [34]. We now compare the two approaches on a common topic, such as Covid-19. Ebrahim et al. [21] followed an inductive approach in which the frames were not predefined but emerged from the text (e.g., deadly spread, stay home, what if, the cost of Covid-19) using headlines as the unit of analysis. In contrast, the deductive approach has studied very different labels. El-Behary et al. [22] followed the method of yes/no questions, but in addition to the 5 generic frames presented before, they also used blame frame and fear frame. Adiprasetio et al. [1] and Rodelo [50] used the 5 generic frames with yes/no questions, while Catalán-Matamoros et al. [14] used the 5 frames and read the headline and subheadline to decide the main frame. Table 1 summarizes some of the the existing approaches. This previous work showed how frame labels can be different, and also that frame analysis has been done at both headline and article levels. These two approaches (inductive and deductive) that originated in journalism have since been replicated in the computing literature. We decided to follow the deductive approach because a prede- fined list of frames allows to compare among topics, countries, previous literature, and also because they represent a fixed list of labels for machine classification models. Furthermore, the induc- tive approach tends to be more specific to a topic, and from the computing viewpoint, past work has tried to justify topic modeling as a technique to extract frames from articles. Ylä-Antitila et al. [60] proposed topic modeling as a frame ex- traction technique. They argued that topics can be interpreted as frames if three requirements are met: frames are operationalized as connections between concepts; subject-specific data is selected; and topics are adequately validated as frames, for which they suggested a practical procedure. This approach was based on the choice of a specific topic (e.g., climate change) and the use of Latent Dirichlet Allocation (LDA) as a technique to extract a number of subtopics. In a second phase, a qualitative study of the top 10 words of each subtopic was performed, and the different subtopics were elimi- nated or grouped, reducing the number and establishing a tentative description. In a third phase, the top 10 articles belonging to that frame/topic were taken, and if the description of the topic fitted at least 8 of the 10 articles, that topic/frame remained. The frames found in this article were: green growth, emission cuts, negotiations and treaties, environmental risk, cost of carbon emissions, Chinese emissions, economics of energy production, climate change, en- vironmental activism, North-South burden sharing, state leaders negotiating, and citizen participation. From Entman’s definition of frame [23], it seems that the deduc- tive approach is more refined than the inductive approach (which seems to resemble the detection of sub-themes.) For example, with regard to climate change, there are stories on how people have been affected by climate change from an emotional point of view, thus personalizing the problem. In this case, we could categorize the corresponding frame as human interest, as the writer of the article is selecting "some aspects of a perceived reality and make them 628 Framing the News: From Human Perception to Large Language Model Inferences ICMR ’23, June 12–15, 2023, Thessaloniki, Greece more salient". The language subtleties with which news articles are presented cannot be captured with basic topic modeling. Isoaho et al.[30] held the position that while the benefits of scale and scope in topic modeling were clear, there were also a number of problems, namely that topic outputs do not correspond to the methodological definition of frames, and thus topic modeling remained an incomplete method for frame analysis. Topic modeling, in the practice of journalistic research, is a useful technique to deal with the large datasets that are available, yet is often not enough to do more thorough analyses [31]. In our work, we clearly notice that frame analysis is not topic modeling. For example, two documents could be about the same topic, say Covid-19 vaccination, but one article could emphasize the number of deaths after vaccination, while the other emphasized the role of the vaccine as a solution to the epidemic. We also consider that the larger the number of possible frame types, the more likely it is to end up doing topic modeling instead of frame analysis. Using a deductive approach, Dallas et al. [12] created a dataset with articles about polemic topics such as immigration, same sex marriage, or smoking, and they defined 15 types of frames: "economic, capacity and resources, morality, fairness and equality, legality, constitutionality and jurisprudence, policy prescription and evaluation, crime and punishment, security and defense, health and safety, quality of life, cultural identity, political, external regulation and reputation, other". In this case, they authors did not use a list of questions. Instead, for each article, annotators were asked to identify any of the 15 framing dimensions present in the article and to label text blurbs that cued them (based on the definitions of each of the frame dimensions) and decide the main frame of each article. In our case, we followed the idea of detecting the main frame by reading the text instead of answering questions, but instead of using the 15 frames proposed in [12] , we used the 5 generic frames proposed in [52]. A final decision in our work was the type of text to analyze, whether headlines or whole article. For this decision, the chosen classification method was also going to be important. For example, Khanehzar et al. [33] used traditional approaches such as SVMs as baseline, and demonstrated the improvement in frame classifica- tion with the use of pre-trained languages models such as BERT, RoBERTa and XLNet, following a fine-tuning approach, setting as input text a maximum of 256 tokens (although the maximum number of input tokens in these models is 512 tokens.) Liu et al. [37] classified news headlines about the gun problem in the United States, arguing for the choice of headlines as a unit of analysis based on previous journalism literature [6], [44], that advocated for the importance and influence of headlines on readers and the subsequent perception of articles. From a computational viewpoint, using headlines is also an advantage, since you avoid the 512 token limitation in BERT-based models. Therefore, we decided to work with headlines about a controversial issue, namely the Covid-19 no-vax movement. Continuing with the question of the methods used for classi- fication, much work has been developed in prompt engineering, especially since the release of GPT-3. Liu et al.[36] presented a good overview of the work done on this new NLP paradigm, not only explaining the concept of prompt engineering, but also the differ- ent strategies that can be followed both in the design of prompts, Table 1: Summary of deductive approaches for frame analysis Ref Frames Goal Technique Number of samples [12] 15 generic frames: "Economic", "Capac- ity and resources", "Morality", "Fair- ness and equality", "Legality, constitu- tionality and jurisprudence", "Policy prescription and evaluation", "Crime and punishment", "Security and de- fense", "Health and safety", "Quality of life", "Cultural identity", "Public opin- ion", "Political", "External regulation and reputation", "Other". To label frames of full articles Reading the full article, the annotator defines the main frame 20000 articles [33] 15 generic frames Classification BERT based models 12000 articles [52] 5 generic frames: "human interest", "conflict", "morality", "attribution of responsibility", and "economic conse- quences". To label frames of full articles Yes/No ques- tions. 2600 articles and 1522 tv news stories [37] 9 specific frames:“Politics”, “Public opinion”, “Society/Culture”, and “Economic consequences” , “2nd Amendment” (Gun Rights), “Gun control/regulation”, “Mental health”, “School/Public space safety”, and “Race/Ethnicity”. To label frames of full articles/ Classification Reading the full article, the annotator defines the main frame. BERT based models 2990 headlines [22] 5 generic frames + blame frame and fear frame To label frames of full articles Yes/No ques- tions. 1170 articles [1] 5 generic frames To label frames of full articles Reading the full article, the annotator defines the main frame. 6713 articles [50] 5 generic frames + pandemic frames To label frames of full articles Yes/No ques- tions. 2742 articles [14] 5 generic frames, journalistic role and pandemic frames To label frames of full articles Reading the headline and subheadline, the annotator defines the main frame. 131 headlines + subheadlines the potential applications, and the challenges to face when using this approach. Prompt engineering applications include knowledge probing [46], information extraction [53], NLP reasoning [57], ques- tion answering [32], text generation [20], multi-modal learning [58], and text classification [24], the latter being the prompt-engineering use case in our work. Puri et al.[45] presented a very interesting idea that we apply to our classification task. This consists of pro- viding the language model with natural language descriptions of classification tasks as input, and training it to generate the correct answer in natural language via a language modeling objective. It is a zero-shot learning approach, in which no examples are used to explain the task to the model. Radford et al. [48] demonstrated that language models can learn tasks without any explicit supervision. We have followed this approach to find an alternative way to do frame analysis. As mentioned before, the emergence of giant models like GPT-3, BLOOM, and ChatGPT are a very active research topic. To the best of our knowledge, on one hand our work extends the computational analysis of news related to the covid-19 no-vax movement, which illustrates the influence of the press on the ways societies think about relevant issues [40], [59], and on the other hand it adds to the literature of human-machine interaction, regarding the design of GPT-3 prompts for classification tasks [39], [2]. 3 DATA: EUROPEAN COVID-19 NEWS DATASET We used part of the European Covid-19 News dataset collected in our recent work [3]. This dataset contains 51320 articles on Covid- 19 vaccination from 19 newspapers from 5 different countries: Italy, 629 ICMR ’23, June 12–15, 2023, Thessaloniki, Greece David Alonso del Barrio and Daniel Gatica-Perez France, Spain, Switzerland and UK. The articles cover a time period of 22 months, from January 2020 to October 2021. All content was translated into English to be able to work in a common language. The dataset was used for various analyses, such as name entity recognition, sentiment analysis, and subtopic modeling, to under- stand how Covid-19 vaccination was reported in Europe through the print media (in digital format.) The subtopic modeling analysis revealed a subsample of articles on the no-vax movement, which is the one we have used in this paper. We took the headlines of the articles associated with the no-vax movement, selecting all articles containing any of the keywords in Table 2 in the headline or in the main text. This corresponds to a total of 1786 headlines. Table 2: Keywords used to identify no-vax articles Keywords NO VAX TOPIC "anti-vaxxers", "anti-vaccine", "anti-vaxx", "anti-corona", "no-vax", "no vax","anti-vaccin" In Table 3, we show the number of headlines per country and newspaper. France is the country with the most no-vax articles in the corpus, with 523 articles, followed by Italy with 508. However, note that there are 6 newspapers from France, while only 2 from Italy. Corriere della Sera is the newspaper that dealt most frequently with the subject (429 articles), while The Telegraph is the second one (206 articles). The total number of articles normalized by the number of newspapers per country is also shown in the last column of the Table. Using these normalized values, the ranking is Italy, UK, France, Switzerland, and Spain. Table 3: Number of headlines by newspaper and country COUNTRY NEWSPAPER HEADLINES TOTAL (NORM. TOTAL) FRANCE La Croix 94 523 (87.1) Le Monde 125 Les Echos 49 Liberation 97 Lyon Capitale 8 Ouest France 150 ITALY Corriere della Sera 429 508 (254.0) Il Sole 24 Ore 79 SPAIN 20 minutos 27 303 (50.5) ABC 50 El Diario 32 El Mundo 77 El Español 22 La Vanguardia 95 SWITZERLAND 24 heures 97 230 (76.6) La Liberté 22 Le Temps 111 UNITED KINGDOM The Irish News 16 222 (111.0) The Telegraph 206 1786 4 METHODOLOGY 4.1 Human labeling of news frames To carry out the labeling of the frames in our corpus of headlines, we first designed a codebook, which contained the definitions of each of the frame types and a couple of examples of each type, as well as a definition of the corpus subject matter and definitions of the concept of frame analysis, so that the annotators could understand the task to be performed. The codebook follows the proposed by [52] with 5 generic frames (attribution of responsibility, human interest, conflict, morality, and economic consequences) plus one additional ’no-frame’ category. Two researchers were engaged to annotate a sample of the collected newspaper articles following a three-phase training procedure. In the first phase, annotators had to read the codebook and get familiar with the task. In the second phase, they were asked to identify the main frame in the same subset of 50 headlines. At the end of the second phase, the intercoder reliability (ICR) was 0.58 between the 2 annotators. We analyzed those cases where there were discrepancies, and observed that in some cases, there was not a unique main frame, because both annotators had valid arguments to select one of the frames. In other cases, the discrepancies were due to slight misunderstanding of the definitions. In the third phase, the annotators coded again 50 headlines, and the ICR increased to was 0.66. We realized that the possibility of having two frames remained. They discussed the cases in which they had disagreed, and if the other person’s arguments were considered valid, it could be said that there were two frames. After this three-phase training procedure, annotators were ready to annotate the dataset independently. We divided the dataset into two equal parts, and each person annotated 893 headlines. 4.2 Fine-tuning GPT-3.5 and BERT-based models With the annotated dataset, we investigated two NLP approaches: the first one involves fine-tuning a pre-trained model; the second one is prompt engineering. Pre-trained language models have been Figure 1: Pre-train, fine-tune, prompt trained with large text strings based on two unsupervised tasks, next sentence prediction and masked language model. Figure 1 summarizes these techniques. In the first approach, a model with a fixed architecture is pre- trained as a language model (LM), predicting the likelihood of the observed textual data. This can be done due to the availability of large, raw text data needed to train LMs. This learning process can produce general purpose features of the modeled language. The learning process produces robust, general-purpose features of the language being modeled. The above pre-trained LM is then adapted to different downstream tasks, by introducing additional parameters and adjusting them using task-specific objective functions. In this approach, the focus was primarily on goal engineering, designing the training targets used in both the pre-training and the fine-tuning stages [36]. 630 Framing the News: From Human Perception to Large Language Model Inferences ICMR ’23, June 12–15, 2023, Thessaloniki, Greece We present an example to illustrate the idea. Imagine that the task is sentiment analysis, and we have a dataset with sentences and their associated sentiment, and a pre-trained model, which is a saved neural network trained with a much larger dataset. For that pre-trained model to address the target task, we unfreeze a few of the top layers of the saved model base and jointly train both the newly-added classifier layers and the last layers of the base model. This allows to "fine-tune" the higher-order feature representations in the base model to make them more relevant for the sentiment analysis task. In this way, instead of having to obtain a very large dataset with target labels to train a model, we can reuse the pre- trained model and use a much smaller train dataset. We use a part of our dataset as examples for the model to learn the task, while the other part of the dataset is used to evaluate model performance. Previous works related to frame classification in the computing literature have used fine-tuning, BERT-based models. In our work, we have done the same as a baseline, but we aimed to go one step further and also produce results using fine-tuning of GPT-3.5. 4.3 Prompt-engineering with GPT-3.5 Model fine-tuning has been widely used, but with the emergence of generative models such as GPT-3, another way to approach classification tasks has appeared. The idea is to use the pre-trained model directly and convert the task to be performed into a format as close as possible to the tasks for which it has been pre-trained. That is, if the model has been pre-trained from next word prediction as in the case of GPT-3, classification can be done by defining a prompt, where the input to the model is an incomplete sentence, and the model must complete it with a word or several words, just as it has been trained. This avoids having to use part of the already labeled dataset to teach the task to be performed to the model, and a previous labeling is not needed [36]. In this approach, instead of adapting pre-trained LMs to down- stream tasks via objective engineering, downstream tasks are re- formulated to look more like those solved during the original LM training with the help of a textual prompt. For example, when recog- nizing the emotion of a social media post, “I missed the bus today.”, we may continue with a prompt “I felt so _”, and ask the LM to fill the blank with an emotion-bearing word. Or if we choose the prompt “English: I missed the bus today. French: _”), an LM may be able to fill in the blank with a French translation. In this way, by selecting the appropriate prompts, we can influence the model behavior so that the pre-trained LM itself can be used to predict the desired output, even without any additional task-specific training [36]. We use this emerging NLP approach to classify frames at headline level. We are not aware of previous uses of this strategy to classify frames as we propose here. The idea is the following. Prompt engi- neering consists of giving a prompt to the model, and understands that prompt as an incomplete sentence. To do prompt engineer- ing with our dataset, we needed to define an appropriate prompt that would produce the headline frames as output. We defined sev- eral experiments with the Playground of GPT-3, in order to find the best prompt for our task. In our initial experiments, we fol- lowed existing approaches in prompt engineering to do sentiment analysis, where the individual answer was an adjective, and this adjective was matched with a sentiment. In a similar fashion, we decided to build a thesaurus of adjectives that define each of the frames. For instance, the human interest frame could be ’interest- ing’, ’emotional’, ’personal’, ’human’. The conflict frame could be: ’conflictive’, ’bellicose’, ’troublesome’, ’rowdy’, ’quarrelsome’, ’trou- blemaker’, ’agitator’, etc. After the list of adjectives was defined, we needed to define the prompt in order to get, as an answer, one of the adjectives in our thesaurus to match them with the frame. We used the GPT-3 playground using the headline as input and asking for the frame as output, but the strategy did not work. In our final experiment, instead of giving the headline as input, we gave the definitions of each type of frame plus the headline, and we asked the model to choose between the different types of frames as output. In this way, the output of the model was directly one of the frames, and we avoided the step of matching adjectives with frames. An example is shown in Figure 2. Figure 2: GPT-3.5 for frame inference: input and output For the GPT-3 configuration 1, there are 3 main concepts: • TEMPERATURE [0-1]. This parameter controls randomness, lowering it results in less random completions. • TOP_P [0-1]. This parameter controls diversity via nucleus sampling. • MAX_TOKENS[1-4000]. This parameter indicates the maxi- mum number of tokens to generate, • MODEL. GPT-3 offer four main models with different levels of power, suitable for different tasks. Davinci is the most capable model, and Ada is the fastest. After testing with the GPT-3 playground and varying different hyper-parameters to assess performance, we set the temperature to 0, since the higher the temperature the more random the response. Furthermore, the Top-p parameter was set to 1, as it would likely get a set of the most likely words for the model to choose from. The maximum number of tokens was set to 2; in this way, the model is asked to choose between one of the responses. As a model, we used the one with the best performance at the time of experimental design, which was TEXT-DAVINCI-003, recognized as GPT 3.5. 5 RESULTS: HUMAN LABELING OF FRAMES IN NO-VAX NEWS HEADLINES (RQ1) In this section, we present and discuss the results of the analysis related to our first RQ. Figure 3 shows the distribution of frames per country at headline level, with human interest and no-frame being the predominant 1https://beta.openai.com/docs/introduction 631 ICMR ’23, June 12–15, 2023, Thessaloniki, Greece David Alonso del Barrio and Daniel Gatica-Perez ones. Attribution of responsibility is the third one except in Switzer- land, where the corresponding frame is conflict. Finally, morality and economic are the least represented in the dataset for every country. Figure 3: Non-normalized distribution of frames per country The monthly distribution of frames aggregated for all countries is shown in Fig. 4. We can see two big peaks, the first one in January 2021 and the second one in August 2021. In all countries, the vac- cination process started at the end of December 2020, so it makes sense that the no-vax movement started to be more predominant in the news in January 2021. Human interest is the most predominant frame. Manual inspection shows that this is because the headlines are about personal cases of people who are pro- or anti- vaccine. Attribution of responsibility is also present. Manual inspection in- dicates that local politicians and health authorities had to make decisions about who could be vaccinated at the beginning of the process. The second peak at the end of summer 2021 coincided with the health pass (also called Covid passport in some countries), and we can observe a peak in the curve corresponding to the con- flict frame, reflecting the demonstrations against the measure of mandatory health passes taken by country governments. In Figure 5, we compare the sentiment per frame and per country, to understand if there were any major differences. The sentiment analysis labels were obtained using BERT-sent from the Hugging Face package [47], used in our previous work (please refer to our original analysis in [3] for details.) We normalized the results be- tween 0 and 1 to compare frames between countries. We see that the sentiment is predominantly neutral (in blue). Examining in more Figure 4: Non-normalized monthly distribution of frames. detail the negative and positive sentiment of each frame category, we observed a few trends: • Attribution of responsibility: Negative sentiment represents 30-40% of the cases, while positive tone is only found in residual form in Italy, Switzerland, and the United Kingdom. • Conflict: Negative sentiment represents 20-35% of the cases. • Economic: Predominantly neutral, with only negative tone in Italy and UK (in the latter case, all headlines with this frame were considered negative.) • Human interest: Negative sentiment represents 30-40% of the cases, while positive tone is only found in residual form in Italy, Spain, and Switzerland. • Morality: Predominantly neutral, with negative tone in Italy, Switzerland, and the United Kingdom, • No frame: 20-30% of negative content. Figure 5: Sentiment of headline by frame and by country 632 Framing the News: From Human Perception to Large Language Model Inferences ICMR ’23, June 12–15, 2023, Thessaloniki, Greece Regarding the results of the annotation process, the fact that the distribution of the 6 frame types is relatively similar between coun- tries suggests that the anti-vaccine movement issue was treated in a similar way in these countries. The fact that human interest is the most dominant frame indicates that this issue was treated from a more human and emotional approach, with headlines about personal experiences, celebrities giving their opinion about vacci- nation, and politicians defending vaccine policies. Moreover, the reason for many headlines being classified as no-frame is partly due to how data was selected. We chose articles that contained words related to no-vax, either in the headline or in the article. This resulted in many headlines not containing anything specific related to no-vax, while the no-vax content was actually included in the main text of the corresponding articles. It is worth mentioning that prior to obtaining the results, we had expected that attribution of responsibility would be among the most prominent frames, since governments took many measures such as mandatory health pass requirements to access certain sites; we had also expected that the conflict frame would be prominent, since there were many demonstrations in Europe. In reality, however, these frames categories were not reflected as frequently at the headline level. Regarding the analysis at the temporal level, it is clear that certain events were captured by the press, such as the start of vaccination or the mandatory vaccination passport. Finally, the sentiment analysis of the different frames shows that the predominant tone in all of them is neutral or negative, with very similar trends between countries. This association between senti- ment analysis and frames has been discussed in previous literature [11] [43]. 6 RESULTS: GPT-3.5 FOR FRAME CLASSIFICATION OF HEADLINES (RQ2) Here, we present and discuss the results related to our second RQ. 6.1 Fine-tuning GPT-3.5 Table 4 shows the results of the 6-class classification task using 5-cross validation. Three models were used: GPT-3.5 and two BERT- based models. We observe that, on average, GPT-3.5 performs better than the BERT-based models. This is somehow expected as GPT- 3.5 is a much larger model. Overall, in the case of fine-tuning, the best performance for the six-class frame classification task is 72% accuracy, which is promising, with an improvement over previous models based on BERT. Yet, it should be noted that the performance differences are modest (2% improvement between GPT-3.5 and RoBERTa). Table 4: Classification results for six-class frame classifica- tion and 5-fold cross validation ACCURACY 0 1 2 3 4 AVERAGE BERT 0.68 0.69 0.72 0.64 0.70 0.67 RoBERTa 0.70 0.72 0.72 0.67 0.71 0.70 GPT3 0.75 0.70 0.72 0.71 0.71 0.72 On the other hand, BERT is open-source, while GPT-3 has an economic cost as the use of the model is not free, which monetarily limits the number of experiments that can be performed with it, as well as the different configurations one can explore to improve performance. This is important because much of the improvement in performance requires empirical explorations of model parameters More specifically, the cost of an experiment for each of the folds has a cost of 4 dollars (at the time of writing this paper.) This represents a limitation in practice. Furthermore, GPT-3 has a significant carbon footprint. Similarly, for prompt engineering (discussed in the next subsection), choosing the right prompt (i.e., the words that best define the task so that the model is able to perform adequately) is also based on trial and error. This also has an impact on carbon footprint. In connection with this topic, Strubell et al.[55] argue that improvements in the accu- racy of models depend on the availability of large computational resources, which involve large economic and environmental costs. A criticism has been made as ’the rich get richer’, in the sense that not all research groups have sufficient infrastructure resources and access to funding needed to use these models and improve their performance. Also in relation to this analysis, the work of Bender et al. [4] evaluates the costs and risks of the use of large language models, stating that researchers should be aware of the impact that these models have on the environment, and assess whether the benefits outweigh the risks. The work in [4] provides a very telling example, where people living in the Maldives or Sudan are affected by floods and pay the environmental price of training English LLMs, when similar models have not been produced for languages like Dhivehi or Sudanese Arab. In short, there is a need to establish ways to use this technological development responsibly, and it all starts with being aware of the risks it presents. 6.2 Prompt-engineering with GPT-3.5 For each headline, we got the frame that the model considered the most likely, and we compared these GPT-3.5 inferences with the frames labeled by the annotators. The agreement between model and annotator was of 49%. Analyzing the results, and specifically looking at the cases where the annotator and GPT-3.5 disagreed, we discovered that according to the frame definitions, the model in some cases proposed a frame that indeed made sense. This ob- servation, together with our previous experience in the annotation process, where headlines could have more than one valid frame, led us to design a second post-hoc experiment. We took all the headlines where each of the two annotators had disagreed with GPT-3.5, and we asked the annotators to state whether they would agree (or not) with each GPT-inferred label for a given headline. It is important to emphasize that the annotators did not know the origin of that label, i.e., they did not know if it was the label they had originally assigned, or if it was a random one. In this way, we could quantify how GPT-3.5 worked according to valid arguments provided by the annotators. In this post-hoc experiment, the model agreed in 76% of cases with the annotators. Looking at the results of the classification models, the 49% accu- racy of the prompt-engineering approach can be considered low, yet we consider that it is a valid avenue for further investigation, as in the second post-hoc analysis, we found that the model agrees 633 ICMR ’23, June 12–15, 2023, Thessaloniki, Greece David Alonso del Barrio and Daniel Gatica-Perez with human annotators in 76% of the cases. Clearly, framing in- volves aspects of subjectivity [42]. Much of what we do as people has a subjective component, influenced by how we feel or how we express opinions. News reading is never fully objective, and the annotators en- gaged in the frame classification task, influenced by their personal state of mind, experience, and culture, may perceive information differently. Monarch affirms that "for simple tasks, like binary labels on objective tasks, the statistics are fairly straightforward to decide which is the ‘correct’ label when different annotators disagree. But for subjective tasks, or even objective tasks with continuous data, there are no simple heuristics for deciding what the correct label should be" [42]. Subjectivity is involved in both the generation and perception of information: the assumption that there is only one frame is com- plicated by the point of view of the reader. In the case of news, the information sender (the journalist) has an intention, but the receiver (the reader) plays a role and is influenced by it. In psychology, this is known as the lens model of interpersonal communication, where the sender has certain objectives, but the receiver can interpret or re-interpret what the sender wants to say, with more or less accuracy [26]. Following this discussion on subjectivity, the question arose as to what would happen if, instead of headlines, we used the complete article as a source of analysis. We wondered if longer text could make the frame labeling task clearer than when using headlines. Yet another possible hypothesis is that having to read longer texts could lead to the same subject being presented from different angles. Please recall that in the existing literature discussed in Section 2, both headlines and full articles have been used from frame analysis (see Table 1.) This remains as an issue for future work. 7 CONCLUSIONS In this paper, we first presented an analysis of human-generated news frames on the covid-19 no-vax movement in Europe, and then studied different approaches using large language models for automatic inference of frames. We conclude by answering the two research questions we posed: RQ1: What are the main frames in the news headlines about the covid-19 anti-vaccine movement in 5 European countries? After annotating the headlines, we found that of the 1786 headlines, the predominant frame is human interest (45.3% of cases), which presents a news item with an emotional angle, putting a face to a problem or situation. We also found that a substantial proportion of headlines were annotated as not presenting any frame (40.2% of cases). Finally, the other frame types are found more infrequently. RQ2: Can prompt engineering be used for classification of head- lines according to frames? We first used fine-tuning of a number of language models, and found that GPT-3.5 produced classification ac- curacy of 72% on a six-frame classification task. This represented a modest 2% improvement over BERT-based models, at a significantly larger environmental cost. We then presented a new way of classi- fying frames using prompts. At the headline level, inferences made with GPT-3.5 reached 49% of agreement with human-generated frame labels. In many cases, the GPT-3.5 model inferred frame types that were considered as valid choices by human annotators, and in an post-doc experiment, the human-machine agreement reached 76%. These results have opened several new directions for future work. ACKNOWLEDGMENTS This work was supported by the AI4Media project, funded by the European Commission (Grant 951911) under the H2020 Programme ICT-48-2020. We also thank the newspapers for sharing their online articles. Finally, we thank our colleagues Haeeun Kim and Emma Bouton-Bessac for their support with annotations, and Victor Bros and Oleksii Polegkyi for discussions. ');
INSERT INTO articleApp_article VALUES(37,'Large Language Model Augmented Narrative Driven Recommendations',' Narrative-driven recommendation (NDR) presents an information access problem where users solicit recommendations with verbose descriptions of their preferences and context, for example, travelers soliciting recommendations for points of interest while describing their likes/dislikes and travel circumstances. These requests are increasingly important with the rise of natural language-based conversational interfaces for search and recommendation systems. However, NDR lacks abundant training data for models, and current platforms commonly do not support these requests. Fortunately, classical user-item interaction datasets contain rich textual data, e.g., reviews, which often describe user preferences and context – this may be used to bootstrap training for NDR models. In this work, we explore using large language models (LLMs) for data augmentation to train NDR models. We use LLMs for authoring synthetic narrative queries from user-item interactions with few-shot prompting and train retrieval models for NDR on synthetic queries and user-item interaction data. Our experiments demonstrate that this is an effective strategy for training small-parameter retrieval models that outperform other retrieval and LLM baselines for narrative-driven recommendation.',0,'narrative-driven recommendation, information access, natural language, conversational interfaces, data augmentation, large language models, retrieval models','https://drive.google.com/file/d/1RoCLTbvFrSeiU5fhegdvIU69E5j9o3ok/view?usp=drivesdk',NULL,'  INTRODUCTION Recommender systems personalized to users are an important component of several industry-scale platforms [16, 17, 46]. These systems function by inferring users’ interests from their prior interactions on the platform and making recommendations based on these inferred interests. While recommendations based on historical interactions are effective, users soliciting recommendations often start with a vague idea about their desired target items or may desire recommendations depending on the context of use, often missing in historical interaction data (Figure 1). In these scenarios, it is common for users to solicit recommendations through long-form narrative queries describing their broad interests and context. Information access tasks like these have been studied as narrative-driven recommendations (NDR) for items ranging from books [5] and movies [18], to points of interest [1]. Bogers and Koolen [5] note these narrative requests to be common on discussion forums and several subreddits1, but, there is a lack of support for these complex natural language queries in current recommenders.  However, with the emergence of conversational interfaces for information access tasks, support for complex NDR tasks is likely to become necessary. In this context, recent work has noted an increase in complex and subjective natural language requests compared to more conventional search interfaces [13, 34]. Furthermore, the emergence of large language models (LLM) with strong language understanding capabilities presents the potential for fulfilling such complex requests [9, 33]. This work explores the potential for re-purposing historical user-item recommendation datasets, traditionally used for training collaborative filtering recommenders, with LLMs to support NDR.  Specifically, given a user’s interactions, 𝐷𝑢, with items and their accompanying text documents (e.g., reviews, descriptions) 𝐷𝑢 = {𝑑𝑖}𝑁𝑢 𝑖=1, selected from a user-item interaction dataset I, we prompt InstructGPT, a 175B parameter LLM, to author a synthetic narrative query 𝑞𝑢 based on 𝐷𝑢 (Figure 2). Since we expect the query 𝑞𝑢 to be noisy and not fully representative of all the user reviews, 𝐷𝑢 is filtered to retain only a fraction of the reviews based on a language-model assigned likelihood of 𝑞𝑢 given a user document, 𝑑𝑖. Then, a pre-trained LM based retrieval model (110M parameters) is fine-tuned for retrieval on the synthetic queries and filtered reviews.  Our approach, which we refer to as Mint2, follows from the observation that while narrative queries and suggestions are often made in online discussion forums, and could serve as training data, the number of these posts and the diversity of domains for which they are available is significantly smaller than the size and diversity of passively gathered user-item interaction datasets. E.g. while Bogers and Koolen [5] note nearly 25,000 narrative requests for books on the LibraryThing discussion forum, a publicly available user-item interaction dataset for Goodreads contains interactions with nearly 2.2M books by 460k users [43] .  We empirically evaluate Mint in a publicly available test collection for point of interest recommendation: pointrec [1]. To train 1r/MovieSuggestions, r/booksuggestions, r/Animesuggest 2Mint: Data augMentation with INteraction narraTives. 777RecSys ’23, September 18–22, 2023, Singapore, Singapore Mysore, McCallum, Zamani Figure 1: An example narrative query soliciting point of interest recommendations. The query describes the users p');
DELETE FROM sqlite_sequence;
INSERT INTO sqlite_sequence VALUES('django_migrations',27);
INSERT INTO sqlite_sequence VALUES('django_admin_log',77);
INSERT INTO sqlite_sequence VALUES('django_content_type',14);
INSERT INTO sqlite_sequence VALUES('auth_permission',56);
INSERT INTO sqlite_sequence VALUES('auth_group',0);
INSERT INTO sqlite_sequence VALUES('auth_user',29);
INSERT INTO sqlite_sequence VALUES('articleApp_article_auteurs',58);
INSERT INTO sqlite_sequence VALUES('articleApp_article_references',479);
INSERT INTO sqlite_sequence VALUES('accountsApp_utilisateur',7);
INSERT INTO sqlite_sequence VALUES('accountsApp_moderateur',21);
INSERT INTO sqlite_sequence VALUES('articleApp_auteur',19);
INSERT INTO sqlite_sequence VALUES('articleApp_institution',11);
INSERT INTO sqlite_sequence VALUES('articleApp_reference',209);
INSERT INTO sqlite_sequence VALUES('accountsApp_utilisateur_Favoris',2);
INSERT INTO sqlite_sequence VALUES('articleApp_article',37);
CREATE UNIQUE INDEX "auth_group_permissions_group_id_permission_id_0cd325b0_uniq" ON "auth_group_permissions" ("group_id", "permission_id");
CREATE INDEX "auth_group_permissions_group_id_b120cbf9" ON "auth_group_permissions" ("group_id");
CREATE INDEX "auth_group_permissions_permission_id_84c5c92e" ON "auth_group_permissions" ("permission_id");
CREATE UNIQUE INDEX "auth_user_groups_user_id_group_id_94350c0c_uniq" ON "auth_user_groups" ("user_id", "group_id");
CREATE INDEX "auth_user_groups_user_id_6a12ed8b" ON "auth_user_groups" ("user_id");
CREATE INDEX "auth_user_groups_group_id_97559544" ON "auth_user_groups" ("group_id");
CREATE UNIQUE INDEX "auth_user_user_permissions_user_id_permission_id_14a6b632_uniq" ON "auth_user_user_permissions" ("user_id", "permission_id");
CREATE INDEX "auth_user_user_permissions_user_id_a95ead1b" ON "auth_user_user_permissions" ("user_id");
CREATE INDEX "auth_user_user_permissions_permission_id_1fbb5f2c" ON "auth_user_user_permissions" ("permission_id");
CREATE INDEX "django_admin_log_content_type_id_c4bce8eb" ON "django_admin_log" ("content_type_id");
CREATE INDEX "django_admin_log_user_id_c564eba6" ON "django_admin_log" ("user_id");
CREATE UNIQUE INDEX "django_content_type_app_label_model_76bd3d3b_uniq" ON "django_content_type" ("app_label", "model");
CREATE UNIQUE INDEX "auth_permission_content_type_id_codename_01ab375a_uniq" ON "auth_permission" ("content_type_id", "codename");
CREATE INDEX "auth_permission_content_type_id_2f476e4b" ON "auth_permission" ("content_type_id");
CREATE INDEX "django_session_expire_date_a5c62663" ON "django_session" ("expire_date");
CREATE UNIQUE INDEX "articleApp_article_auteurs_article_id_auteur_id_cbe28e7f_uniq" ON "articleApp_article_auteurs" ("article_id", "auteur_id");
CREATE INDEX "articleApp_article_auteurs_article_id_8660bc96" ON "articleApp_article_auteurs" ("article_id");
CREATE INDEX "articleApp_article_auteurs_auteur_id_b6a966b7" ON "articleApp_article_auteurs" ("auteur_id");
CREATE UNIQUE INDEX "articleApp_article_references_article_id_reference_id_eea3a257_uniq" ON "articleApp_article_references" ("article_id", "reference_id");
CREATE INDEX "articleApp_article_references_article_id_61cd129d" ON "articleApp_article_references" ("article_id");
CREATE INDEX "articleApp_article_references_reference_id_4579a1f3" ON "articleApp_article_references" ("reference_id");
CREATE UNIQUE INDEX "accountsApp_utilisateur_Favoris_utilisateur_id_article_id_2e3c5fff_uniq" ON "accountsApp_utilisateur_Favoris" ("utilisateur_id", "article_id");
CREATE INDEX "accountsApp_utilisateur_Favoris_utilisateur_id_82349c8a" ON "accountsApp_utilisateur_Favoris" ("utilisateur_id");
CREATE INDEX "accountsApp_utilisateur_Favoris_article_id_740f529c" ON "accountsApp_utilisateur_Favoris" ("article_id");
CREATE INDEX "articleApp_auteur_institution_id_c39463ed" ON "articleApp_auteur" ("institution_id");
COMMIT;
